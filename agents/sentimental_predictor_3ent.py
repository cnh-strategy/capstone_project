# ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import torch 
import torch.nn as nn 
import torch.optim as optim 
import time 
from transformers import AutoTokenizer, AutoModelForSequenceClassification 
from sklearn.preprocessing import StandardScaler 
import joblib 
import sys 
from datetime import timedelta

# --- ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬ (í•™ìŠµ ë°ì´í„°) ---
stock_df = pd.read_csv('C:\\Users\\jinfo\\Desktop\\Programming\\capstone_project\\stock_data.csv') # ì£¼ê°€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
news_df = pd.read_csv('C:\\Users\\jinfo\\Desktop\\Programming\\capstone_project\\news_data.csv') # ë‰´ìŠ¤ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
stock = stock_df.copy() # ì›ë³¸ ë³´ì¡´ì„ ìœ„í•´ ë³µì‚¬
news = news_df.copy()

# ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬
news['summary'] = news['summary'].fillna('') # ìš”ì•½(summary) ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’(NaN)ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
news = news.dropna(subset=['date']) # ë‚ ì§œ(date) ì»¬ëŸ¼ì— ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°
news['date'] = pd.to_datetime(news['date']) # ë‚ ì§œ ì»¬ëŸ¼ì„ datetime ê°ì²´ë¡œ ë³€í™˜

# FinBERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ê°ì„± ë¶„ì„ìš©)
tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-tone') 
finbert = AutoModelForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone') 
news['text'] = news['title'] + ' ' + news['summary'] # ì œëª©ê³¼ ìš”ì•½ì„ í•©ì³ ê°ì„± ë¶„ì„ì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ìƒì„±

# FinBERTë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
def finbert_sentiment_scores(texts, batch_size=16):
    finbert.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    scores = []
    with torch.no_grad(): # ê¸°ìš¸ê¸° ê³„ì‚°ì„ ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ)
        for i in range(0, len(texts), batch_size): # ë°°ì¹˜ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            batch_texts = texts[i:i+batch_size]
            # í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§• ë° PyTorch í…ì„œë¡œ ë³€í™˜
            inputs = tokenizer(list(batch_texts), return_tensors='pt', padding=True, truncation=True, max_length=512)
            outputs = finbert(**inputs) # ëª¨ë¸ ì˜ˆì¸¡
            # ë¡œì§“(logits)ì„ ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax)ë¥¼ í†µí•´ í™•ë¥ ë¡œ ë³€í™˜í•˜ê³  NumPy ë°°ì—´ë¡œ ì €ì¥
            scores.extend(torch.softmax(outputs.logits, dim=1).cpu().numpy())
    return np.array(scores)


# ë‰´ìŠ¤ ë°ì´í„°ì— ê°ì„± ì ìˆ˜(ê¸ì •/ë¶€ì •/ì¤‘ë¦½ í™•ë¥ ) ì¶”ê°€
news[['prob_positive', 'prob_negative', 'prob_neutral']] = finbert_sentiment_scores(news['text'].values)
news['date'] = pd.to_datetime(news['date']).dt.normalize() # ë‚ ì§œì˜ ì‹œ/ë¶„/ì´ˆ ì œê±° (ì¼ ë‹¨ìœ„ë¡œ í†µì¼)

# ì¼ë³„/ì¢…ëª©ë³„ í‰ê·  ê°ì„± ì ìˆ˜ ë° ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ ê³„ì‚°
daily_sentiments = (news.groupby(['date','ticker'])
    .agg(prob_positive=('prob_positive','mean'), # ì¼ë³„ í‰ê·  ê¸ì • í™•ë¥ 
         prob_negative=('prob_negative','mean'), # ì¼ë³„ í‰ê·  ë¶€ì • í™•ë¥ 
         prob_neutral=('prob_neutral','mean'), # ì¼ë³„ í‰ê·  ì¤‘ë¦½ í™•ë¥ 
         n_news=('title','count')).reset_index()) # ì¼ë³„ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜

# ê¸°ìˆ ì  ë¶„ì„ ì§€í‘œ(Feature)ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ (í˜„ì¬ ëª¨ë¸ì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, ì˜ˆì¸¡ ì‹œí€€ìŠ¤ ì™„ì„±ì„ ìœ„í•´ ìœ ì§€)
def add_tech_features(df):
    df['ma5']     = df['Close'].rolling(window=5).mean() 
    df['ma10']     = df['Close'].rolling(window=10).mean() 
    df['vol5']     = df['Volume'].rolling(window=5).mean() if 'Volume' in df.columns else 0
    df['ret']     = df['Close'].pct_change() # ì¼ë³„ ìˆ˜ìµë¥  (ì „ì¼ ëŒ€ë¹„ ë³€í™”ìœ¨)
    df['ret_next']     = df['ret'].shift(-1) 
    return df

# ì£¼ê°€ ë°ì´í„°ì— ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€ ë° ì „ì²˜ë¦¬
stock['Date'] = pd.to_datetime(stock['Date']).dt.normalize() # ë‚ ì§œ í˜•ì‹ í†µì¼
# ì¢…ëª©ë³„(Symbol)ë¡œ ì •ë ¬ í›„ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
stock = stock.sort_values(['Symbol','Date']).groupby('Symbol', group_keys=False).apply(add_tech_features)
stock['next_close'] = stock.groupby('Symbol')['Close'].shift(-1) # ë‹¤ìŒ ë‚  ì¢…ê°€ë¥¼ ì˜ˆì¸¡ ëª©í‘œ(Target)ë¡œ ì„¤ì •
# ìˆ˜ìµë¥ ê³¼ íƒ€ê²Ÿê°’ì˜ ê²°ì¸¡ì¹˜ ì œê±°
stock = stock.dropna(subset=['ret','next_close']) 

# ì£¼ê°€ ë°ì´í„°ì™€ ì¼ë³„ ê°ì„± ë°ì´í„°ë¥¼ ë³‘í•©
data = pd.merge(stock, daily_sentiments, how='left',
    left_on=['Date','Symbol'], right_on=['date','ticker'])
# ë³‘í•© í›„ ê°ì„± ê´€ë ¨ ë°ì´í„° ê²°ì¸¡ì¹˜(ë‰´ìŠ¤ê°€ ì—†ë˜ ë‚ )ëŠ” 0.0ìœ¼ë¡œ ì±„ì›€
for c in ['prob_positive','prob_negative','prob_neutral','n_news']:
    if c in data: data[c] = data[c].fillna(0.0)

# LSTM ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  íŠ¹ì„±(Feature) ë¦¬ìŠ¤íŠ¸ ì •ì˜
FEATURES = ['prob_positive','prob_negative','prob_neutral','n_news','ret','Close'] 

# ì‹œê³„ì—´ ë°ì´í„°(ì‹œí€€ìŠ¤)ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def create_sequences(df, window_size=10):
    df = df.sort_values('Date')
    arr = df[FEATURES].values # íŠ¹ì„± ë°ì´í„° (ì…ë ¥ X)
    tgt = df['next_close'].values # ë‹¤ìŒ ë‚  ì¢…ê°€ (ì¶œë ¥ y)
    seqs, tgts = [], []
    for i in range(len(df) - window_size):
        seqs.append(arr[i:i+window_size]) # window_size ê¸°ê°„ì˜ ì…ë ¥ ì‹œí€€ìŠ¤
        tgts.append(tgt[i+window_size]) # ì‹œí€€ìŠ¤ ë‹¤ìŒ ë‚ ì˜ ì¢…ê°€
    return np.array(seqs), np.array(tgts)

# ì „ì²´ ë°ì´í„°ì…‹ì„ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜
def prepare_dataset(df, window_size=10):
    X, y, symbol_all, date_all = [], [], [], []
    for symbol in df['Symbol'].unique(): 
        sd = df[df['Symbol']==symbol].sort_values('Date')
        seq, tgt = create_sequences(sd, window_size)
        if len(seq) > 0: 
            X.append(seq)
            y.append(tgt)
            symbol_all.extend([symbol] * len(seq))
            date_all.extend(sd['Date'].iloc[window_size:].values) 
    return (np.concatenate(X), np.concatenate(y), symbol_all, date_all) if X else (np.array([]), np.array([]), [], [])

window_size = 10 # ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì • (ì—¬ê¸°ì„œëŠ” 10ì¼)

# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„° ê¸°ê°„ ë¶„í• 
train_df = data[(data['Date']>=pd.to_datetime('2024-01-01')) & (data['Date']<=pd.to_datetime('2024-10-31'))]
val_df = data[(data['Date']>=pd.to_datetime('2024-11-01')) & (data['Date']<=pd.to_datetime('2024-12-31'))]

# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„
X_train, y_train, symbol_train, date_train = prepare_dataset(train_df, window_size)
X_val, y_val, symbol_val, date_val = prepare_dataset(val_df, window_size)

# ë°ì´í„° í‘œì¤€í™” í•¨ìˆ˜
def standardize(X, y):
    nsamples, nwin, nfeat = X.shape
    # ì…ë ¥(X) ë°ì´í„°ë¥¼ í‰íƒ„í™”í•˜ì—¬ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ í›„ ë³€í™˜
    scaler_x = StandardScaler().fit(X.reshape(-1, nfeat))
    X_scaled = scaler_x.transform(X.reshape(-1, nfeat)).reshape(nsamples, nwin, nfeat)
    # íƒ€ê²Ÿ(y) ë°ì´í„°ë„ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ í›„ ë³€í™˜
    scaler_y = StandardScaler().fit(y.reshape(-1, 1))
    y_scaled = scaler_y.transform(y.reshape(-1, 1)).flatten()
    return X_scaled, y_scaled, scaler_x, scaler_y

# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„° í‘œì¤€í™” ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
X_train_scaled, y_train_scaled, scaler_x, scaler_y = standardize(X_train, y_train)
# ì£¼ì˜: ê²€ì¦ ë°ì´í„°ëŠ” í•™ìŠµ ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ëŸ¬(scaler_x, scaler_y)ë¥¼ ì‚¬ìš©í•´ì•¼ ì •í™•í•˜ì§€ë§Œ ì›ë³¸ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ë…ë¦½ì ìœ¼ë¡œ í‘œì¤€í™” -> ì´ê²Œ ë­” ì†Œë¦¬ì¼ê¹Œ..
X_val_scaled, y_val_scaled, _, _ = standardize(X_val, y_val) 

# PyTorch Dataset í´ë˜ìŠ¤ ì •ì˜
class StockDataset(torch.utils.data.Dataset):
    def __init__(self, X, y):
        self.X, self.y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)
    def __len__(self): return len(self.X) 
    def __getitem__(self, idx): return self.X[idx], self.y[idx] 

# DataLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ ë°°ì¹˜ ì²˜ë¦¬ ì¤€ë¹„
train_loader = torch.utils.data.DataLoader(StockDataset(X_train_scaled, y_train_scaled), batch_size=32, shuffle=True)
val_loader = torch.utils.data.DataLoader(StockDataset(X_val_scaled, y_val_scaled), batch_size=32, shuffle=False)

# --- LSTM ëª¨ë¸ ì •ì˜ ---
class StockSentimentLSTM(nn.Module):
    def __init__(self, hidden_dim=64, num_layers=1, input_size=len(FEATURES)): 
        super().__init__()
        # input_sizeëŠ” FEATURES ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´(6)
        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True, dropout=0.1)
        self.fc = nn.Linear(hidden_dim, 1) # ì˜ˆì¸¡ ì¢…ê°€(1ì°¨ì›) ì¶œë ¥
    def forward(self, x):
        out, _ = self.lstm(x) # LSTM ìˆœì „íŒŒ
        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©í•˜ì—¬ FC ë ˆì´ì–´ì— ì „ë‹¬
        return self.fc(out[:, -1, :]).squeeze() 

# ëª¨ë¸ í•™ìŠµ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # GPU ë˜ëŠ” CPU ì„ íƒ -> ì¼ë‹¨ CPU
model = StockSentimentLSTM().to(device) 
criterion = nn.MSELoss() # ì†ì‹¤ í•¨ìˆ˜: í‰ê·  ì œê³± ì˜¤ì°¨ (MSE)
optimizer = optim.Adam(model.parameters(), lr=3e-4) # ìµœì í™” í•¨ìˆ˜: Adam

# --- ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ ---
def train_model(train_loader, val_loader, model, criterion, optimizer, device, epochs=20):
    train_losses, val_losses = [], []
    for epoch in range(epochs):
        t1, train_loss = time.time(), 0
        model.train() # í•™ìŠµ ëª¨ë“œ
        for inputs, targets in train_loader: 
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad() 
            outputs = model(inputs) 
            loss = criterion(outputs, targets) 
            loss.backward() 
            optimizer.step() 
            train_loss += loss.item() * inputs.size(0)
        train_loss /= len(train_loader.dataset)
        train_losses.append(train_loss)

        val_loss = 0
        model.eval() # í‰ê°€ ëª¨ë“œ
        with torch.no_grad(): 
            for inputs, targets in val_loader: 
                inputs, targets = inputs.to(device), targets.to(device)
                val_loss += criterion(model(inputs), targets).item() * inputs.size(0)
        val_loss /= len(val_loader.dataset)
        val_losses.append(val_loss)

        print(f"Epoch {epoch+1} - Train Loss: {train_loss:.6f} - Val Loss: {val_loss:.6f} - Time: {time.time()-t1:.1f}s")
    return train_losses, val_losses

# ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
train_losses, val_losses = train_model(train_loader, val_loader, model, criterion, optimizer, device)

# --- ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ---
torch.save(model.state_dict(), "model_lstm_stocksentiment.pt") # í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì €ì¥
joblib.dump(scaler_x, "scaler_x.pkl") # ì…ë ¥ ë°ì´í„° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
joblib.dump(scaler_y, "scaler_y.pkl") # íƒ€ê²Ÿ ë°ì´í„° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥


print("ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")

# --- ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì˜ˆì¸¡ (validation set) ---
model_eval = StockSentimentLSTM(hidden_dim=64, num_layers=1, input_size=len(FEATURES)) 
model_eval.load_state_dict(torch.load("model_lstm_stocksentiment.pt")) # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
model_eval.eval() 
model_eval.to(device) 
scaler_x = joblib.load("scaler_x.pkl") 
scaler_y = joblib.load("scaler_y.pkl") 

# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
def plot_predictions(model, val_loader, scaler_y):
    model.eval()
    preds, targets = [], []
    with torch.no_grad():
        for inputs, y_true in val_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            preds.extend(outputs.cpu().numpy())
            targets.extend(y_true.numpy())

    # ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ì—­ë³€í™˜
    preds = scaler_y.inverse_transform(np.array(preds).reshape(-1,1)).flatten()
    targets = scaler_y.inverse_transform(np.array(targets).reshape(-1,1)).flatten()

    plt.figure(figsize=(12, 6))
    plt.plot(targets, label='Actual Next Close', linewidth=2)
    plt.plot(preds, label='Predicted Next Close', linewidth=2, alpha=0.8)
    # ì œëª©ì—ì„œ no_tech ì œê±°
    plt.title('Actual vs Predicted Next Day Close (Validation set)')
    plt.xlabel('Sample Index (Time-ordered)')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()

# ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ì‹œê°í™” ì‹¤í–‰
plot_predictions(model_eval, val_loader, scaler_y)

# í•™ìŠµ ë° ê²€ì¦ ì†ì‹¤ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” í•¨ìˆ˜
def plot_loss(train_losses, val_losses):
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss (MSE)')
    plt.legend()
    plt.grid()
    plt.show()

# ì†ì‹¤ ê·¸ë˜í”„ ì‹œê°í™” ì‹¤í–‰
plot_loss(train_losses, val_losses)

print("\n--- ëª¨ë¸ ì„±ëŠ¥ ì¶”ê°€ ë¶„ì„ ì‹œì‘ ---\n")

def evaluate_and_analyze(model, val_loader, scaler_y, X_val, y_val, symbol_val, date_val):
    """
    ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë˜ëŒë¦° í›„,
    ì¢…ëª©ë³„ MSEë¥¼ ê³„ì‚°í•˜ê³  ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ì˜ˆì¸¡ ì‹œì ì˜ ë°ì´í„°ë¥¼ ë°˜í™˜
    """
    model.eval()
    preds_scaled, targets_scaled = [], []
    
    # 1. ìŠ¤ì¼€ì¼ë§ëœ ì˜ˆì¸¡ê°’ ë° ì‹¤ì œê°’ ì¶”ì¶œ
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            preds_scaled.extend(outputs.cpu().numpy())
            targets_scaled.extend(targets.cpu().numpy())
            
    preds_scaled = np.array(preds_scaled)
    targets_scaled = np.array(targets_scaled)
    
    # 2. ì›ë˜ ê°€ê²© ìŠ¤ì¼€ì¼ë¡œ ì—­ë³€í™˜ (Inverse Transform)
    preds = scaler_y.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()
    targets = scaler_y.inverse_transform(targets_scaled.reshape(-1, 1)).flatten()

    # 3. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬
    results_df = pd.DataFrame({
        'Date': date_val,
        'Symbol': symbol_val,
        'Actual': targets,
        'Predicted': preds
    })
    
    results_df['Squared_Error'] = (results_df['Actual'] - results_df['Predicted']) ** 2
    results_df['Absolute_Error'] = np.abs(results_df['Actual'] - results_df['Predicted'])

    # 4. ì¢…ëª©ë³„ RMSE ê³„ì‚° ë° ì¶œë ¥
    mse_per_symbol = results_df.groupby('Symbol')['Squared_Error'].mean().sort_values(ascending=False)
    rmse_per_symbol = np.sqrt(mse_per_symbol)
    
    print("## ğŸ“ˆ ì¢…ëª©ë³„ RMSE (Root Mean Squared Error) - ì„±ëŠ¥ ë¶„ì„")
    print(rmse_per_symbol.to_string(float_format='%.4f'))
    print("\n" + "="*50 + "\n")

    # 5. ì „ì²´ ë°ì´í„°ì—ì„œ ê°€ì¥ ì˜ ì˜ˆì¸¡ëœ(ì˜¤ì°¨ê°€ ì‘ì€) ìƒ˜í”Œ ì°¾ê¸°
    best_sample = results_df.sort_values('Absolute_Error').iloc[0]
    
    best_date = best_sample['Date']
    best_symbol = best_sample['Symbol']
    
    # 6. ê°€ì¥ ì˜ ì˜ˆì¸¡ëœ ì‹œì ì˜ ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
    
    # í•´ë‹¹ ì¢…ëª©ì˜ ì „ì²´ ì‹œê³„ì—´ ë°ì´í„°
    symbol_data = data[data['Symbol'] == best_symbol].sort_values('Date').reset_index(drop=True)
    
    # ì˜ˆì¸¡ ì‹œì ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŒ
    end_index = symbol_data[symbol_data['Date'] == best_date].index[0]
    start_index = end_index - window_size
    
    # í•´ë‹¹ ì‹œì ì˜ ì…ë ¥ ë°ì´í„° (ì‹œí€€ìŠ¤) ì¶”ì¶œ
    best_input_data = symbol_data.iloc[start_index : end_index]
    
    print(f"## â­ ê°€ì¥ ì˜ ì˜ˆì¸¡ëœ ì‹œì ì˜ ìƒì„¸ ë¶„ì„ (ì ˆëŒ€ ì˜¤ì°¨ ìµœì†Œ)")
    print(f" - ì˜ˆì¸¡ ì‹œì : {best_date.strftime('%Y-%m-%d')}")
    print(f" - ì¢…ëª©: {best_symbol}")
    print(f" - ì‹¤ì œ ë‹¤ìŒë‚  ì¢…ê°€: {best_sample['Actual']:.2f}")
    print(f" - ì˜ˆì¸¡ëœ ë‹¤ìŒë‚  ì¢…ê°€: {best_sample['Predicted']:.2f}")
    print(f" - ì ˆëŒ€ ì˜¤ì°¨: {best_sample['Absolute_Error']:.4f}\n")
    
    print(f"## ğŸ“œ ì…ë ¥ ë°ì´í„° ({window_size}ì¼ ì‹œí€€ìŠ¤) - {best_symbol} ({best_input_data['Date'].min().strftime('%Y-%m-%d')} ~ {best_input_data['Date'].max().strftime('%Y-%m-%d')})")
    print(best_input_data[['Date'] + FEATURES].to_string(index=False, float_format='%.4f'))

    return results_df

# ë¶„ì„ ì‹¤í–‰
results_df = evaluate_and_analyze(model_eval, val_loader, scaler_y, X_val, y_val, symbol_val, date_val)

# --- ê°œë³„ ì¢…ëª© ì˜ˆì¸¡ ì‹œê°í™” (validations set) ---

def plot_single_symbol(results_df, symbol):
    """íŠ¹ì • ì¢…ëª©ì— ëŒ€í•œ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ì‹œê°í™”"""
    symbol_df = results_df[results_df['Symbol'] == symbol].reset_index(drop=True)
    if symbol_df.empty:
        print(f"\nê²½ê³ : ì¢…ëª© {symbol}ì— ëŒ€í•œ ë°ì´í„°ê°€ ê²€ì¦ ì…‹ì— ì—†ìŠµë‹ˆë‹¤.")
        return

    plt.figure(figsize=(14, 7))
    plt.plot(symbol_df['Date'], symbol_df['Actual'], label=f'{symbol} Actual Close', linewidth=2)
    plt.plot(symbol_df['Date'], symbol_df['Predicted'], label=f'{symbol} Predicted Close', linewidth=2, alpha=0.8, linestyle='--')
    
    plt.title(f'{symbol}: Actual vs Predicted Next Day Close')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# RMSEê°€ ê°€ì¥ ë†’ì•˜ë˜ (ê°€ì¥ ì˜ˆì¸¡ì´ ì–´ë ¤ì› ë˜) ì¢…ëª© ì‹œê°í™”
worst_symbol = results_df.groupby('Symbol')['Squared_Error'].mean().idxmax()
print(f"\n--- ê°€ì¥ ì˜ˆì¸¡ì´ ì–´ë ¤ì› ë˜ ì¢…ëª© ì‹œê°í™”: {worst_symbol} ---\n")
plot_single_symbol(results_df, worst_symbol)

# RMSEê°€ ê°€ì¥ ë‚®ì•˜ë˜ (ê°€ì¥ ì˜ˆì¸¡ì´ ì‰¬ì› ë˜) ì¢…ëª© ì‹œê°í™”
best_overall_symbol = results_df.groupby('Symbol')['Squared_Error'].mean().idxmin()
print(f"\n--- ê°€ì¥ ì˜ˆì¸¡ì´ ì‰¬ì› ë˜ ì¢…ëª© ì‹œê°í™”: {best_overall_symbol} ---\n")
plot_single_symbol(results_df, best_overall_symbol)
# ----------------------------------------------------
# 1. ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ ìˆ˜í–‰ (3,735ê°œ ì‹œí€€ìŠ¤ ì „ì²´) - ì¬ì‹¤í–‰ í•„ìš” ì—†ìŒ.
#    ì´ì „ ì½”ë“œì—ì„œ pred_results_df_allì— í•„ìš”í•œ ë³€ìˆ˜ë“¤ì´ ì´ë¯¸ ìƒì„±ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# ----------------------------------------------------

# 2. ì˜ˆì¸¡ ê²°ê³¼ DataFrame ì¬ì •ì˜ (ì „ì²´ ìœ íš¨ ì˜ˆì¸¡ ê²°ê³¼)
#    (ì´ ë¶€ë¶„ì´ 2025ë…„ 1ì›” ë°ì´í„°ê¹Œì§€ í¬í•¨ëœ ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.)
pred_results_df_all = pd.DataFrame({
    'Date': date_pred,
    'Symbol': symbol_pred,
    'Actual_Next_Close': targets_price,
    'Predicted_Next_Close': preds_price
}).sort_values(['Symbol', 'Date']).reset_index(drop=True)

if pred_results_df_all.empty:
    print("\n[ì¬ì‹œë„ ì‹¤íŒ¨] ì˜ˆì¸¡ëœ ì‹œí€€ìŠ¤ê°€ ì—†ì–´ ê²°ê³¼ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit()

print("\n" + "="*50)
print("## âœ… ì „ì²´ ìœ íš¨ ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„")
print(f"ì´ ìœ íš¨ ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {len(pred_results_df_all)}ê°œ")
print(f"ìµœì´ˆ ì˜ˆì¸¡ì¼: {pred_results_df_all['Date'].min().strftime('%Y-%m-%d')}")
print(f"ìµœì¢… ì˜ˆì¸¡ì¼: {pred_results_df_all['Date'].max().strftime('%Y-%m-%d')}")
print("="*50 + "\n")

# ----------------------------------------------------
# 3. 2025ë…„ 1ì›” ì˜ˆì¸¡ ê²°ê³¼ í•„í„°ë§
# ----------------------------------------------------
start_date_pred_target = pd.to_datetime('2025-01-01')
end_date_pred_target = pd.to_datetime('2025-01-31')

pred_results_df_jan = pred_results_df_all[
    (pred_results_df_all['Date'] >= start_date_pred_target) & 
    (pred_results_df_all['Date'] <= end_date_pred_target)
].sort_values(['Symbol', 'Date']).reset_index(drop=True)

if pred_results_df_jan.empty:
    print("\n[ê²°ê³¼ ì—†ìŒ] 2025ë…„ 1ì›” ì˜ˆì¸¡ì— í•´ë‹¹í•˜ëŠ” ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ê°€ 2025ë…„ 1ì›” 31ì¼ê¹Œì§€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit()

print("="*50)
print("## âœ… 2025ë…„ 1ì›” ì˜ˆì¸¡ ê²°ê³¼")
print(f"ì´ ìœ íš¨ ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {len(pred_results_df_jan)}ê°œ")
print(f"ìµœì´ˆ ì˜ˆì¸¡ì¼: {pred_results_df_jan['Date'].min().strftime('%Y-%m-%d')}")
print(f"ìµœì¢… ì˜ˆì¸¡ì¼: {pred_results_df_jan['Date'].max().strftime('%Y-%m-%d')}")
print(pred_results_df_jan.head().to_string(float_format='%.4f'))
print("="*50 + "\n")

# ----------------------------------------------------
# 4. 2025ë…„ 1ì›” ì˜ˆì¸¡ ì¢…ê°€ vs ì‹¤ì œ ì¢…ê°€ ì‹œê°í™” ë° RMSE ê³„ì‚°
# ----------------------------------------------------

def plot_single_symbol_jan(results_df, symbol):
    """2025ë…„ 1ì›” íŠ¹ì • ì¢…ëª©ì— ëŒ€í•œ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ì‹œê°í™”"""
    symbol_df = results_df[results_df['Symbol'] == symbol].reset_index(drop=True)
    if symbol_df.empty:
        return

    plt.figure(figsize=(14, 7))
    plt.plot(symbol_df['Date'], symbol_df['Actual_Next_Close'], label=f'{symbol} Actual Next Close', linewidth=2)
    plt.plot(symbol_df['Date'], symbol_df['Predicted_Next_Close'], label=f'{symbol} Predicted Next Close', linewidth=2, alpha=0.8, linestyle='--')
    
    # RMSE ê³„ì‚° ë° ì œëª©ì— ì¶”ê°€
    mse = ((symbol_df['Actual_Next_Close'] - symbol_df['Predicted_Next_Close']) ** 2).mean()
    rmse = np.sqrt(mse)
    
    plt.title(f'January 2025 Forecast (W=10) - {symbol}: Actual vs Predicted Next Day Close (RMSE: {rmse:.4f})')
    plt.xlabel('Date')
    plt.ylabel('Next Day Close Price')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# 2025ë…„ 1ì›” ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•´ ì‹œê°í™” ì‹¤í–‰
predicted_symbols_jan = pred_results_df_jan['Symbol'].unique()
print(f"### ğŸ“ˆ 2025ë…„ 1ì›” ì˜ˆì¸¡ ì‹œê°í™” ì‹œì‘ ({len(predicted_symbols_jan)} ì¢…ëª©)")

for symbol in predicted_symbols_jan:
    plot_single_symbol_jan(pred_results_df_jan, symbol)
    
# ì „ì²´ ì¢…ëª©ì— ëŒ€í•œ RMSE ê³„ì‚°
mse_pred_per_symbol_jan = pred_results_df_jan.groupby('Symbol').apply(
    lambda x: ((x['Actual_Next_Close'] - x['Predicted_Next_Close']) ** 2).mean()
)
rmse_pred_per_symbol_jan = np.sqrt(mse_pred_per_symbol_jan).sort_values(ascending=False)

print("\n## ğŸ’° 2025ë…„ 1ì›” ì¢…ëª©ë³„ ì˜ˆì¸¡ RMSE")
print(rmse_pred_per_symbol_jan.to_string(float_format='%.4f'))