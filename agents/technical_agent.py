# agents/technical_agent.py

import os
import json
from typing import List, Optional
from datetime import datetime

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import yfinance as yf
from torch.utils.data import DataLoader, TensorDataset

from agents.base_agent import (
    BaseAgent, StockData, Target, Opinion, Rebuttal
)

from core.technical_classes.technical_data_set import (
    build_dataset as build_dataset_tech,
    load_dataset as load_dataset_tech,
)

from config.agents import agents_info, dir_info
from prompts import OPINION_PROMPTS, REBUTTAL_PROMPTS, REVISION_PROMPTS

# ===============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ===============================================================
def r4(x):
    """ì†Œìˆ˜ì  4ìë¦¬ ë°˜ì˜¬ë¦¼"""
    try:
        return float(f"{float(x):.4f}")
    except:
        return x

class TechnicalAgent(BaseAgent, nn.Module):
    """
    TechnicalAgent: ê¸°ìˆ ì  ë¶„ì„ ê¸°ë°˜ ì£¼ê°€ ì˜ˆì¸¡ ì—ì´ì „íŠ¸
    
    ì£¼ê°€ ì°¨íŠ¸ ë°ì´í„°(ê°€ê²©, ê±°ë˜ëŸ‰, ê¸°ìˆ ì  ì§€í‘œ)ë¥¼ ë¶„ì„í•˜ì—¬
    ì£¼ê°€ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
    
    ì£¼ìš” ê¸°ëŠ¥:
    - RSI, SMA ë“± ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
    - 2ì¸µ LSTM + Time-Attention ë©”ì»¤ë‹ˆì¦˜
    - Attention ê°€ì¤‘ì¹˜ë¥¼ í™œìš©í•œ ì‹œê°„ ì¤‘ìš”ë„ ë¶„ì„
    - GradÃ—Input ë° Occlusionì„ í†µí•œ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
    - Monte Carlo Dropoutì„ í†µí•œ ë¶ˆí™•ì‹¤ì„± ì¶”ì •
    - LLMì„ í™œìš©í•œ Opinion, Rebuttal, Revision ìƒì„±
    
    Attributes:
        agent_id: ì—ì´ì „íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "TechnicalAgent")
        window_size: ì‹œê³„ì—´ ìœˆë„ìš° í¬ê¸°
        hidden_dims: LSTM ë ˆì´ì–´ë³„ hidden dimensions
        dropout: Dropout ë¹„ìœ¨
        input_dim: ì…ë ¥ feature ì°¨ì›
    """

    def __init__(self,
        agent_id="TechnicalAgent",
        input_dim=agents_info["TechnicalAgent"]["input_dim"],
        rnn_units1=agents_info["TechnicalAgent"]["rnn_units1"], # 1ì¸µ hidden (ì•„ì—°ìˆ˜ì •)
        rnn_units2=agents_info["TechnicalAgent"]["rnn_units2"], # 2ì¸µ hidden (ì•„ì—°ìˆ˜ì •)
        dropout=agents_info["TechnicalAgent"]["dropout"],
        data_dir=dir_info["data_dir"],
        window_size=agents_info["TechnicalAgent"]["window_size"],
        epochs=agents_info["TechnicalAgent"]["epochs"],
        learning_rate=agents_info["TechnicalAgent"]["learning_rate"],
        batch_size=agents_info["TechnicalAgent"]["batch_size"],
        **kwargs
    ):
        # 1) nn.Module ë¨¼ì € ì´ˆê¸°í™”
        nn.Module.__init__(self)

        # 2) BaseAgent ì´ˆê¸°í™”
        BaseAgent.__init__(self, agent_id=agent_id, data_dir=data_dir, **kwargs)


        # ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ì•„ì—°ìˆ˜ì •)
        self.input_dim = int(input_dim)
        self.u1         = int(rnn_units1)
        self.u2         = int(rnn_units2)
        self.window_size= int(window_size)
        self.epochs     = int(epochs)
        self.lr         = float(learning_rate)
        self.batch_size = int(batch_size)

        # LSTMx2 + time attention (ì•„ì—°ìˆ˜ì •)
        self.lstm1 = nn.LSTM(self.input_dim, self.u1, batch_first=True)
        self.lstm2 = nn.LSTM(self.u1, self.u2, batch_first=True)
        self.attn_vec = nn.Parameter(torch.randn(self.u2))
        self.fc = nn.Linear(self.u2, 1)
        self.drop = nn.Dropout(float(dropout))


        # Optimizer / Loss ì„¤ì •
        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr) # ì•„ì—°ìˆ˜ì • lr
        # ê¸°ì¡´: MSE Loss ì‚¬ìš©
        # self.loss_fn = nn.MSELoss()
        # ìˆ˜ì •: Huber Loss ì‚¬ìš© - ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•˜ê³  ë” ì•ˆì •ì ì¸ í•™ìŠµ
        # delta=1.0ìœ¼ë¡œ ì¡°ì • (íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ë§ í›„ ì ì ˆí•œ ê°’)
        self.loss_fn = nn.HuberLoss(delta=1.0)
        self.last_pred = None
        self.last_attn = None  # (ì•„ì—°ìˆ˜ì •) time-attention ìºì‹œ
        self._last_idea = None  # TechnicalAgent ì „ìš© ì„¤ëª… ì •ë³´ ì €ì¥ìš©

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # LSTMÃ—2 + time-attentionì´ ìˆìœ¼ë©´ ì‚¬ìš© (ì•„ì—°ìˆ˜ì •)
        """
        ì…ë ¥: x (B, T, F)
        ì¶œë ¥: (B, 1)  â€” ë‹¤ìŒë‚  ìˆ˜ìµë¥ (í•™ìŠµ ìŠ¤ì¼€ì¼)
        """
        h1, _ = self.lstm1(x)
        h1 = self.drop(h1)
        h2, _ = self.lstm2(h1)
        h2 = self.drop(h2)

        # tiem-attention: ê° ì‹œì  ê°€ì¤‘ì¹˜
        w = torch.softmax(torch.matmul(h2, self.attn_vec), dim=1)  # [B,T]
        self._last_attn = w.detach()                               # ì•„ì—°ìˆ˜ì •
        ctx = (h2 * w.unsqueeze(-1)).sum(dim=1)                    # [B,u2]
        return self.fc(ctx)                                        # [B,1]

    def _safe_names(self, feature_cols, F):
        """
        ê¸°ëŠ¥: í”¼ì²˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ ì…ë ¥ ì°¨ì›(F)ì— ë§ì¶° ë³´ì •.
        ì…ë ¥: feature_cols(list|None), F(int)
        ì¶œë ¥: ê¸¸ì´ Fì˜ í”¼ì²˜ëª… ë¦¬ìŠ¤íŠ¸
        """
        cols = list(feature_cols) if feature_cols else []
        if len(cols) != F:
            cols = cols[:F] + [f"f{i}" for i in range(len(cols), F)]
        return cols

    def _safe_dates(self, dates, T):
        """
        ê¸°ëŠ¥: ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìœˆë„ìš° ê¸¸ì´(T)ì— ë§ì¶° ë³´ì •.
        ì…ë ¥: dates(list|None), T(int)
        ì¶œë ¥: ê¸¸ì´ Tì˜ ë‚ ì§œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        """
        if not dates or len(dates) != T:
            return [f"t-{T-1-i}" for i in range(T)]
        return [str(d) for d in dates]

    def _scale_like_train(self, X_np):
        """
        ê¸°ëŠ¥: í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¼ëŸ¬(self.scaler)ë¡œ ì…ë ¥ ìŠ¤ì¼€ì¼ ì •í•©.
        ì…ë ¥: X_np(np.ndarray) (1,T,F)
        ì¶œë ¥: ìŠ¤ì¼€ì¼ëœ np.ndarray (ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜)
        """
        try:
            out = self.scaler.transform(X_np)
            if isinstance(out, tuple) and len(out) >= 1:
                return out[0]
            return out
        except Exception:
            return X_np

    @torch.no_grad()
    def time_importance_from_attention(self, X_last: torch.Tensor) -> np.ndarray:
        """
        ê¸°ëŠ¥: ëª¨ë¸ì˜ time-attention ê°€ì¤‘ì¹˜ë¡œ ì‹œê°„ ì¤‘ìš”ë„ ê³„ì‚°.
        ì…ë ¥: X_last(torch.Tensor) (1,T,F)
        ì¶œë ¥: ì •ê·œí™”ëœ ì‹œê°„ ì¤‘ìš”ë„ np.ndarray (T,)
        ì£¼ì˜: forwardë¥¼ 1íšŒ í˜¸ì¶œí•´ _last_attn ìƒì„±.
        """
        self.eval()
        _ = self(X_last)
        attn = getattr(self, "_last_attn", None)  # [B,T]

        if attn is None:
            T = X_last.shape[1]
            return np.ones(T, dtype=float) / T
        w = attn[0].abs().cpu().numpy()
        # 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (T, 1) -> (T,)
        w = w.flatten() if w.ndim > 1 else w
        s = w.sum()
        result = w / s if s > 0 else np.ones_like(w) / len(w)
        # ë°˜í™˜ê°’ì´ 1ì°¨ì›ì¸ì§€ í™•ì¸
        return result.flatten() if result.ndim > 1 else result

    def gradxinput_attrib(self, X_last: torch.Tensor, eps: float = 0.0):
        """
        ê¸°ëŠ¥: GradÃ—Inputìœ¼ë¡œ (ì‹œê°„, í”¼ì²˜) ê¸°ì—¬ë„ ì‚°ì¶œ.
        ì…ë ¥: X_last(torch.Tensor):(1,T,F), eps(float): ì…ë ¥ ë…¸ì´ì¦ˆ ì•ˆì •í™”
        ì¶œë ¥: (per_time(T,), per_feat(F,), gi(T,F)) ê° np.ndarray
        ì£¼ì˜: eval ëª¨ë“œ, y.sum()ì— ëŒ€í•´ ì—­ì „íŒŒ.
        """
        self.eval()
        x = X_last.clone().detach().to(next(self.parameters()).device)

        if eps > 0:
            x = x + eps * torch.randn_like(x)
        x.requires_grad_(True)
        y = self(x).sum()
        self.zero_grad(set_to_none=True)
        y.backward()
        gi = (x.grad * x).abs()[0].detach().cpu().numpy()  # (T,F)
        per_time = gi.sum(axis=1)
        per_feat = gi.mean(axis=0)
        return per_time, per_feat, gi

    @torch.no_grad()
    def occlusion_time(self, X_last: torch.Tensor, fill: str = "zero", batch: int = 32):
        """
        ê¸°ëŠ¥: í•œ ì‹œì ì”© ê°€ë ¤ Î”ì˜ˆì¸¡ìœ¼ë¡œ ì‹œê°„ ì¤‘ìš”ë„ ê³„ì‚°.
        ì…ë ¥: X_last(1,T,F), fill: 'zero' ë˜ëŠ” í‰ê· ì¹˜ ëŒ€ì²´, batch: ë°°ì¹˜ í¬ê¸°
        ì¶œë ¥: ì •ê·œí™”ëœ ì‹œê°„ ì¤‘ìš”ë„ np.ndarray (T,)
        ë³µì¡ë„: O(T) ì „í–¥ íŒ¨ìŠ¤(ë°°ì¹˜ ì²˜ë¦¬)
        """
        self.eval()
        base = float(self(X_last).item())
        _, T, F = X_last.shape
        Xs = []
        for t in range(T):
            x = X_last.clone()
            if fill == "zero":
                x[:, t, :] = 0
            else:
                x[:, t, :] = X_last.mean(dim=1, keepdim=True)[:, 0, :]
            Xs.append(x)
        deltas = []
        for i in range(0, T, batch):
            xb = torch.cat(Xs[i:i+batch], dim=0)
            yb = self(xb).flatten().cpu().numpy()
            deltas.extend(np.abs(yb - base).tolist())
        s = sum(deltas)
        return np.array([v/s if s > 0 else 1.0/T for v in deltas], dtype=float)

    @torch.no_grad()
    def occlusion_feature(self, X_last: torch.Tensor, fill: str = "zero", batch: int = 32):
        """
        ê¸°ëŠ¥: í•œ í”¼ì²˜ì”© ê°€ë ¤ Î”ì˜ˆì¸¡ìœ¼ë¡œ í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚°.
        ì…ë ¥: X_last(1,T,F), fill: 'zero' ë˜ëŠ” í‰ê· ì¹˜ ëŒ€ì²´, batch: ë°°ì¹˜ í¬ê¸°
        ì¶œë ¥: ì •ê·œí™”ëœ í”¼ì²˜ ì¤‘ìš”ë„ np.ndarray (F,)
        ë³µì¡ë„: O(F) ì „í–¥ íŒ¨ìŠ¤(ë°°ì¹˜ ì²˜ë¦¬)
        """
        self.eval()
        base = float(self(X_last).item())
        _, T, F = X_last.shape
        Xs = []
        for f in range(F):
            x = X_last.clone()
            if fill == "zero":
                x[:, :, f] = 0
            else:
                x[:, :, f] = X_last.mean(dim=(1, 2), keepdim=True)[:, 0, 0]
            Xs.append(x)
        deltas = []
        for i in range(0, F, batch):
            xb = torch.cat(Xs[i:i+batch], dim=0)
            yb = self(xb).flatten().cpu().numpy()
            deltas.extend(np.abs(yb - base).tolist())
        s = sum(deltas)
        return np.array([v/s if s > 0 else 1.0/F for v in deltas], dtype=float)

    def explain_last(
        self,
        X_last: torch.Tensor,
        dates: list | None = None,
        top_k: int = 5,
        use_shap: bool = True, # ê¸°ë³¸ì€ ë¹ ë¥´ê²Œ off, í•„ìš”ì‹œ true
        shap_weight_time: float = 0.20,      # ì‹œê°„ ì¤‘ìš”ë„ì—ì„œ SHAP ê°€ì¤‘ì¹˜(ì„ì˜ì„¤ì •)
        shap_weight_feat: float = 0.30       # í”¼ì²˜ ì¤‘ìš”ë„ì—ì„œ SHAP ê°€ì¤‘ì¹˜(ì„ì˜ì„¤ì •)
        ):
        """
        ê¸°ëŠ¥: Attention + GradÃ—Input + Occlusion ìœµí•©ìœ¼ë¡œ ìµœì‹  ìœˆë„ìš° ì„¤ëª… íŒ¨í‚· ìƒì„±.
        ì…ë ¥: X_last(1,T,F), dates(list|None), top_k(int)
        ì²˜ë¦¬:
        1) í•™ìŠµ ìŠ¤ì¼€ì¼ ì •í•© â†’ í…ì„œí™”
        2) time-attention, GradÃ—Input, Occlusion ê³„ì‚°
        3) ì •ê·œí™”Â·ê°€ì¤‘ í‰ê· ìœ¼ë¡œ per_time/per_feature ì‚°ì¶œ
        4) ë‚ ì§œë³„ ìƒìœ„ í”¼ì²˜(top_k)ì™€ ì¦ê±° ë²¡í„°(evidence) í¬í•¨
        ì¶œë ¥: dict
        - per_time: [{date,sum_abs}]
        - per_feature: [{feature,sum_abs}]
        - time_attention: {date:weight}
        - time_feature: {date:{feat:score}}
        - evidence: ì›ì²œ ì§€í‘œë“¤(attention, gradxinput, occlusion)
        - raw: GradÃ—Input ì›ì‹œ(TÃ—F)
        """
        # ìŠ¤ì¼€ì¼ ì •í•©
        device = next(self.parameters()).device
        X_np = X_last.detach().cpu().numpy()
        X_scaled = self._scale_like_train(X_np)
        Xs = torch.tensor(X_scaled, dtype=torch.float32, device=device)

        T, F = Xs.shape[1], Xs.shape[2]
        feat_cols_src = getattr(self.stockdata, "feature_cols", [])
        feat_names = self._safe_names(feat_cols_src, F)
        if dates is None:
            dates = getattr(self.stockdata, f"{self.agent_id}_dates", [])
        dates = self._safe_dates(dates, T)

        # ì‹œê°„ ì¤‘ìš”ë„
        time_attn = self.time_importance_from_attention(Xs)  # (T,)

        # GradxInput
        g_time, g_feat, gi_raw = self.gradxinput_attrib(Xs, eps=0.0) # (T,), (F,)

        # Occlusion
        occ_time = self.occlusion_time(Xs, fill="zero", batch=32) # (T,)
        occ_feat = self.occlusion_feature(Xs, fill="zero", batch=32) # (F,)

        # ì •ê·œí™” ë° ìœµí•©
        g_time_n = g_time / (g_time.sum() + 1e-12)
        g_feat_n = g_feat / (g_feat.sum() + 1e-12)
        occ_feat_n = occ_feat / (occ_feat.sum() + 1e-12)

        # (ì˜µì…˜) SHAP ì¶”ê°€
        shap_time = None
        shap_feat = None
        shap_used = False
        if use_shap:
            try:
                shap_res = self.shap_last(Xs, background_k=64)  # ì•„ë˜ì— ì •ì˜
                shap_time = shap_res["per_time"]    # (T,) í•©=1
                shap_feat = shap_res["per_feature"] # (F,) í•©=1
                shap_used = True
            except Exception as _:
                shap_time = None
                shap_feat = None
                shap_used = False  # ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ê¸°ë³¸ 3ìš”ì†Œë¡œ ì§„í–‰

        # ìœµí•© ê°€ì¤‘ì¹˜(ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±, í•„ìš”ì‹œ ê²€ì¦ì…‹ì—ì„œ í•™ìŠµ ê°€ëŠ¥)
        if shap_time is not None and shap_feat is not None:
            # ì‹œê°„ ì¤‘ìš”ë„: attn 0.4, GI 0.25, occ 0.15, shap (ì¸ì) â†’ í•© 1ë¡œ ì¬ì •ê·œí™”
            w_time = np.array([0.4, 0.25, 0.15, float(shap_weight_time)], dtype=float)
            w_time = w_time / w_time.sum()
            per_time = (
                w_time[0]*time_attn +
                w_time[1]*g_time_n +
                w_time[2]*occ_time +
                w_time[3]*shap_time
            )
            # í”¼ì²˜ ì¤‘ìš”ë„: GI 0.5, occ 0.2, shap (ì¸ì) â†’ í•© 1ë¡œ ì¬ì •ê·œí™”
            w_feat = np.array([0.5, 0.2, float(shap_weight_feat)], dtype=float)
            w_feat = w_feat / w_feat.sum()
            per_feat = (
                w_feat[0]*g_feat_n +
                w_feat[1]*occ_feat_n +
                w_feat[2]*shap_feat
            )
        else:
            # SHAP ë¯¸ì‚¬ìš©/ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ê³ ì • ë¹„ìœ¨
            per_time = 0.5 * time_attn + 0.3 * g_time_n + 0.2 * occ_time
            per_feat = 0.7 * g_feat_n   + 0.3 * occ_feat_n

        # per_timeê³¼ per_featì´ 1ì°¨ì›ì¸ì§€ í™•ì¸í•˜ê³  ë³€í™˜
        per_time = per_time.flatten() if per_time.ndim > 1 else per_time
        per_feat = per_feat.flatten() if per_feat.ndim > 1 else per_feat

        # ë‚ ì§œë³„ ìƒìœ„ í”¼ì²˜(GradÃ—Input ê¸°ì¤€ìœ¼ë¡œ ê°„ë‹¨)
        gi_abs = np.abs(gi_raw)
        time_feature = {}
        for t_idx, d in enumerate(dates):
            pairs = sorted(
                zip(feat_names, gi_abs[t_idx].tolist()),
                key=lambda z: z[1], reverse=True
            )[:top_k]
            time_feature[str(d)] = {k: float(v) for k, v in pairs}

        # ê²°ê³¼ íŒ¨í‚·
        time_attention = {str(d): r4(w) for d, w in zip(dates, time_attn.tolist())}
        per_time_list = [{"date": str(d), "sum_abs": r4(v)} for d, v in zip(dates, per_time.tolist())]
        per_feat_list = [{"feature": k, "sum_abs": r4(v)} for k, v in sorted(zip(feat_names, per_feat.tolist()),
                                                                          key=lambda z: z[1], reverse=True)]

        evidence = {
            "attention": [r4(x) for x in time_attn.tolist()],
            "gradxinput_feat": [r4(x) for x in g_feat.tolist()],
            "occlusion_time": [r4(x) for x in occ_time.tolist()],
            "window_size": int(T),
            "shap_used": bool(shap_used)
            }

        return {
            "per_time": per_time_list,
            "per_feature": per_feat_list,
            "time_attention": time_attention,
            "time_feature": time_feature,
            "evidence": evidence,
            "raw": {"gradxinput": gi_abs.tolist()}  # ì›ì‹œê°’ì€ ë¹„ë¼ìš´ë”© ìœ ì§€ ê°€ëŠ¥
          }

    # ---------------- SHAP ë³´ì¡°: ë°°ê²½ ìƒ˜í”Œ ì¶”ì¶œ ----------------
    def _background_windows(self, k: int = 64):
        """
        í•™ìŠµ/ê²€ì¦ êµ¬ê°„ì—ì„œ ìœˆë„ìš° kê°œë¥¼ ê· ë“± ê°„ê²©ìœ¼ë¡œ ë½‘ì•„ ë°°ê²½ìœ¼ë¡œ ì‚¬ìš©.
        íŒŒì¼ ìƒë‹¨ ìˆ˜ì • ì—†ì´ ë‚´ë¶€ì—ì„œ lazy import.
        """
        try:
            from core.technical_classes.technical_data_set import load_dataset  
            X, _, _, _ = load_dataset(self.ticker, agent_id=self.agent_id, save_dir=self.data_dir)
            if len(X) <= 1:
                return None
            k = min(int(k), len(X) - 1)
            idx = np.linspace(0, len(X) - 2, num=k, dtype=int)
            X_bg = X[idx]
            X_bg_scaled, _ = self.scaler.transform(X_bg)
            dev = next(self.parameters()).device
            return torch.tensor(X_bg_scaled, dtype=torch.float32, device=dev)
        except Exception:
            return None

    # ---------------- SHAP ê³„ì‚°(GradientExplainer) ----------------
    #@torch.no_grad()
    def shap_last(self, X_last: torch.Tensor, background_k: int = 64):
        """
        GradientExplainerë¡œ SHAP ê°’ì„ 1ê°œ ìœˆë„ìš°(1,T,F)ì— ëŒ€í•´ ê³„ì‚°.
        ë°˜í™˜: {"per_time": (T,), "per_feature": (F,)}
        """
        try:
            import shap  # lazy import (ìƒë‹¨ ìˆ˜ì • ë¶ˆí•„ìš”)
        except Exception as e:
            raise RuntimeError("shap ë¯¸ì„¤ì¹˜ ë˜ëŠ” ë¡œë“œ ì‹¤íŒ¨: pip install shap==0.45.0") from e

        self.eval()

        # ë°°ê²½ êµ¬ì„±(ì—†ìœ¼ë©´ í˜„ì¬ ì…ë ¥ ë³µì œ)
        X_bg = self._background_windows(k=background_k)
        if X_bg is None:
            X_bg = X_last.repeat(32, 1, 1)

        # ì…ë ¥ ìŠ¤ì¼€ì¼ ë§ì¶”ê¸°
        X_np = X_last.detach().cpu().numpy()
        X_scaled = self._scale_like_train(X_np)
        X_in = torch.tensor(X_scaled, dtype=torch.float32, device=next(self.parameters()).device)
        X_in.requires_grad_(True)  # ì¶”ê°€(shap ìš©)

        # PyTorch ëª¨ë¸ ì§ì ‘ ì „ë‹¬
        explainer = shap.GradientExplainer(self, X_bg)
        sv = explainer.shap_values(X_in)  # np.ndarray ë˜ëŠ” list

        if isinstance(sv, list):
            sv = sv[0]
        if sv.ndim == 2:  # (T,F) â†’ (1,T,F) í˜¸í™˜
            sv = sv[None, ...]

        sv_abs = np.abs(sv)          # (1,T,F)
        per_time = sv_abs.sum(axis=2)[0]      # (T,)
        per_feat = sv_abs.mean(axis=1)[0]     # (F,)

        # ì •ê·œí™”(í•©=1)
        per_time = per_time / (per_time.sum() + 1e-12)
        per_feat = per_feat / (per_feat.sum() + 1e-12)
        return {"per_time": per_time, "per_feature": per_feat}




    # -----------------------------------------------------------
    # ì•„ì´ë””ì–´ ì••ì¶•(LLM í† í° ì ˆì•½ìš©)
    # -----------------------------------------------------------
    @staticmethod
    def _pack_idea(exp: dict, top_time=8, top_feat=6, coverage=0.8):
        """ìƒìœ„ ì‹œê°„, í”¼ì²˜ë§Œ ì••ì¶•, ì»¤ë²„ë¦¬ì§€ 80%ê¹Œì§€ ëˆ„ì """
        per_time = sorted(exp["per_time"], key=lambda z: z["sum_abs"], reverse=True)
        total = sum(z["sum_abs"] for z in per_time) or 1.0
        acc, picked = 0.0, []
        for z in per_time:
            acc += z["sum_abs"]
            picked.append({"date": z["date"], "weight": r4(z["sum_abs"]/total)})
            if acc/total >= coverage or len(picked) >= top_time:
                break

        per_feat = sorted(exp["per_feature"], key=lambda z: z["sum_abs"], reverse=True)[:top_feat]
        top_features = [{"feature": f["feature"], "weight": r4(f["sum_abs"])} for f in per_feat]
        peak = picked[0]["date"] if picked else None
        return {
            "top_time": picked,
            "top_features": top_features,
            "peak_date": peak,
            "window_size": exp.get("evidence",{}).get("window_size")}


    # -----------------------------------------------------------
    # ctx ìƒì„±ìš© í—¬í¼ ë¸”ë¡ë“¤
    # -----------------------------------------------------------

    # LLM Reasoning ë©”ì‹œì§€ (ì•„ì—°ìˆ˜ì •)

    def _build_messages_opinion(self, stock_data, target):
        """TechnicalAgentìš© LLM í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ êµ¬ì„± + ì„¤ëª…ê°’ í¬í•¨"""
        last = float(getattr(stock_data, "last_price", target.next_close))

        # ìµœì‹  ìœˆë„ìš° ì„¤ëª… ì‚°ì¶œ
        X_last = self.searcher(self.ticker)
        if not isinstance(X_last, torch.Tensor):
          X_last = torch.tensor(X_last, dtype=torch.float32)
        T = X_last.shape[1]
        # dates ìˆ˜ì •
        dates = getattr(self.stockdata, f"{self.agent_id}_dates", [])

        exp = self.explain_last(X_last, dates, top_k=5, use_shap=True)
        idea = self._pack_idea(exp)  # í•­ìƒ ìƒˆë¡œ ê³„ì‚°
        self._last_idea = idea  # ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥ (TechnicalAgent ì „ìš©)

        # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸
        ctx = {
            "ticker": getattr(stock_data, "ticker", "Unknown"),
            "last_price": r4(last),
            "next_close": r4(target.next_close),
            "uncertainty": r4(target.uncertainty),
            "confidence": r4(target.confidence),
            "sigma": r4(target.uncertainty or 0.0),
            "beta": r4(target.confidence or 0.0),
            "window_size": int(self.window_size),
            "idea": idea,  # í•µì‹¬ë§Œ
            # "evidence": exp.get("evidence", {})
        }

        system_text = OPINION_PROMPTS[self.agent_id]["system"]
        tmpl = OPINION_PROMPTS[self.agent_id]["user"]
        user_text = tmpl.replace("{context}", json.dumps(ctx, ensure_ascii=False))
        return system_text, user_text


    def _build_messages_rebuttal(self,
                                my_opinion: Opinion,
                                target_opinion: Opinion,
                                stock_data: StockData) -> tuple[str, str]:

        t = stock_data.ticker or "UNKNOWN"
        ccy = (stock_data.currency or "USD").upper()
        agent_data = getattr(stock_data, self.agent_id, None)
        if not agent_data or not isinstance(agent_data, dict):
            raise ValueError(f"{self.agent_id} ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜: dictí˜• ì»¬ëŸ¼ ë°ì´í„°ê°€ í•„ìš”í•¨")

        ctx = {
            "ticker": t,
            "currency": ccy,
            "data_summary": getattr(stock_data, "feature_cols", []), # ìˆ˜ì •
            "me": {
                "agent_id": self.agent_id,
                "next_close": float(my_opinion.target.next_close),
                "reason": str(my_opinion.reason)[:2000],
                "uncertainty": float(my_opinion.target.uncertainty),
                "confidence": float(my_opinion.target.confidence),
            },
            "other": {
                "agent_id": target_opinion.agent_id,
                "next_close": float(target_opinion.target.next_close),
                "reason": str(target_opinion.reason)[:2000],
                "uncertainty": float(target_opinion.target.uncertainty),
                "confidence": float(target_opinion.target.confidence),
            }
        }
        # ê° ì»¬ëŸ¼ë³„ ìµœê·¼ ì‹œê³„ì—´ ê·¸ëŒ€ë¡œ í¬í•¨
    
        for col, values in agent_data.items():
            if isinstance(values, (list, tuple)):
                ctx[col] = values[-self.window_size:] # ìˆ˜ì •
            else:
                ctx[col] = [values]

        # ì•„ì—° ìˆ˜ì •
        system_text = REBUTTAL_PROMPTS[self.agent_id]["system"]
        tmpl = REBUTTAL_PROMPTS[self.agent_id]["user"]
        user_text = tmpl.replace("{context}", json.dumps(ctx, ensure_ascii=False))
    
        return system_text, user_text


    def _build_messages_revision(
        self,
        my_opinion: Opinion,
        others: List[Opinion],
        rebuttals: Optional[List[Rebuttal]] = None,
        stock_data: StockData = None,
    ) -> tuple[str, str]:
        """
        Revisionìš© LLM ë©”ì‹œì§€ ìƒì„±ê¸°
        - ë‚´ ì˜ê²¬(my_opinion), íƒ€ ì—ì´ì „íŠ¸ ì˜ê²¬(others), ì£¼ê°€ë°ì´í„°(stock_data) ê¸°ë°˜
        - rebuttals ì¤‘ ë‚˜(self.agent_id)ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ ë‚´ìš©ë§Œ í¬í•¨
        """
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
        t = getattr(stock_data, "ticker", "UNKNOWN")
        ccy = getattr(stock_data, "currency", "USD").upper()
        agent_data = getattr(stock_data, self.agent_id, None)
        if not agent_data or not isinstance(agent_data, dict):
            raise ValueError(f"{self.agent_id} ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜: dictí˜• ì»¬ëŸ¼ ë°ì´í„°ê°€ í•„ìš”í•¨")

        # íƒ€ ì—ì´ì „íŠ¸ ì˜ê²¬ ë° rebuttal í†µí•© ìš”ì•½
        others_summary = []
        for o in others:
            entry = {
                "agent_id": o.agent_id,
                "predicted_price": float(o.target.next_close),
                "confidence": float(o.target.confidence),
                "uncertainty": float(o.target.uncertainty),
                "reason": str(o.reason)[:500],
            }

            # ë‚˜ì—ê²Œ ì˜¨ rebuttalë§Œ stance/message ì¶”ì¶œ
            if rebuttals:
                related_rebuts = [
                    {"stance": r.stance, "message": r.message}
                    for r in rebuttals
                    if r.from_agent_id == o.agent_id and r.to_agent_id == self.agent_id
                ]
                if related_rebuts:
                    entry["rebuttals_to_me"] = related_rebuts

            others_summary.append(entry)
            

        # Context êµ¬ì„±
        ctx = {
            "ticker": t,
            "currency": ccy,
            "agent_type": self.agent_id,
            "my_opinion": {
                "predicted_price": float(my_opinion.target.next_close),
                "confidence": float(my_opinion.target.confidence),
                "uncertainty": float(my_opinion.target.uncertainty),
                "reason": str(my_opinion.reason)[:1000],
            },
            "others_summary": others_summary,
            "data_summary": getattr(stock_data, "feature_cols", []), # ìˆ˜ì •
        }

        # ìµœê·¼ ì‹œê³„ì—´ ë°ì´í„° í¬í•¨ (ê¸°ìˆ /ì‹¬ë¦¬ì  íŒ¨í„´)
        for col, values in agent_data.items():
            if isinstance(values, (list, tuple)):
                ctx[col] = values[-14:]  # ìµœê·¼ 14ì¼ì¹˜
            else:
                ctx[col] = [values]

        # Prompt êµ¬ì„±
        prompt_set = REVISION_PROMPTS.get(self.agent_id)
        system_text = prompt_set["system"]
        user_text = prompt_set["user"].format(context=json.dumps(ctx, ensure_ascii=False, indent=2))

        return system_text, user_text

    # ===============================================================
    # TechnicalAgent ì „ìš© ë©”ì„œë“œë“¤ (TechnicalBaseAgentì—ì„œ ì´ë™)
    # ===============================================================

    def searcher(self, ticker: Optional[str] = None, rebuild: bool = False):
        """TechnicalAgent ì „ìš© searcher - technical_data_set ì‚¬ìš©"""
        agent_id = self.agent_id
        ticker = ticker or self.ticker
        self.ticker = ticker
        
        dataset_path = os.path.join(self.data_dir, f"{ticker}_{agent_id}_dataset.csv")
        cfg = agents_info.get(self.agent_id, {}) 

        need_build = rebuild or (not os.path.exists(dataset_path))
        if need_build:
            print(f"âš™ï¸ {ticker} {agent_id} dataset not found. Building new dataset..." if not os.path.exists(dataset_path) else f"âš™ï¸ {ticker} {agent_id} rebuild requested. Building dataset...")
            build_dataset_tech(
                ticker=ticker,
                save_dir=self.data_dir,
                period=cfg.get("period", "5y"),
                interval=cfg.get("interval", "1d"),
            )
    
        # CSV ë¡œë“œ
        X, y, feature_cols, dates_all = load_dataset_tech(
            ticker, agent_id=agent_id, save_dir=self.data_dir
            )

        # ìµœê·¼ window
        X_latest = X[-1:]

        # StockData êµ¬ì„±
        self.stockdata = StockData(ticker=ticker)
        self.stockdata.feature_cols = feature_cols
        
        # dates_all êµ¬ì¡°ì— ë”°ë¼ "ë§ˆì§€ë§‰ ìœˆë„ìš°" ë‚ ì§œë§Œ ì¶”ì¶œ
        if dates_all:
            if isinstance(dates_all[0], (list, tuple)):
                # dates_all: [ [ìœˆë„ìš°1ì˜ Tê°œ ë‚ ì§œ], [ìœˆë„ìš°2ì˜ Tê°œ ë‚ ì§œ], ... ]
                last_dates = dates_all[-1]
            else:
                # ë§Œì•½ 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¼ë©´, ë’¤ì—ì„œ window_sizeë§Œí¼ ì‚¬ìš©
                win = int(self.window_size)
                last_dates = dates_all[-win:]
        else:
            last_dates = []
        
        
        # ì „ì²´ëŠ” *_dates_allë¡œ, ë§ˆì§€ë§‰ ìœˆë„ìš°ëŠ” *_datesë¡œ ì €ì¥
        setattr(self.stockdata, f"{agent_id}_dates_all", dates_all or [])
        setattr(self.stockdata, f"{agent_id}_dates",     last_dates or [])
        
        # last_price ì•ˆì „ ë³€í™˜
        try:
            data = yf.download(ticker, period="5y", interval="1d", auto_adjust=True, progress=False)
            if data is not None and not data.empty:
                last_val = data["Close"].iloc[-1]
                self.stockdata.last_price = float(last_val.item() if hasattr(last_val, "item") else last_val)
            else:
                self.stockdata.last_price = None
        except Exception:
            self.stockdata.last_price = None

        # í†µí™”ì½”ë“œ
        try:
            self.stockdata.currency = yf.Ticker(ticker).info.get("currency", "USD")
        except Exception:
            self.stockdata.currency = "USD"

        df_latest = pd.DataFrame(X_latest[0], columns=feature_cols)  # (T, F)
        feature_dict = {col: df_latest[col].tolist() for col in df_latest.columns}
        setattr(self.stockdata, agent_id, feature_dict)

        # StockData ìƒì„± ì™„ë£Œ (ë¡œê·¸ëŠ” DebateAgentì—ì„œ ì²˜ë¦¬)

        return torch.tensor(X_latest, dtype=torch.float32)

    def pretrain(self):
        """Agentë³„ ì‚¬ì „í•™ìŠµ ë£¨í‹´ (ëª¨ë¸ ìƒì„±, í•™ìŠµ, ì €ì¥, self.model ì—°ê²°ê¹Œì§€ í¬í•¨)"""
        epochs = agents_info[self.agent_id]["epochs"]
        lr = agents_info[self.agent_id]["learning_rate"]
        batch_size = agents_info[self.agent_id]["batch_size"]

        # ë°ì´í„° ë¡œë“œ
        X, y, cols, _ = load_dataset_tech(self.ticker, self.agent_id, save_dir=self.data_dir)
        print(f"[{datetime.now().strftime('%H:%M:%S')}] Pretraining {self.agent_id}")

        split_idx = int(len(X) * 0.8)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]

        # íƒ€ê¹ƒ ìŠ¤ì¼€ì¼ ì¡°ì • - ìƒìŠ¹/í•˜ë½ìœ¨ì„ 100ë°°ë¡œ ìŠ¤ì¼€ì¼ë§
        y_train *= 100.0
        y_val   *= 100.0

        self.scaler.fit_scalers(X_train, y_train)
        self.scaler.save(self.ticker)

        X_train, y_train = map(torch.tensor, self.scaler.transform(X_train, y_train))
        X_train, y_train = X_train.float(), y_train.float()

        # ëª¨ë¸ = self (nn.Module)
        model = self
        # í˜¹ì‹œ ì˜ˆì „ì— ì˜ëª» ë“±ë¡ëœ submodule "model"ì´ ìˆìœ¼ë©´ ì œê±°
        self._modules.pop("model", None)

        # í•™ìŠµ
        model.train()

        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        loss_fn = torch.nn.HuberLoss(delta=1.0)

        train_loader = DataLoader(TensorDataset(X_train, y_train.view(-1, 1)),
                                  batch_size=batch_size, shuffle=True)

        # í•™ìŠµ ë£¨í”„
        for epoch in range(epochs):
            total_loss = 0.0
            for Xb, yb in train_loader:
                y_pred = model(Xb)
                loss = loss_fn(y_pred, yb)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            if (epoch + 1) % 5 == 0:
                print(f"  Epoch {epoch+1:03d} | Loss: {total_loss/len(train_loader):.6f}")

        # ëª¨ë¸ ì €ì¥ ë° ì—°ê²°
        os.makedirs(self.model_dir, exist_ok=True)
        model_path = os.path.join(self.model_dir, f"{self.ticker}_{self.agent_id}.pt")
        torch.save({"model_state_dict": model.state_dict()}, model_path)

        print(f" {self.agent_id} ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {model_path}")

    def predict(self, X, n_samples: int = 30, current_price: float = None, X_last: np.ndarray = None):
        """
        Monte Carlo Dropout ê¸°ë°˜ ì˜ˆì¸¡ + ë¶ˆí™•ì‹¤ì„±(Ïƒ) ë° confidence ê³„ì‚° (ì•ˆì •í˜•)
        """
        # 1) ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì¤€ë¹„
        model = self  # TechnicalAgent ìì²´ê°€ nn.Module
        self.scaler.load(self.ticker)

        # 2) ì…ë ¥ ë³€í™˜ + í•™ìŠµê³¼ ë™ì¼ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        if isinstance(X, np.ndarray):
            X_raw_np = X.copy()
        elif isinstance(X, torch.Tensor):
            X_raw_np = X.detach().cpu().numpy().copy()
        else:
            raise TypeError(f"Unsupported input type: {type(X)}")

        X_scaled, _ = self.scaler.transform(X_raw_np)
        device = next(model.parameters()).device
        X_tensor = torch.tensor(X_scaled, dtype=torch.float32, device=device)

        # 3) Monte Carlo Dropout ì¶”ë¡ 
        model.train()  # dropout í™œì„±í™”
        preds = []
        with torch.no_grad():
            for _ in range(n_samples):
                y_pred = model(X_tensor).cpu().numpy().flatten()
                preds.append(y_pred)

        preds = np.stack(preds)              # (n_samples, seq_len or 1)
        mean_pred = preds.mean(axis=0)       # (seq_len,)
        std_pred = np.abs(preds.std(axis=0)) # í•­ìƒ ì–‘ìˆ˜

        # 4) Ïƒ ê¸°ë°˜ confidence ê³„ì‚°
        sigma = float(std_pred[-1])
        sigma = max(sigma, 1e-6)
        confidence = 1 / (1 + np.log1p(sigma))

        # 5) íƒ€ê¹ƒ ì—­ìŠ¤ì¼€ì¼ë§ ë° ê°€ê²© ë³€í™˜
        if hasattr(self.scaler, "y_scaler") and self.scaler.y_scaler is not None:
            mean_pred = self.scaler.inverse_y(mean_pred)
            std_pred = self.scaler.inverse_y(std_pred)

        # current_price ê²°ì •
        if current_price is None:
            last_price = getattr(getattr(self, "stockdata", None), "last_price", None)
            current_price = 100.0 if last_price is None else last_price

        # í•™ìŠµ íƒ€ê¹ƒì€ "ë‹¤ìŒë‚  ìˆ˜ìµë¥ (%)"ì´ë¯€ë¡œ 100ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì‚¬ìš©
        predicted_return = float(mean_pred[-1]) / 100.0
        predicted_price = current_price * (1 + predicted_return)

        # 6) Target ìƒì„±
        target = Target(
            next_close=float(predicted_price),
            uncertainty=sigma,
            confidence=float(confidence),
        )
        return target

    def reviewer_draft(self, stock_data: StockData = None, target: Target = None) -> Opinion:
        """(1) searcher â†’ (2) predicter â†’ (3) LLM(JSON Schema)ë¡œ reason ìƒì„± â†’ Opinion ë°˜í™˜"""

        # 1) ë°ì´í„° ìˆ˜ì§‘
        if stock_data is not None:
            self.stockdata = stock_data
        else:
        # ë‚´ë¶€ì— ì—†ìœ¼ë©´ searcher í•œ ë²ˆ ëŒë ¤ì„œ ë§Œë“ ë‹¤
            if getattr(self, "stockdata", None) is None:
                if not self.ticker:
                    raise RuntimeError(
                        f"[{self.agent_id}] tickerê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                        "reviewer_draft í˜¸ì¶œ ì „ì— tickerë¥¼ ì§€ì •í•˜ê±°ë‚˜ searcher(ticker)ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
                    )
                _ = self.searcher(self.ticker)  # self.stockdata ì„¸íŒ…
            stock_data = self.stockdata

        # 2) ì˜ˆì¸¡ê°’ ìƒì„±
        if target is None:
            # searcherëŠ” ìœ„ì—ì„œ í•œ ë²ˆ ëŒì•˜ìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ìµœì‹  ìœˆë„ìš°ë¡œ predictë§Œ ìˆ˜í–‰
            X_input = self.searcher(self.ticker)              # (1,T,F)
            target = self.predict(X_input)

        # 3) LLM í˜¸ì¶œ(reason ìƒì„±) - ì „ë‹¬ë°›ì€ stock_data ì‚¬ìš©
        sys_text, user_text = self._build_messages_opinion(self.stockdata, target)

        parsed = self._ask_with_fallback(
            self._msg("system", sys_text),
            self._msg("user", user_text),
            {
                "type": "object", 
                "properties": {"reason": {"type": "string"}}, 
                "required": ["reason"], 
                "additionalProperties": False}
        )

        reason = parsed.get("reason", "(ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨)")

        # 4) Opinion ê¸°ë¡/ë°˜í™˜ (í•­ìƒ ìµœì‹  ê°’ append)
        self.opinions.append(Opinion(
                    agent_id=self.agent_id, 
                    target=target, 
                    reason=reason))

        # ìµœì‹  ì˜¤í”¼ë‹ˆì–¸ ë°˜í™˜
        return self.opinions[-1]

    def reviewer_rebut(self, my_opinion: Opinion, other_opinion: Opinion, round: int) -> Rebuttal:
        """LLMì„ í†µí•´ ìƒëŒ€ ì˜ê²¬ì— ëŒ€í•œ ë°˜ë°•/ì§€ì§€ ìƒì„±"""

        # ë©”ì‹œì§€ ìƒì„± (context êµ¬ì„±ì€ ë³„ë„ í—¬í¼ì—ì„œ)
        sys_text, user_text = self._build_messages_rebuttal(
            my_opinion=my_opinion,
            target_opinion=other_opinion,
            stock_data=self.stockdata
        )

        # LLM í˜¸ì¶œ
        parsed = self._ask_with_fallback(
            self._msg("system", sys_text),
            self._msg("user", user_text),
            {
                "type": "object",
                "properties": {
                    "stance": {"type": "string", "enum": ["REBUT", "SUPPORT"]},
                    "message": {"type": "string"}
                },
                "required": ["stance", "message"],
                "additionalProperties": False
            }
        )

        # ê²°ê³¼ ì •ë¦¬ ë° ê¸°ë¡
        result = Rebuttal(
            from_agent_id=my_opinion.agent_id,
            to_agent_id=other_opinion.agent_id,
            stance=parsed.get("stance", "REBUT"),
            message=parsed.get("message", "(ë°˜ë°•/ì§€ì§€ ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨)")
        )

        # ì €ì¥
        self.rebuttals[round].append(result)

        # ë””ë²„ê¹… ë¡œê·¸
        if self.verbose:
            print(
                f"[{self.agent_id}] rebuttal ìƒì„± â†’ {result.stance} "
                f"({my_opinion.agent_id} â†’ {other_opinion.agent_id})"
            )

        return result
    
    # DebateAgent.get_rebuttal() í˜¸í™˜ìš© ë˜í¼
    def reviewer_rebuttal(
        self,
        my_opinion: Opinion,
        other_opinion: Opinion,
        round_index: int,
    ) -> Rebuttal:
        return self.reviewer_rebut(
            my_opinion=my_opinion,
            other_opinion=other_opinion,
            round=round_index,
        )

    def reviewer_revise(
        self,
        my_opinion: Opinion,
        others: List[Opinion],
        rebuttals: List[Rebuttal],
        stock_data: StockData,
        fine_tune: bool = True,
        lr: float = 1e-4,
        epochs: int = 20,
    ):
        """
        Revision ë‹¨ê³„
        - Ïƒ ê¸°ë°˜ Î²-weighted ì‹ ë¢°ë„ ê³„ì‚°
        - Î³ ìˆ˜ë ´ìœ¨ë¡œ ì˜ˆì¸¡ê°’ ë³´ì •
        - fine-tuning (ìˆ˜ìµë¥  ë‹¨ìœ„)
        - reasoning ìƒì„±
        """
        gamma = getattr(self, "gamma", 0.3)               # ìˆ˜ë ´ìœ¨ (0~1)
        delta_limit = getattr(self, "delta_limit", 0.05)  # fine-tuning ë³´ì • í•œê³„
        current_price = getattr(self.stockdata, "last_price", 100.0)  # ìˆ˜ì •: í•­ìƒ ì´ˆë°˜ì— í˜„ì¬ê°€ í™•ë³´

        try:
            # Î² ê³„ì‚° (ë¶ˆí™•ì‹¤ì„± ì‘ì„ìˆ˜ë¡ ì‹ ë¢° ë†’ìŒ)
            my_price = float(my_opinion.target.next_close)           # ìˆ˜ì •: float ìºìŠ¤íŒ…
            my_sigma = abs(my_opinion.target.uncertainty or 1e-6)

            # ìˆ˜ì •: othersê°€ ì—†ì„ ë•Œ ë°©ì–´
            if len(others) == 0:
                revised_price = my_price
            else:
                other_prices = np.array([o.target.next_close for o in others], dtype=float)
                other_sigmas = np.array([abs(o.target.uncertainty or 1e-6) for o in others], dtype=float)

                all_sigmas = np.concatenate([[my_sigma], other_sigmas])

                inv_sigmas = 1 / (all_sigmas + 1e-6)
                betas = inv_sigmas / inv_sigmas.sum()

                # ë…¼ë¬¸ì‹ ìˆ˜ë ´ ì—…ë°ì´íŠ¸
                # y_i_rev = y_i + Î³ Î£ Î²_j (y_j - y_i)
                delta = np.sum(betas[1:] * (other_prices - my_price))
                revised_price = my_price + gamma * delta

        except Exception as e:
            print(f"[{self.agent_id}] revised_target ê³„ì‚° ì‹¤íŒ¨: {e}")
            revised_price = my_opinion.target.next_close  # ìˆ˜ì •: ì—¬ê¸°ì„œëŠ” ê°€ê²©ë§Œ ë˜ëŒë¦¼

        # ìˆ˜ì •: í•­ìƒ delta_limitë¡œ í´ë¨í”„ (try/except ë°–ì—ì„œ ê³µí†µ ì ìš©)
        price_uplimit = current_price * (1 + delta_limit)
        price_downlimit = current_price * (1 - delta_limit)
        revised_price = float(min(max(revised_price, price_downlimit), price_uplimit))

        # Fine-tuning (return ë‹¨ìœ„)
        loss_value = None
        if fine_tune:
            try:
                revised_return = (revised_price / current_price) - 1.0   # ì˜ˆ: 0.012
                revised_return_scaled = revised_return * 100.0           # ì˜ˆ: 1.2

                # ìŠ¤ì¼€ì¼ëŸ¬ ê¸°ì¤€ì— ë§ì¶”ì–´ íƒ€ê¹ƒ ë³€í™˜
                if getattr(self.scaler, "y_scaler", None) is not None:
                    y_target_scaled = self.scaler.y_scaler.transform(
                        np.array([[revised_return_scaled]], dtype=float)
                    )[0, 0]
                else:
                    y_target_scaled = revised_return_scaled

                # ìµœì‹  ì…ë ¥
                X_input = self.searcher(self.ticker)  # (1, T, F)

                # TechnicalAgent(nn.Module) â†’ self ìì²´ ì‚¬ìš©
                model = self
                device = next(model.parameters()).device

                if isinstance(X_input, torch.Tensor):
                    X_tensor = X_input.to(device).float()
                else:
                    X_tensor = torch.tensor(X_input, dtype=torch.float32, device=device)

                y_tensor = torch.tensor([[y_target_scaled]], dtype=torch.float32, device=device)

                model.train()
                optimizer = torch.optim.Adam(model.parameters(), lr=lr)
                # pretrainê³¼ í†µì¼: HuberLoss
                criterion = torch.nn.HuberLoss(delta=1.0)

                for _ in range(epochs):
                    optimizer.zero_grad()
                    pred = model(X_tensor)
                    loss = criterion(pred, y_tensor)
                    loss.backward()
                    optimizer.step()

                loss_value = float(loss.item())
                print(f"[{self.agent_id}] fine-tuning ì™„ë£Œ: loss={loss_value:.6f}")

            except Exception as e:
                print(f"[{self.agent_id}] fine-tuning ì‹¤íŒ¨: {e}")

        # fine-tuning ì´í›„ ìƒˆ ì˜ˆì¸¡ ìƒì„±
        try:
            X_latest = self.searcher(self.ticker)
            new_target = self.predict(X_latest)
        except Exception as e:
            print(f"[{self.agent_id}] predict ì‹¤íŒ¨: {e}")
            new_target = my_opinion.target

        # reasoning ìƒì„±
        try:
            sys_text, user_text = self._build_messages_revision(
                my_opinion=my_opinion,
                others=others,
                rebuttals=rebuttals,
                stock_data=stock_data,
            )
        except Exception as e:
            print(f"[{self.agent_id}] _build_messages_revision ì‹¤íŒ¨: {e}")
            sys_text, user_text = (
                "ë„ˆëŠ” ê¸ˆìœµ ë¶„ì„ê°€ë‹¤. ê°„ë‹¨íˆ reasonë§Œ ìƒì„±í•˜ë¼.",
                json.dumps({"reason": "ê¸°ë³¸ ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨"}),
            )

        parsed = self._ask_with_fallback(
            self._msg("system", sys_text),
            self._msg("user", user_text),
            {
                "type": "object",
                "properties": {"reason": {"type": "string"}},
                "required": ["reason"],
                "additionalProperties": False,
            },
        )

        revised_reason = parsed.get("reason", "(ìˆ˜ì • ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨)")
        revised_opinion = Opinion(
            agent_id=self.agent_id,
            target=new_target,
            reason=revised_reason,
        )

        self.opinions.append(revised_opinion)
        print(f"[{self.agent_id}] revise ì™„ë£Œ â†’ new_close={new_target.next_close:.2f}, loss={loss_value}")
        return self.opinions[-1]

    def load_model(self, model_path: Optional[str] = None):
        """ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ (ê°ì²´/ë”•ì…”ë„ˆë¦¬/state_dict ìë™ ì¸ì‹ + model ìë™ ìƒì„±)"""
        if model_path is None:
            model_path = os.path.join(self.model_dir, f"{self.ticker}_{self.agent_id}.pt")

        if not os.path.exists(model_path):
            return False

        try:
            checkpoint = torch.load(model_path, map_location=torch.device("cpu"))

            # í˜¹ì‹œ ì˜ˆì „ì— ì˜ëª» ë“±ë¡ëœ ì„œë¸Œëª¨ë“ˆ "model"ì´ ìˆìœ¼ë©´ ì œê±°
            # (selfë¥¼ ì„œë¸Œëª¨ë“ˆë¡œ ë„£ì–´ë²„ë¦° ê³¼ê±° ì½”ë“œ ëŒ€ë¹„ìš©)
            self._modules.pop("model", None)

            # ë‹¤ì–‘í•œ ì €ì¥ í¬ë§· ì²˜ë¦¬
            if isinstance(checkpoint, torch.nn.Module):
                state_dict = checkpoint.state_dict()
            elif isinstance(checkpoint, dict):
                state_dict = (
                    checkpoint.get("model_state_dict")
                    or checkpoint.get("state_dict")
                    or checkpoint
                )
            else:
                print(f"[{self.agent_id}] ì•Œ ìˆ˜ ì—†ëŠ” ì²´í¬í¬ë§·: {type(checkpoint)}")
                return False

            # ë°”ë¡œ selfì— ë¡œë“œ
            self.load_state_dict(state_dict)
            self.eval()

            # self.model ì— selfë¥¼ ë„£ìœ¼ë©´ ìˆœí™˜ ì°¸ì¡°(submodule ë“±ë¡)ë¼ì„œ ë„£ì§€ ì•ŠëŠ” ê²Œ ì•ˆì „
            # (TechnicalAgent.predict / pretrain ì€ model = self ë¡œ ë™ì‘í•˜ë¯€ë¡œ ë³„ë„ self.model í•„ìš” ì—†ìŒ)

            return True

        except Exception as e:
            print(f"[{self.agent_id}] load_model ì‹¤íŒ¨: {e}")
            return False

    def evaluate(self, ticker: str = None):
        """ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€"""
        if ticker is None:
            ticker = self.ticker

        # 1) ë°ì´í„° ë¡œë“œ
        X, y, feature_cols, _ = load_dataset_tech(
            ticker,
            agent_id=self.agent_id,
            save_dir=self.data_dir
        )

        # 2) ì‹œê³„ì—´ ë¶„í•  (80% í›ˆë ¨, 20% ê²€ì¦)
        split_idx = int(len(X) * 0.8)
        X_val = X[split_idx:]
        y_val = y[split_idx:]

        # 3) ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ + y ìŠ¤ì¼€ì¼(í•™ìŠµê³¼ ë™ì¼í•˜ê²Œ Ã—100)  # ìˆ˜ì •
        self.scaler.load(ticker)

        # ğŸ”§ ìˆ˜ì •: yë¥¼ 1Dë¡œ ë§ì¶°ì¤ë‹ˆë‹¤.
        y_val_scaled = (y_val * 100.0).reshape(-1)

        X_val_scaled, y_val_scaled = self.scaler.transform(
            X_val,
            y_val_scaled
        )

        # ğŸ”§ ìˆ˜ì •: transform ê²°ê³¼ë„ í™•ì‹¤íˆ 1Dë¡œ ì •ë¦¬
        y_val_scaled = np.asarray(y_val_scaled).reshape(-1)

        # 4) ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ (ì—†ìœ¼ë©´ pretrain)                # ìˆ˜ì •
        model_path = os.path.join(self.model_dir, f"{ticker}_{self.agent_id}.pt")
        if not self.load_model(model_path):                      # ìˆ˜ì •
            self.pretrain()
            self.load_model(model_path)

        model = self
        model.eval()                                             # ìˆ˜ì •

        # 5) ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡
        predictions = []
        actual_returns = []

        with torch.no_grad():                                    # ìˆ˜ì •
            for i in range(len(X_val_scaled)):
                X_input = X_val_scaled[i:i+1]   # (1, T, F)
                X_tensor = torch.tensor(X_input, dtype=torch.float32)

                pred_scaled = model(X_tensor).item()             # ì˜ˆì¸¡ê°’ (ìŠ¤ì¼€ì¼ëœ y)
                predictions.append(pred_scaled)
                actual_returns.append(float(y_val_scaled[i]))       # ìŠ¤ì¼€ì¼ëœ íƒ€ê¹ƒ

        predictions = np.array(predictions)
        actual_returns = np.array(actual_returns)

        # 6) ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ìŠ¤ì¼€ì¼ëœ ìˆ˜ìµë¥  ê¸°ì¤€)              # ìˆ˜ì •
        mae = np.mean(np.abs(predictions - actual_returns))
        rmse = np.sqrt(np.mean((predictions - actual_returns) ** 2))

        # ìƒê´€ê³„ìˆ˜ (ë¶„ì‚° 0 ë°©ì§€)                                 # ìˆ˜ì •
        if np.std(predictions) == 0 or np.std(actual_returns) == 0:
            correlation = 0.0
        else:
            correlation = float(np.corrcoef(predictions, actual_returns)[0, 1])

        # 7) ë°©í–¥ ì •í™•ë„ (ë¶€í˜¸ ê¸°ì¤€ â†’ ìƒìŠ¹/í•˜ë½ ì¼ì¹˜ìœ¨)
        pred_direction = np.sign(predictions)
        actual_direction = np.sign(actual_returns)
        direction_accuracy = float(np.mean(pred_direction == actual_direction) * 100.0)

        return {
            "mae": mae,
            "rmse": rmse,
            "correlation": correlation,
            "direction_accuracy": direction_accuracy,
            "n_samples": len(predictions),
        }
