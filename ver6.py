# ----------------------------------------------------
# â­ Colab í™˜ê²½ ì„¤ì •: Google Drive ë§ˆìš´íŠ¸
# ----------------------------------------------------
from google.colab import drive
import os
import sys

# â­â­ í•„ìˆ˜ ìˆ˜ì •: Google Drive ê²½ë¡œ ì„¤ì •
# ë°ì´í„°ë¥¼ ì €ì¥í•œ ì‹¤ì œ Drive í´ë” ê²½ë¡œë¡œ ë°”ê¿”ì£¼ì„¸ìš”.
base_path = '/content/drive/MyDrive/capstone'

try:
    drive.mount('/content/drive')
    if not os.path.exists(base_path):
        os.makedirs(base_path)
        print(f"Drive ê²½ë¡œ {base_path}ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"Google Drive ë§ˆìš´íŠ¸ ì˜¤ë¥˜ ë˜ëŠ” í´ë” ìƒì„± ì˜¤ë¥˜: {e}")

# ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import time
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.preprocessing import StandardScaler
import joblib
from datetime import timedelta
import warnings
warnings.filterwarnings('ignore')

# ----------------------------------------------------
# --- ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬ (í•™ìŠµ ë°ì´í„°) ---
# ----------------------------------------------------

# Google Drive ê²½ë¡œì—ì„œ íŒŒì¼ ë¡œë“œ
stock_file = os.path.join(base_path, 'stock_data.csv')
news_file = os.path.join(base_path, 'news_data2.csv')

try:
    stock_df = pd.read_csv(stock_file)
    news_df = pd.read_csv(news_file)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {stock_file}, {news_file}")
except FileNotFoundError:
    print(f"ì˜¤ë¥˜: Drive ê²½ë¡œ({base_path})ì—ì„œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œì™€ íŒŒì¼ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit()

stock = stock_df.copy()
news = news_df.copy()

# ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ ë° EODhd ê°ì„± ì ìˆ˜ ì¤€ë¹„
news['summary'] = news['summary'].fillna('')
news = news.dropna(subset=['date'])
news['date'] = pd.to_datetime(news['date'])
news['sentiment_score'] = pd.to_numeric(news['sentiment_score'], errors='coerce').fillna(0.0)


# FinBERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-tone')
finbert = AutoModelForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')
news['text'] = news['title'] + ' ' + news['summary']

# FinBERTë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ (GPU í™œìš©)
def finbert_sentiment_scores(texts, batch_size=32):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    finbert.to(device)

    finbert.eval()
    scores = []
    with torch.no_grad():
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]

            inputs = tokenizer(list(batch_texts), return_tensors='pt',
                               padding=True, truncation=True, max_length=512)

            inputs = {name: tensor.to(device) for name, tensor in inputs.items()}

            outputs = finbert(**inputs)
            scores.extend(torch.softmax(outputs.logits, dim=1).cpu().numpy())

    return np.array(scores)

# ë‰´ìŠ¤ ë°ì´í„°ì— FinBERT ê°ì„± ì ìˆ˜(ê¸ì •/ë¶€ì •/ì¤‘ë¦½ í™•ë¥ ) ì¶”ê°€
print("--- FinBERTë¥¼ ì‚¬ìš©í•œ ê°ì„± ì ìˆ˜ ê³„ì‚° ì‹œì‘ (GPU í™œìš©) ---")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"í˜„ì¬ FinBERT ê°ì„± ë¶„ì„ ì¥ì¹˜: {device}")
news[['prob_positive', 'prob_negative', 'prob_neutral']] = finbert_sentiment_scores(news['text'].values)
print("--- ê°ì„± ì ìˆ˜ ê³„ì‚° ì™„ë£Œ ---")
news['date'] = pd.to_datetime(news['date']).dt.normalize()

# ì¼ë³„/ì¢…ëª©ë³„ í‰ê·  ê°ì„± ì ìˆ˜ ë° ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ ê³„ì‚°
daily_sentiments = (news.groupby(['date','ticker'])
    .agg(prob_positive=('prob_positive','mean'),
         prob_negative=('prob_negative','mean'),
         prob_neutral=('prob_neutral','mean'),
         n_news=('title','count'),
         eod_sentiment=('sentiment_score', 'mean')).reset_index())

# ê¸°ìˆ ì  ë¶„ì„ ì§€í‘œ(Feature)ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
def add_tech_features(df):
    df['ma5']     = df['Close'].rolling(window=5).mean()
    df['ma10']    = df['Close'].rolling(window=10).mean()
    df['ret']     = df['Close'].pct_change()
    df['ret_next']      = df['ret'].shift(-1)
    return df

# ì£¼ê°€ ë°ì´í„°ì— ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€ ë° ì „ì²˜ë¦¬
stock['Date'] = pd.to_datetime(stock['Date']).dt.normalize()
stock = stock.sort_values(['Symbol','Date']).groupby('Symbol', group_keys=False).apply(add_tech_features)
stock['next_close'] = stock.groupby('Symbol')['Close'].shift(-1)
stock = stock.dropna(subset=['ret','next_close'])


# ----------------------------------------------------------------------
# â­â­ Colab UTC ì‹œê°„ëŒ€ ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ ì½”ë“œ ì¶”ê°€ (ë¬¸ì œ í•´ê²° í•µì‹¬) â­â­
# ----------------------------------------------------------------------
# ë³‘í•© ì „ ë‚ ì§œ ì»¬ëŸ¼ì˜ ì‹œê°„ëŒ€ ì •ë³´ë¥¼ ì œê±°í•˜ì—¬ í†µì¼
if stock['Date'].dt.tz is not None:
    stock['Date'] = stock['Date'].dt.tz_localize(None)

if daily_sentiments['date'].dt.tz is not None:
    daily_sentiments['date'] = daily_sentiments['date'].dt.tz_localize(None)
print("âœ… ë‚ ì§œ ì»¬ëŸ¼ì˜ UTC ì‹œê°„ëŒ€ ì •ë³´ í†µì¼ ì™„ë£Œ.")
# ----------------------------------------------------------------------


# ì£¼ê°€ ë°ì´í„°ì™€ ì¼ë³„ ê°ì„± ë°ì´í„°ë¥¼ ë³‘í•©
data = pd.merge(stock, daily_sentiments, how='left',
    left_on=['Date','Symbol'], right_on=['date','ticker'])

for c in ['prob_positive','prob_negative','prob_neutral','n_news', 'eod_sentiment']:
    if c in data: data[c] = data[c].fillna(0.0)

# Google Drive ê²½ë¡œì— íŒŒì¼ ì €ì¥
processed_file = os.path.join(base_path, 'processed_data_with_both_sentiment_V2.csv') # â­ íŒŒì¼ëª… V2ë¡œ ë³€ê²½
try:
    data.to_csv(processed_file, index=False)
    print(f"âœ… ê°ì„± ë¶„ì„ì´ ì™„ë£Œëœ ë°ì´í„° ({processed_file}) ì €ì¥ ì™„ë£Œ")
except Exception as e:
    print(f"ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# LSTM ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  íŠ¹ì„±(Feature) ë¦¬ìŠ¤íŠ¸ ì •ì˜
FEATURES = ['prob_positive','prob_negative','prob_neutral','n_news','ret','Close', 'eod_sentiment']

# ì‹œê³„ì—´ ë°ì´í„°(ì‹œí€€ìŠ¤) ìƒì„± ë° ë°ì´í„°ì…‹ ì¤€ë¹„ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
def create_sequences(df, window_size=10):
    df = df.sort_values('Date')
    arr = df[FEATURES].values
    tgt = df['next_close'].values
    seqs, tgts = [], []
    for i in range(len(df) - window_size):
        seqs.append(arr[i:i+window_size])
        tgts.append(tgt[i+window_size])
    return np.array(seqs), np.array(tgts)

def prepare_dataset(df, window_size=10):
    X, y, symbol_all, date_all = [], [], [], []
    for symbol in df['Symbol'].unique():
        sd = df[df['Symbol']==symbol].sort_values('Date')
        seq, tgt = create_sequences(sd, window_size)
        if len(seq) > 0:
            X.append(seq)
            y.append(tgt)
            symbol_all.extend([symbol] * len(seq))
            date_all.extend(sd['Date'].iloc[window_size:].values)
    return (np.concatenate(X), np.concatenate(y), symbol_all, date_all) if X else (np.array([]), np.array([]), [], [])

window_size = 10

# â­â­ ìˆ˜ì •: í•™ìŠµ ë°ì´í„° ê¸°ê°„ì„ 2020-01-01ë¶€í„° 2024-10-31ê¹Œì§€ë¡œ í™•ì¥
train_df = data[(data['Date']>=pd.to_datetime('2020-01-01')) & (data['Date']<=pd.to_datetime('2024-10-31'))]
val_df = data[(data['Date']>=pd.to_datetime('2024-11-01')) & (data['Date']<=pd.to_datetime('2024-12-31'))]
pred_df = data.copy()

# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„
X_train, y_train, symbol_train, date_train = prepare_dataset(train_df, window_size)
X_val, y_val, symbol_val, date_val = prepare_dataset(val_df, window_size)
X_pred_all, y_pred_all, symbol_pred_all, date_pred_all = prepare_dataset(pred_df, window_size)

print(f"\n>> ë°ì´í„°ì…‹ ì •ë³´:")
print(f"   í™•ì¥ëœ í•™ìŠµ ë°ì´í„° ì‹œí€€ìŠ¤ ìˆ˜: {len(X_train)}")
print(f"   ê²€ì¦ ë°ì´í„° ì‹œí€€ìŠ¤ ìˆ˜: {len(X_val)}")


# ë°ì´í„° í‘œì¤€í™” í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
def standardize(X, y, scaler_x=None, scaler_y=None, fit_mode=True):
    nsamples, nwin, nfeat = X.shape

    if fit_mode:
        scaler_x = StandardScaler().fit(X.reshape(-1, nfeat))
    X_scaled = scaler_x.transform(X.reshape(-1, nfeat)).reshape(nsamples, nwin, nfeat)

    if fit_mode:
        scaler_y = StandardScaler().fit(y.reshape(-1, 1))
    y_scaled = scaler_y.transform(y.reshape(-1, 1)).flatten()

    return X_scaled, y_scaled, scaler_x, scaler_y

X_train_scaled, y_train_scaled, scaler_x, scaler_y = standardize(X_train, y_train, fit_mode=True)
X_val_scaled, y_val_scaled, _, _ = standardize(X_val, y_val, scaler_x, scaler_y, fit_mode=False)
X_pred_all_scaled, y_pred_all_scaled, _, _ = standardize(X_pred_all, y_pred_all, scaler_x, scaler_y, fit_mode=False)

# PyTorch Dataset ë° DataLoader ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
class StockDataset(torch.utils.data.Dataset):
    def __init__(self, X, y):
        self.X, self.y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)
    def __len__(self): return len(self.X)
    def __getitem__(self, idx): return self.X[idx], self.y[idx]

train_loader = torch.utils.data.DataLoader(StockDataset(X_train_scaled, y_train_scaled), batch_size=32, shuffle=True)
val_loader = torch.utils.data.DataLoader(StockDataset(X_val_scaled, y_val_scaled), batch_size=32, shuffle=False)
pred_all_loader = torch.utils.data.DataLoader(StockDataset(X_pred_all_scaled, y_pred_all_scaled), batch_size=32, shuffle=False)

# --- LSTM ëª¨ë¸ ì •ì˜ ---
class StockSentimentLSTM(nn.Module):
    # â­ ìˆ˜ì •: hidden_dim=128, num_layers=2ë¡œ êµ¬ì¡° ê°•í™”
    def __init__(self, hidden_dim=128, num_layers=2, input_size=len(FEATURES)):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True, dropout=0.1)
        self.fc = nn.Linear(hidden_dim, 1)
    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :]).squeeze()

# ëª¨ë¸ í•™ìŠµ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\n>> LSTM í•™ìŠµ ì¥ì¹˜: {device} (GPU ì‚¬ìš©)")
model = StockSentimentLSTM().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=3e-4)

# --- ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ --- (ê¸°ì¡´ê³¼ ë™ì¼)
def train_model(train_loader, val_loader, model, criterion, optimizer, device, epochs=20):
    # (í•™ìŠµ ë¡œì§ ìƒëµ)
    train_losses, val_losses = [], []
    for epoch in range(epochs):
        t1, train_loss = time.time(), 0
        model.train()
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * inputs.size(0)
        train_loss /= len(train_loader.dataset)
        train_losses.append(train_loss)

        val_loss = 0
        model.eval()
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                val_loss += criterion(model(inputs), targets).item() * inputs.size(0)
        val_loss /= len(val_loader.dataset)
        val_losses.append(val_loss)

        print(f"Epoch {epoch+1} - Train Loss: {train_loss:.6f} - Val Loss: {val_loss:.6f} - Time: {time.time()-t1:.1f}s")
    return train_losses, val_losses

# ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
train_losses, val_losses = train_model(train_loader, val_loader, model, criterion, optimizer, device)

# --- ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (Drive ê²½ë¡œ) ---
torch.save(model.state_dict(), os.path.join(base_path, "model_lstm_bothsentiment_V2.pt"))
joblib.dump(scaler_x, os.path.join(base_path, "scaler_x_both_V2.pkl"))
joblib.dump(scaler_y, os.path.join(base_path, "scaler_y_both_V2.pkl"))

print("ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")

# --- ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì˜ˆì¸¡ (Drive ê²½ë¡œ) ---
model_eval = StockSentimentLSTM(hidden_dim=128, num_layers=2, input_size=len(FEATURES)) # â­ ìˆ˜ì •ëœ êµ¬ì¡° ë°˜ì˜
model_eval.load_state_dict(torch.load(os.path.join(base_path, "model_lstm_bothsentiment_V2.pt")))
model_eval.eval()
model_eval.to(device)
scaler_x = joblib.load(os.path.join(base_path, "scaler_x_both_V2.pkl"))
scaler_y = joblib.load(os.path.join(base_path, "scaler_y_both_V2.pkl"))

# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
def plot_predictions(model, val_loader, scaler_y):
    # (í•¨ìˆ˜ ë‚´ìš© ìƒëµ)
    model.eval()
    preds, targets = [], []
    with torch.no_grad():
        for inputs, y_true in val_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            preds.extend(outputs.cpu().numpy())
            targets.extend(y_true.numpy())

    preds = scaler_y.inverse_transform(np.array(preds).reshape(-1,1)).flatten()
    targets = scaler_y.inverse_transform(np.array(targets).reshape(-1,1)).flatten()

    plt.figure(figsize=(12, 6))
    plt.plot(targets, label='Actual Next Close', linewidth=2)
    plt.plot(preds, label='Predicted Next Close', linewidth=2, alpha=0.8)
    plt.title('Actual vs Predicted Next Day Close (Validation set - V2 Reinforced)')
    plt.xlabel('Sample Index (Time-ordered)')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()

# ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ì‹œê°í™” ì‹¤í–‰
plot_predictions(model_eval, val_loader, scaler_y)

# í•™ìŠµ ë° ê²€ì¦ ì†ì‹¤ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” í•¨ìˆ˜
def plot_loss(train_losses, val_losses):
    # (í•¨ìˆ˜ ë‚´ìš© ìƒëµ)
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title('Training and Validation Loss (V2 Reinforced)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss (MSE)')
    plt.legend()
    plt.grid()
    plt.show()

# ì†ì‹¤ ê·¸ë˜í”„ ì‹œê°í™” ì‹¤í–‰
plot_loss(train_losses, val_losses)

print("\n--- ëª¨ë¸ ì„±ëŠ¥ ì¶”ê°€ ë¶„ì„ ì‹œì‘ ---\n")

def evaluate_and_analyze(model, val_loader, scaler_y, X_val, y_val, symbol_val, date_val):
    model.eval()
    preds_scaled, targets_scaled = [], []
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            preds_scaled.extend(outputs.cpu().numpy())
            targets_scaled.extend(targets.cpu().numpy())

    preds = scaler_y.inverse_transform(np.array(preds_scaled).reshape(-1, 1)).flatten()
    targets = scaler_y.inverse_transform(np.array(targets_scaled).reshape(-1, 1)).flatten()

    # 3. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬ (ì²« ë²ˆì§¸ ì •ì˜)
    results_df = pd.DataFrame({
        'Date': date_val,
        'Symbol': symbol_val,
        'Actual': targets,
        'Predicted': preds
    })

    results_df['Squared_Error'] = (results_df['Actual'] - results_df['Predicted']) ** 2
    # â­ ì˜¤ë¥˜ í•´ê²°: Absolute_Error ì»¬ëŸ¼ì„ ì—¬ê¸°ì„œ ìƒì„±í•©ë‹ˆë‹¤.
    results_df['Absolute_Error'] = np.abs(results_df['Actual'] - results_df['Predicted'])

    # [â­ ì œê±°ëœ ë¶€ë¶„: ì•„ë˜ 3ì¤„ì€ ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ì •ì˜ì´ë¯€ë¡œ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤.]
    # results_df = pd.DataFrame({'Date': date_val, 'Symbol': symbol_val, 'Actual': targets, 'Predicted': preds})
    # results_df['Squared_Error'] = (results_df['Actual'] - results_df['Predicted']) ** 2

    # 4. ì¢…ëª©ë³„ RMSE ê³„ì‚° ë° ì¶œë ¥
    mse_per_symbol = results_df.groupby('Symbol')['Squared_Error'].mean().sort_values(ascending=False)
    rmse_per_symbol = np.sqrt(mse_per_symbol)

    print("## ğŸ“ˆ ì¢…ëª©ë³„ RMSE (Root Mean Squared Error) - ì„±ëŠ¥ ë¶„ì„ (Validation Set - V2)")
    print(rmse_per_symbol.to_string(float_format='%.4f'))
    print("\n" + "="*50 + "\n")

    # 5. ê°€ì¥ ì˜ ì˜ˆì¸¡ëœ ì‹œì  ì°¾ê¸° (Absolute_Error ì»¬ëŸ¼ ì‚¬ìš©)
    best_sample = results_df.sort_values('Absolute_Error').iloc[0]
    best_date = best_sample['Date']
    best_symbol = best_sample['Symbol']
    symbol_data = data[data['Symbol'] == best_symbol].sort_values('Date').reset_index(drop=True)
    end_index = symbol_data[symbol_data['Date'] == best_date].index[0]
    start_index = end_index - window_size
    best_input_data = symbol_data.iloc[start_index : end_index]

    print(f"## â­ ê°€ì¥ ì˜ ì˜ˆì¸¡ëœ ì‹œì ì˜ ìƒì„¸ ë¶„ì„ (ì ˆëŒ€ ì˜¤ì°¨ ìµœì†Œ)")
    print(f" - ì˜ˆì¸¡ ì‹œì : {best_date}")
    print(f" - ì¢…ëª©: {best_symbol}")
    print(f" - ì‹¤ì œ ë‹¤ìŒë‚  ì¢…ê°€: {best_sample['Actual']:.2f}")
    print(f" - ì˜ˆì¸¡ëœ ë‹¤ìŒë‚  ì¢…ê°€: {best_sample['Predicted']:.2f}")
    print(f" - ì ˆëŒ€ ì˜¤ì°¨: {best_sample['Absolute_Error']:.4f}\n")
    print(f"## ğŸ“œ ì…ë ¥ ë°ì´í„° ({window_size}ì¼ ì‹œí€€ìŠ¤)")
    print(best_input_data[['Date'] + FEATURES].to_string(index=False, float_format='%.4f'))

    return results_df

# ë¶„ì„ ì‹¤í–‰
results_df = evaluate_and_analyze(model_eval, val_loader, scaler_y, X_val, y_val, symbol_val, date_val)

# --- ê°œë³„ ì¢…ëª© ì˜ˆì¸¡ ì‹œê°í™” (validations set) ---
def plot_single_symbol(results_df, symbol, suffix="Validation Set V2"):
    symbol_df = results_df[results_df['Symbol'] == symbol].reset_index(drop=True)
    if symbol_df.empty:
        print(f"\nê²½ê³ : ì¢…ëª© {symbol}ì— ëŒ€í•œ ë°ì´í„°ê°€ ê²€ì¦ ì…‹ì— ì—†ìŠµë‹ˆë‹¤.")
        return

    plt.figure(figsize=(14, 7))
    plt.plot(symbol_df['Date'], symbol_df['Actual'], label=f'{symbol} Actual Close', linewidth=2)
    plt.plot(symbol_df['Date'], symbol_df['Predicted'], label=f'{symbol} Predicted Close', linewidth=2, alpha=0.8, linestyle='--')

    plt.title(f'{symbol}: Actual vs Predicted Next Day Close ({suffix})')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# RMSEê°€ ê°€ì¥ ë†’ì•˜ë˜ (ê°€ì¥ ì˜ˆì¸¡ì´ ì–´ë ¤ì› ë˜) ì¢…ëª© ì‹œê°í™”
worst_symbol = results_df.groupby('Symbol')['Squared_Error'].mean().idxmax()
print(f"\n--- ê°€ì¥ ì˜ˆì¸¡ì´ ì–´ë ¤ì› ë˜ ì¢…ëª© ì‹œê°í™”: {worst_symbol} ---\n")
plot_single_symbol(results_df, worst_symbol)

# RMSEê°€ ê°€ì¥ ë‚®ì•˜ë˜ (ê°€ì¥ ì˜ˆì¸¡ì´ ì‰¬ì› ë˜) ì¢…ëª© ì‹œê°í™”
best_overall_symbol = results_df.groupby('Symbol')['Squared_Error'].mean().idxmin()
print(f"\n--- ê°€ì¥ ì˜ˆì¸¡ì´ ì‰¬ì› ë˜ ì¢…ëª© ì‹œê°í™”: {best_overall_symbol} ---\n")
plot_single_symbol(results_df, best_overall_symbol)

# ----------------------------------------------------
# â­â­ 2025ë…„ 1ì›” ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„ ì¶”ê°€ â­â­
# ----------------------------------------------------

def get_predictions(model, data_loader, scaler_y):
    model.eval()
    preds_scaled, targets_scaled = [], []
    with torch.no_grad():
        for inputs, targets in data_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            preds_scaled.extend(outputs.cpu().numpy())
            targets_scaled.extend(targets.cpu().numpy())

    preds_scaled = np.array(preds_scaled)
    targets_scaled = np.array(targets_scaled)

    preds_price = scaler_y.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()
    targets_price = scaler_y.inverse_transform(targets_scaled.reshape(-1, 1)).flatten()

    return preds_price, targets_price

# 1. ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ ìˆ˜í–‰ (ì „ì²´ ì‹œí€€ìŠ¤)
preds_price_all, targets_price_all = get_predictions(model_eval, pred_all_loader, scaler_y)

# 2. ì˜ˆì¸¡ ê²°ê³¼ DataFrame ì¬ì •ì˜
pred_results_df_all = pd.DataFrame({
    'Date': date_pred_all,
    'Symbol': symbol_pred_all,
    'Actual_Next_Close': targets_price_all,
    'Predicted_Next_Close': preds_price_all
}).sort_values(['Symbol', 'Date']).reset_index(drop=True)

if pred_results_df_all.empty:
    print("\n[ì¬ì‹œë„ ì‹¤íŒ¨] ì˜ˆì¸¡ëœ ì‹œí€€ìŠ¤ê°€ ì—†ì–´ ê²°ê³¼ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit()

print("\n" + "="*50)
print("## âœ… ì „ì²´ ìœ íš¨ ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„")
print(f"ì´ ìœ íš¨ ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {len(pred_results_df_all)}ê°œ")
print(f"ìµœì´ˆ ì˜ˆì¸¡ì¼: {pred_results_df_all['Date'].min()}")
print(f"ìµœì¢… ì˜ˆì¸¡ì¼: {pred_results_df_all['Date'].max()}")
print("="*50 + "\n")

# 3. 2025ë…„ 1ì›” ì˜ˆì¸¡ ê²°ê³¼ í•„í„°ë§
start_date_pred_target = pd.to_datetime('2025-01-01')
end_date_pred_target = pd.to_datetime('2025-01-31')

pred_results_df_jan = pred_results_df_all[
    (pred_results_df_all['Date'] >= start_date_pred_target) &
    (pred_results_df_all['Date'] <= end_date_pred_target)
].sort_values(['Symbol', 'Date']).reset_index(drop=True)

if pred_results_df_jan.empty:
    print("\n[ê²°ê³¼ ì—†ìŒ] 2025ë…„ 1ì›” ì˜ˆì¸¡ì— í•´ë‹¹í•˜ëŠ” ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit()

print("="*50)
print("## âœ… 2025ë…„ 1ì›” ì˜ˆì¸¡ ê²°ê³¼ (V2 Reinforced)")
print(f"ì´ ìœ íš¨ ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {len(pred_results_df_jan)}ê°œ")
print(pred_results_df_jan.head().to_string(float_format='%.4f'))
print("="*50 + "\n")

# 4. 2025ë…„ 1ì›” ì˜ˆì¸¡ ì¢…ê°€ vs ì‹¤ì œ ì¢…ê°€ ì‹œê°í™” ë° RMSE ê³„ì‚°
def plot_single_symbol_jan(results_df, symbol):
    symbol_df = results_df[results_df['Symbol'] == symbol].reset_index(drop=True)
    if symbol_df.empty:
        return

    plt.figure(figsize=(14, 7))
    plt.plot(symbol_df['Date'], symbol_df['Actual_Next_Close'], label=f'{symbol} Actual Next Close', linewidth=2)
    plt.plot(symbol_df['Date'], symbol_df['Predicted_Next_Close'], label=f'{symbol} Predicted Next Close', linewidth=2, alpha=0.8, linestyle='--')

    mse = ((symbol_df['Actual_Next_Close'] - symbol_df['Predicted_Next_Close']) ** 2).mean()
    rmse = np.sqrt(mse)

    plt.title(f'January 2025 Forecast (W=10) - {symbol}: V2 Reinforced (RMSE: {rmse:.4f})')
    plt.xlabel('Date')
    plt.ylabel('Next Day Close Price')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# 2025ë…„ 1ì›” ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•´ ì‹œê°í™” ì‹¤í–‰
predicted_symbols_jan = pred_results_df_jan['Symbol'].unique()
print(f"### ğŸ“ˆ 2025ë…„ 1ì›” ì˜ˆì¸¡ ì‹œê°í™” ì‹œì‘ ({len(predicted_symbols_jan)} ì¢…ëª©)")

for symbol in predicted_symbols_jan:
    plot_single_symbol_jan(pred_results_df_jan, symbol)

# ì „ì²´ ì¢…ëª©ì— ëŒ€í•œ RMSE ê³„ì‚°
mse_pred_per_symbol_jan = pred_results_df_jan.groupby('Symbol').apply(
    lambda x: ((x['Actual_Next_Close'] - x['Predicted_Next_Close']) ** 2).mean()
)
rmse_pred_per_symbol_jan = np.sqrt(mse_pred_per_symbol_jan).sort_values(ascending=False)

print("\n## ğŸ’° 2025ë…„ 1ì›” ì¢…ëª©ë³„ ì˜ˆì¸¡ RMSE (V2 Reinforced)")
print(rmse_pred_per_symbol_jan.to_string(float_format='%.4f'))

!jupyter nbconvert --to script /content/drive/MyDrive/Colab Notebooks/ver6.ipynb


