import torch
import torch.nn as nn
import numpy as np
import random
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Literal
import requests
from datetime import datetime, timedelta
import json
import time 
import os
import csv
import yfinance as yf
import pandas as pd

# EODhd API ì„¤ì • (ë”ë¯¸ í‚¤)
API_KEY = 'YOUR_KEY' # ì‹¤ì œ API í‚¤ë¡œ êµì²´ í•„ìš”
BASE_URL_EODHD = 'https://eodhd.com/api/news'
STATUS_FILE = 'collection_status.json' # ìƒíƒœ íŒŒì¼ëª…

from base_agent import BaseAgent 

# ==============================================================================
# ê³µí†µ ë°ì´í„° í¬ë§·
# ==============================================================================

@dataclass
class Target:
    """ì˜ˆì¸¡ ëª©í‘œê°’ ë¬¶ìŒ (í•„ìš” ì‹œ í•„ë“œ í™•ì¥)
    - next_close: ë‹¤ìŒ ê±°ë˜ì¼ ì¢…ê°€ ì˜ˆì¸¡ì¹˜
    - uncertainty: ì˜ˆì¸¡ì˜ ë¶ˆí™•ì‹¤ì„± (ëª¬í…Œ ì¹´ë¥¼ë¡œ ë¶„ì‚°)
    - confidence: ì˜ˆì¸¡ì˜ ì‹ ë¢°ë„ (ë¶ˆí™•ë¥ ì„± ì—­ìˆ˜)
    - idea: ëª¨ë¸ì˜ ì„¤ëª… ê°€ëŠ¥ì„± ê·¼ê±° (SHAP, Feature Importance ë“±)
    """
    next_close : float
    uncertainty: float = 0.0 # í•„ë“œ ì¶”ê°€
    confidence: float = 0.0  # í•„ë“œ ì¶”ê°€
    idea: Dict[str, List[Any]] = field(default_factory=dict) # í•„ë“œ ì¶”ê°€

@dataclass
class Opinion:
    """ì—ì´ì „íŠ¸ì˜ ì˜ê²¬(ì´ˆì•ˆ/ìˆ˜ì •ë³¸ ê³µí†µ í¬ë§·)
    - agent_id: ì˜ê²¬ì„ ë‚¸ ì—ì´ì „íŠ¸ ì‹ë³„ì
    - target  : ì˜ˆì¸¡ íƒ€ê¹ƒ(ì˜ˆ: next_close)
    - reason  : ê·¼ê±° í…ìŠ¤íŠ¸(LLM/ë£° ê¸°ë°˜)
    """
    agent_id: str
    target: Target
    reason: str  # TODO: LLM/ë£° ê¸°ë°˜ ì‚¬ìœ  í…ìŠ¤íŠ¸ ìƒì„±

@dataclass
class Rebuttal:
    """ì—ì´ì „íŠ¸ ê°„ ë°˜ë°•/ì§€ì§€ ë©”ì‹œì§€
    - from_agent_id: ë³´ë‚¸ ìª½
    - to_agent_id  : ë°›ëŠ” ìª½
    - stance       : REBUT(ë°˜ë°•) | SUPPORT(ì§€ì§€)
    - message      : ê·¼ê±° í…ìŠ¤íŠ¸(ê°„ê²° ìš”ì•½)
    """
    from_agent_id: str
    to_agent_id: str
    stance: Literal["REBUT", "SUPPORT"]
    message: str  # TODO: LLM/ë£° ê¸°ë°˜ í•œ ì¤„ ê·¼ê±° ìƒì„±

@dataclass
class RoundLog:
    """ë¼ìš´ë“œë³„ ê¸°ë¡ ìŠ¤ëƒ…ìƒ·(ì˜µì…”ë„ë¡œ ì‚¬ìš©)
    - round_no : ë¼ìš´ë“œ ë²ˆí˜¸
    - opinions : ë¼ìš´ë“œ ë‚´ ê° ì—ì´ì „íŠ¸ ìµœì¢… Opinion
    - rebuttals: ë¼ìš´ë“œ ë‚´ êµí™˜ëœ ë°˜ë°•/ì§€ì§€
    - summary  : {"agent_id": Target(...)} í˜•íƒœì˜ ì§‘ê³„ ìš”ì•½
    """
    round_no: int
    opinions: List[Opinion]
    rebuttals: List[Rebuttal]
    summary: Dict[str, Target]

@dataclass
class StockData:
    """ì—ì´ì „íŠ¸ ì…ë ¥ ì›ì²œ ë°ì´í„°(í•„ìš” ì‹œ ììœ  í™•ì¥)
    - sentimental: ì‹¬ë¦¬/ì»¤ë®¤ë‹ˆí‹°/ë‰´ìŠ¤ ìŠ¤ëƒ…ìƒ·
    - fundamental: ì¬ë¬´/ë°¸ë¥˜ì—ì´ì…˜ ìš”ì•½
    - technical  : ê°€ê²©/ì§€í‘œ ìŠ¤ëƒ…ìƒ·
    - last_price : ìµœì‹  ì¢…ê°€
    - currency   : í†µí™”ì½”ë“œ
    """
    sentimental: Dict
    fundamental: Dict
    technical: Dict
    last_price: Optional[float] = None
    currency: Optional[str] = None


# ==============================================================================
# SentimentalAgent í´ë˜ìŠ¤ êµ¬í˜„
# ==============================================================================
class SentimentalAgent(BaseAgent):
    def __init__(self, agent_id: str, input_features: int = 5):
        super().__init__(agent_id)
        
        # === [FinBERT ë¡œë“œ] ===
        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            self.finbert_tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
            self.finbert_model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
            self.finbert_loaded = True
            print("FinBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except ImportError:
            self.finbert_loaded = False
            print("ê²½ê³ : transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ FinBERT ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì‘ë™")
        # ================================
        
        self.input_features = input_features
        
        # ğŸŒŸ ì‹¤ì œ ëª¨ë¸ êµ¬ì¡° ì •ì˜ (ì„ì‹œ ëª¨ë¸ ëŒ€ì²´)
        # ğŸ’¡ ì°¸ê³ : ëª¨ë¸ ë¡œë“œ ì‹œ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ì •ì˜í•´ì•¼ í•¨
        self.model = nn.Sequential(
            nn.Linear(input_features, 32),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(32, 1) # ì£¼ê°€ ë³€ë™ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ì¶œë ¥ ë ˆì´ì–´
        )
        
        # ğŸŒŸ ì‹¤ì œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        try:
            if os.path.exists(MODEL_PATH):
                self.model.load_state_dict(torch.load(MODEL_PATH))
                self.model.eval() # ì¶”ë¡  ëª¨ë“œ ì„¤ì • (Dropout ìœ ì§€)
                print(f"**ì‹¤ì œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ:** {MODEL_PATH}")
            else:
                print(f"ê²½ê³ : ëª¨ë¸ íŒŒì¼ '{MODEL_PATH}'ì´(ê°€) ì—†ì–´ ì„ì‹œ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì„ì‹œ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
# MODEL_PATHë¥¼ SentimentalAgent í´ë˜ìŠ¤ ì™¸ë¶€ì— ì •ì˜í•˜ê±°ë‚˜ í´ë˜ìŠ¤ ìƒìˆ˜ë¡œ ì •ì˜í•´ì•¼ í•¨
MODEL_PATH = 'sentimental_model.pth'

    # ==========================================================================
    # ğŸŒŸ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì½”ë“œ í†µí•©
    # ==========================================================================

    # load_status í•¨ìˆ˜
    def load_status(self):
        if os.path.exists(STATUS_FILE):
            with open(STATUS_FILE, 'r') as f:
                return json.load(f)
        return {'completed_symbols': []}

    # save_status í•¨ìˆ˜
    def save_status(self, status):
        with open(STATUS_FILE, 'w') as f:
            json.dump(status, f, indent=4)

    # collect_news_data_eodhd í•¨ìˆ˜ (ë°°ì¹˜ ìˆ˜ì§‘ìš©)
    def collect_news_data_eodhd_batch(self, ticker, from_date, to_date):
        """
        EOD Historical Data APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ê¸°ê°„ì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ï¼ˆê°ì„± ì ìˆ˜ í¬í•¨ï¼‰
        """
        all_news = []
        offset = 0
        limit = 1000 

        while True:
            params = {
                's': ticker,
                'from': from_date,
                'to': to_date,
                'api_token': API_KEY,
                'limit': limit,
                'offset': offset,
                'extended': 1 # ê°ì„± ì ìˆ˜ í¬í•¨í•´ì„œ ì €ì¥ (FinBERT í•™ìŠµìš© ë°ì´í„°)
            }

            try:
                response = requests.get(BASE_URL_EODHD, params=params, timeout=30)
            except requests.exceptions.RequestException as e:
                print(f"[{ticker}] API í˜¸ì¶œ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜/íƒ€ì„ì•„ì›ƒ ë°œìƒ: {e}")
                return all_news, offset
                
            if response.status_code == 200:
                news_list = response.json()
                if not news_list:
                    print(f"[{ticker}] ë” ì´ìƒ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break 

                for news in news_list:
                    data = {
                        'date': news.get('date', ''),
                        'title': news.get('title', ''),
                        'summary': news.get('content', ''), 
                        'related': news.get('symbols', ticker), 
                        'ticker': ticker,
                        'sentiment_score': news.get('sentiment', '')
                    }
                    all_news.append(data)

                if len(news_list) < limit:
                    break 
                else:
                    offset += limit
                    time.sleep(1)
                    
            else:
                print(f"[{ticker}] API í˜¸ì¶œ ì˜¤ë¥˜ {response.status_code} - {response.text}")
                print(f"[{ticker}] ì˜¤í”„ì…‹ {offset}ì—ì„œ ìˆ˜ì§‘ ì¤‘ë‹¨.")
                return all_news, offset
                
        return all_news, -1 

    # save_news_to_csv í•¨ìˆ˜
    def save_news_to_csv(self, news_data, filename, mode='a'):
        fieldnames=['date', 'title', 'summary', 'related', 'ticker', 'sentiment_score']
        file_exists = os.path.exists(filename) and mode == 'a' 
        
        with open(filename, mode=mode, newline='', encoding='utf-8') as file:
            writer = csv.DictWriter(file, fieldnames=fieldnames)
            if mode == 'w' or not file_exists:
                writer.writeheader()
            
            for record in news_data:
                writer.writerow(record)
        print(f"ë°ì´í„° {len(news_data)}ê°œë¥¼ {filename} íŒŒì¼ì— ì €ì¥ ì™„ë£Œ")

    # ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
    def collect_historical_data(self, tickers: List[str]):
        """
        5ë…„ì¹˜ ë‰´ìŠ¤ ë° ì£¼ê°€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  CSV íŒŒì¼ë¡œ ì €ì¥
        (ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ì¼íšŒì„±/ë°°ì¹˜ ìˆ˜ì§‘ ë©”ì„œë“œ)
        """
        from_date = '2020-01-01'
        to_date_news = '2024-12-31'
        to_date_stock = '2025-01-01'

        print(f"**ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„:** {from_date} ë¶€í„° {to_date_news} ê¹Œì§€")

        NEWS_FILE = "news_data.csv"
        STOCK_FILE = "stock_data.csv"

        # 1. ê¸°ì¡´ íŒŒì¼ ì‚­ì œ (ì™„ì „ ì´ˆê¸°í™”)
        if os.path.exists(NEWS_FILE):
            os.remove(NEWS_FILE)
            print(f"ê¸°ì¡´ {NEWS_FILE} íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
        if os.path.exists(STATUS_FILE):
            os.remove(STATUS_FILE)
            print(f"ê¸°ì¡´ {STATUS_FILE} íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
            
        # 2. ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
        print("\n--- ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (EODhd, ê°ì„± ì ìˆ˜ í¬í•¨) ---")
        is_first_symbol = True
        news_collection_successful = True

        for ticker in tickers:
            print(f"[{ticker}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
            
            collected_news, last_offset = self.collect_news_data_eodhd_batch(ticker, from_date, to_date_news)
            
            if collected_news:
                save_mode = 'w' if is_first_symbol else 'a'
                self.save_news_to_csv(collected_news, NEWS_FILE, mode=save_mode)
                is_first_symbol = False
                
            if last_offset != -1:
                news_collection_successful = False
                print(f"[{ticker}] ìˆ˜ì§‘ì´ ì¤‘ê°„ì— ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break
                
            print(f"[{ticker}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ.")

        # 3. ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ 
        print("\n--- ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ---")
        if news_collection_successful:
            all_stock_data = []
            for ticker in tickers:
                print(f"{ticker} ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ (yfinance)...")
                df = yf.download(ticker, start=from_date, end=to_date_stock)
                if isinstance(df.columns, pd.MultiIndex):
                    df.columns = [col[0] for col in df.columns]
                df = df.reset_index()
                df['Symbol'] = ticker
                df = df[['Symbol', 'Date', 'Open', 'Close']]
                all_stock_data.append(df)
                
            result = pd.concat(all_stock_data, ignore_index=True)
            result.to_csv(STOCK_FILE, index=False, encoding='utf-8')
            print("stock_data.csv íŒŒì¼ ì €ì¥ ì™„ë£Œ")
        else:
            print("ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì§€ ì•Šì•„ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ì€ ê±´ë„ˆëœë‹ˆë‹¤.")


    # ==========================================================================
    # ì‹¤ì‹œê°„ ì˜ˆì¸¡
    # ==========================================================================

    # --------------------------------------------------------------------------
    # ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜: FinBERT ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
    # --------------------------------------------------------------------------
    def _simulate_finbert_analysis(self, texts: List[str]) -> Dict[str, Any]:
        """
        FinBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ëª©ë¡ì„ ë¶„ì„í•˜ê³  ëª¨ë¸ ì…ë ¥ íŠ¹ì§• ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ”
        ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜

        ì‹¤ì œ FinBERT êµ¬í˜„ ì‹œ, í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ê³  ëª¨ë¸ì— ë„£ì–´ ë²¡í„°(ì„ë² ë”©)ë¥¼ ì¶”ì¶œ
        """
        if not self.finbert_loaded:
            return {"avg_sentiment": 0.0, "vector": [0.5] * self.input_features}
            
        # í…ìŠ¤íŠ¸ ë¶„ì„ ë° ì ìˆ˜ ì§‘ê³„ ì‹œë®¬ë ˆì´ì…˜
        # ì—¬ê¸°ì„œëŠ” ê° í…ìŠ¤íŠ¸ê°€ [ê¸ì •, ì¤‘ë¦½, ë¶€ì •]ì˜ í™•ë¥ ì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
        
        # 1. í…ìŠ¤íŠ¸ ë¶„ì„ ë° ì ìˆ˜ ì§‘ê³„
        total_score = 0
        score_count = 0
        
        for text in texts:
            # FinBERT ë¶„ì„ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜
            # ê¸ì •/ì¤‘ë¦½/ë¶€ì • í™•ë¥ ì„ ì‹œë®¬ë ˆì´ì…˜ (ì˜ˆ: 3ì°¨ì› ë²¡í„°)
            pos = random.uniform(0, 1)
            neg = random.uniform(0, 1)
            neu = random.uniform(0, 1)
            
            # ì •ê·œí™” (FinBERTëŠ” ë³´í†µ softmax ì¶œë ¥)
            total = pos + neg + neu
            if total > 0:
                pos, neg, neu = pos / total, neg / total, neu / total
            
            # ìµœì¢… ê°ì„± ì ìˆ˜ ê³„ì‚° (ì˜ˆ: ê¸ì • - ë¶€ì •)
            current_score = pos - neg 
            total_score += current_score
            score_count += 1
            
        avg_sentiment = total_score / score_count if score_count > 0 else 0.0

        # 2. ëª¨ë¸ ì…ë ¥ ë²¡í„° ìƒì„± (ì˜ˆì‹œ: í‰ê·  ê°ì„± ì ìˆ˜ì™€ ëª‡ ê°€ì§€ ì¶”ê°€ íŠ¹ì§•)
        # 5ì°¨ì› ì…ë ¥ íŠ¹ì§•ì´ë¼ê³  ê°€ì •í•˜ê³ , FinBERT ì„ë² ë”© ê²°ê³¼ 3ì°¨ì› + ì¶”ê°€ íŠ¹ì§• 2ì°¨ì›ì„ ê²°í•©í•œë‹¤ê³  ì‹œë®¬ë ˆì´ì…˜
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ avg_sentimentë¥¼ ë³µì‚¬í•œ ë²¡í„°ë¡œ ëŒ€ì²´
        sentiment_vector = [avg_sentiment] * self.input_features

        return {
            "avg_sentiment": avg_sentiment,
            "vector": sentiment_vector
        }
    
    # --------------------------------------------------------------------------
    # ì‹¤ì‹œê°„ ê°ì„± ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ (searcherë¥¼ ìœ„í•œ ìµœì‹  ë°ì´í„° í˜¸ì¶œ)
    # --------------------------------------------------------------------------
    def _fetch_latest_sentiment_data(self, ticker: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ì¢…ëª©ì˜ ìµœì‹ (ìµœê·¼ 1ì¼) ë‰´ìŠ¤ í…ìŠ¤íŠ¸ë¥¼ APIë¡œ í˜¸ì¶œí•˜ê³ 
        FinBERT ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ëª¨ë¸ ì…ë ¥ ë²¡í„°ë¥¼ ë„ì¶œ
        """
        today = datetime.now().strftime('%Y-%m-%d')
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        params = {
            's': ticker,
            'from': yesterday,
            'to': today,
            'api_token': API_KEY, # ì‹¤ì œ í‚¤ë¡œ êµì²´ í•„ìš”
            'limit': 10, # ìµœì‹  10ê°œ ê¸°ì‚¬ë§Œ
            'extended': 0 # í…ìŠ¤íŠ¸ë§Œ í•„ìš”í•˜ë¯€ë¡œ extended=0 (ë˜ëŠ” ìƒëµ)
        }
        
        news_texts = []
        key_topics = set()

        try:
            response = requests.get(BASE_URL_EODHD, params=params, timeout=10)
            response.raise_for_status() 
            news_list = response.json()
            
            for news in news_list:
                title = news.get('title', '')
                content = news.get('content', '')
                
                # FinBERT ë¶„ì„ì„ ìœ„í•´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                if title:
                    news_texts.append(title)
                    key_topics.add(title)

                # ìš”ì•½ëœ FinBERT ì„ë² ë”© ê²°ê³¼ (vector)ë¥¼ ì‚¬ìš©
            if news_texts:
                finbert_result = self._simulate_finbert_analysis(news_texts)
                avg_sentiment = finbert_result['avg_sentiment']
                sentiment_vector = finbert_result['vector']
            else:
                avg_sentiment = 0.0
                sentiment_vector = [0.5] * self.input_features
                
            return {
                "news_sentiment_score": avg_sentiment, # FinBERT ë¶„ì„ ê²°ê³¼
                "community_score": random.uniform(-0.5, 0.5), # ì—¬ì „íˆ ë”ë¯¸
                "sentiment_vector": sentiment_vector,
                "key_topics": list(key_topics)[:3] 
            }

        except requests.exceptions.RequestException as e:
            print(f"[{ticker}] API í˜¸ì¶œ ì˜¤ë¥˜ ë°œìƒ: {e}. ë”ë¯¸ ë°ì´í„° ì‚¬ìš©.")
            return {
                "news_sentiment_score": 0.0,
                "community_score": 0.0,
                "sentiment_vector": [0.5] * self.input_features, 
                "key_topics": [f"API ì˜¤ë¥˜: {e}"]
            }
        except Exception as e:
            print(f"[{ticker}] ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë”ë¯¸ ë°ì´í„° ì‚¬ìš©.")
            return {
                "news_sentiment_score": 0.0,
                "community_score": 0.0,
                "sentiment_vector": [0.5] * self.input_features, 
                "key_topics": ["ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜ ë°œìƒ"]
            }

    # --------------------------------------------------------------------------
    # forward ë©”ì„œë“œ êµ¬í˜„, dropout layer í¬í•¨
    # --------------------------------------------------------------------------
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ëª¨ë¸ì˜ ìˆœì „íŒŒë¥¼ ì •ì˜í•˜ë©°, Dropout Layerë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
        """
        return self.model(x)

    # --------------------------------------------------------------------------
    # agent.searcher(): predictë¡œ ë‚´ì¼ ì¢…ê°€ë¥¼ ì˜ˆì¸¡í• ë•Œ ì…ë ¥ê°’ì„ í˜¸ì¶œí•˜ëŠ” ë©”ì„œë“œ
    # --------------------------------------------------------------------------
    def searcher(self, ticker: str) -> StockData:
        """
        [í•„ìˆ˜ ë©”ì„œë“œ] íŠ¹ì • ì¢…ëª©(ticker)ì˜ ê°ì„± ë¶„ì„ ì˜ˆì¸¡ì— í•„ìš”í•œ ìµœì‹  ì…ë ¥ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ê³  ë°˜í™˜
        """
        sentimental_data = self._fetch_latest_sentiment_data(ticker)
        
        # ë§ˆì§€ë§‰ ì¢…ê°€ëŠ” yfinance ë“±ìœ¼ë¡œ ë³„ë„ í˜¸ì¶œì´ í•„ìš”í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ë”ë¯¸ê°’ ì‚¬ìš©
        last_price = 50000.0 * (1 + random.uniform(-0.01, 0.01)) 

        data = StockData(
            sentimental=sentimental_data,
            fundamental={},
            technical={},
            last_price=last_price,
            currency="KRW"
        )
        return data

    # --------------------------------------------------------------------------
    # agent.predictor(): ë‚´ì¼ ì¢…ê°€ë¥¼ Target í´ë˜ìŠ¤ë¡œ ë°˜í™˜í•˜ëŠ” ë©”ì„œë“œ, ëª¬í…Œ ì¹´ë¥¼ë¡œ ì˜ˆì¸¡ í¬í•¨
    # --------------------------------------------------------------------------
    def predictor(self, ticker: str) -> Target:
        """
        [í•„ìˆ˜ ë©”ì„œë“œ] ì…ë ¥ê°’ìœ¼ë¡œ ë‚´ì¼ì˜ ì¢…ê°€ë¥¼ ì˜ˆì¸¡í•˜ê³  Target í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë°˜í™˜
        ëª¬í…Œ ì¹´ë¥¼ë¡œ ì˜ˆì¸¡ í¬í•¨
        """
        # 1. ì…ë ¥ ë°ì´í„° ê²€ìƒ‰ (agent.searcher() í˜¸ì¶œ)
        stock_data = self.searcher(ticker)
        last_price = stock_data.last_price or 50000.0

        # 2. ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
        sentiment_vector = stock_data.sentimental["sentiment_vector"]
        input_data = torch.tensor([sentiment_vector], dtype=torch.float32)
        
        # 3. ëª¬í…Œ ì¹´ë¥¼ë¡œ ë“œë¡­ì•„ì›ƒ (Monte Carlo Dropout) ì‹œë®¬ë ˆì´ì…˜
        num_samples = 150
        predictions_raw = [] 

        for _ in range(num_samples):
            with torch.no_grad():
                price_change_rate = self.forward(input_data).item() 
                predictions_raw.append(price_change_rate)

        predictions_np = np.array(predictions_raw)
        predicted_prices = last_price * (1 + predictions_np)

        # 4. Target í´ë˜ìŠ¤ í•„ë“œ ê³„ì‚°
        next_close = float(np.mean(predicted_prices))
        uncertainty = float(np.std(predicted_prices))
        confidence = float(1.0 / (1.0 + uncertainty * 10))
        confidence = min(1.0, confidence)

        # í”¼ì³ì¤‘ìš”ë„ ì¶”ê°€
        ## ì²« ë²ˆì§¸ ì„ í˜• ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ ê°€ì ¸ì˜¤ê¸° (íŠ¹ì§• ìˆ˜: self.input_features)
        weights = self.model[0].weight.data.numpy() # (32, 5) í˜•íƒœ ê°€ì •

        ## ê° ì…ë ¥ í”¼ì²˜ì— ëŒ€í•œ í‰ê·  ì ˆëŒ“ê°’ ê°€ì¤‘ì¹˜ ê³„ì‚° (ì»¬ëŸ¼ë³„ í‰ê· )
        ## ì´ ê°’ì„ í”¼ì²˜ ì¤‘ìš”ë„ë¡œ ì‚¬ìš© (ê°„ë‹¨í•œ ëª¨ë¸ì—ì„œ ì¼ë°˜ì )
        feature_importances_np = np.mean(np.abs(weights), axis=0)
        feature_importances = [float(f) for f in feature_importances_np]

        # shap ì¶”ê°€
        import shap
        try:
            # SHAP Explainer ì •ì˜ (ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ëŠ” DeepExplainer ì‚¬ìš©)
            # ğŸ’¡ í›ˆë ¨ëœ ë°ì´í„° ìƒ˜í”Œì„ ì œê³µí•´ì•¼ ì •í™•ë„ê°€ ë†’ìŒ
            # í˜„ì¬ëŠ” ëª¨ë¸ ì •ì˜ê°€ nn.Sequentialì´ë¯€ë¡œ, í›ˆë ¨ëœ ë°ì´í„° ì¼ë¶€ë¥¼ ë°°ê²½(background)ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •
            
            # ë”ë¯¸ ë°°ê²½ ë°ì´í„° (ì‹¤ì œë¡œëŠ” í›ˆë ¨ ë°ì´í„°ì˜ ì¼ë¶€ì—¬ì•¼ í•¨)
            background_data = torch.randn(10, self.input_features) 
            explainer = shap.DeepExplainer(self.model, background_data)
            
            # 2. í˜„ì¬ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ SHAP ê°’ ê³„ì‚°
            shap_values_np = explainer.shap_values(input_data)[0] # (1, 5) í˜•íƒœ
            shap_values = [float(s) for s in shap_values_np[0]]
            
        except Exception as e:
        # shap ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        shap_values = []
        print(f"SHAP ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # 5. Target í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
        result = Target(
            next_close=next_close,
            uncertainty=uncertainty,
            confidence=confidence,
            idea={
                "sentiment_score": [stock_data.sentimental['news_sentiment_score']], 
                "feature_names": [f"feature_{i+1}" for i in range(self.input_features)],
                "related_news_summary": stock_data.sentimental["key_topics"],
                "mc_price_samples": [float(p) for p in predicted_prices[:5]],
                "feature_importances": feature_importances,
                "shap_values": shap_values
            }
        )
        
        return result

# ==============================================================================
# ì—ì´ì „íŠ¸ ì‚¬ìš© ì˜ˆì‹œ
# ==============================================================================
if __name__ == '__main__':
    print("="*50)
    print("## SentimentalAgent í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    print("="*50)
    
    # ----------------------------------------------------------------------
    # ğŸŒŸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸: ì›ë³¸ ì½”ë“œì˜ ê¸°ëŠ¥ì„ ì‹¤í–‰
    # ----------------------------------------------------------------------
    agent = SentimentalAgent(agent_id="SentimentalAgent")
    tickers_for_collection = ['NVDA', 'MSFT', 'AAPL']
    
    # ì´ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´ 5ë…„ì¹˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  news_data.csv ë° stock_data.csvë¥¼ ìƒì„±
    # agent.collect_historical_data(tickers=tickers_for_collection) 
    # print("\n[Historical Data Pipeline] ì‹¤í–‰ ì™„ë£Œ. CSV íŒŒì¼ í™•ì¸ í•„ìš”.")
    # print("-" * 50)


    # ----------------------------------------------------------------------
    # ğŸš€ í•„ìˆ˜ ë©”ì„œë“œ (searcher/predictor) í…ŒìŠ¤íŠ¸
    # ----------------------------------------------------------------------
    TEST_TICKER = "MSFT"

    # ì˜ˆì¸¡ ë©”ì„œë“œ í˜¸ì¶œ (ticker ì¸ì ì „ë‹¬) - agent.predictor() ì‚¬ìš©
    agent_for_inference = SentimentalAgent(agent_id="SentimentalAgent")
    target_result = agent_for_inference.predictor(ticker=TEST_TICKER)
    
    # searcher() í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_stock_data = agent_for_inference.searcher(TEST_TICKER) 

    print(f"## ìµœì¢… ê°ì„± ì˜ˆì¸¡ ê²°ê³¼ ({TEST_TICKER}) (Target í´ë˜ìŠ¤ ì¶œë ¥ í˜•íƒœ):")
    print(f"ìµœì‹  ì¢…ê°€ (ê°€ì •): {test_stock_data.last_price:.2f} {test_stock_data.currency}")
    print(f"news_sentiment_score (FinBERT ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼): {test_stock_data.sentimental['news_sentiment_score']:.2f}")
    print(f"next_close (ì˜ˆì¸¡ ì¢…ê°€): {target_result.next_close:.2f}")
    print(f"uncertainty (ë¶ˆí™•ì‹¤ì„±): {target_result.uncertainty:.4f}")
    print(f"confidence (ì‹ ë¢°ë„): {target_result.confidence:.4f}")
    print("\nidea (ì„¤ëª… ê°€ëŠ¥ì„±):")
    for key, value in target_result.idea.items():
        if isinstance(value, list) and len(value) > 3:
            print(f"  - {key}: {value[:3]}...")
        else:
            print(f"  - {key}: {value}")
    print("="*50)