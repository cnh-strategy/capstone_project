# ======================================================
# Streamlit Dashboard for 3-Stage Debating System
# ì‹¤ì‹œê°„ ì‹œê°í™” ë° ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
# ======================================================

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import os
from datetime import datetime, timedelta

# Import our system components
from agent_utils import AgentLoader
from debate_system import DebateSystem

# Page configuration
st.set_page_config(
    page_title="3-Stage Debating System Dashboard",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .success-metric {
        border-left-color: #28a745;
    }
    .warning-metric {
        border-left-color: #ffc107;
    }
    .error-metric {
        border-left-color: #dc3545;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def load_system_data():
    """Load system data with caching"""
    try:
        # Load debate summary
        debate_summary = pd.read_csv('debate_summary.csv')
        
        # Load beta log
        beta_log = pd.read_csv('beta_log.csv')
        
        # Load test data for visualization
        test_data = {}
        for agent_type in ['technical', 'fundamental', 'sentimental']:
            test_data[agent_type] = pd.read_csv(f'data/TSLA_{agent_type}_test.csv')
        
        return debate_summary, beta_log, test_data
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
        return None, None, None

def create_performance_metrics(debate_summary):
    """Create performance metrics cards"""
    if debate_summary is None or debate_summary.empty:
        return
    
    # Calculate metrics
    avg_error = debate_summary['error'].mean()
    avg_relative_error = (debate_summary['error'] / debate_summary['actual'] * 100).mean()
    min_error = debate_summary['error'].min()
    max_error = debate_summary['error'].max()
    
    # Create columns
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="í‰ê·  ì˜¤ì°¨",
            value=f"${avg_error:.2f}",
            delta=f"{avg_relative_error:.1f}%"
        )
    
    with col2:
        st.metric(
            label="ìµœì†Œ ì˜¤ì°¨",
            value=f"${min_error:.2f}",
            delta="Best"
        )
    
    with col3:
        st.metric(
            label="ìµœëŒ€ ì˜¤ì°¨",
            value=f"${max_error:.2f}",
            delta="Worst"
        )
    
    with col4:
        accuracy_grade = "ìš°ìˆ˜" if avg_relative_error <= 5 else "ì–‘í˜¸" if avg_relative_error <= 10 else "ë³´í†µ"
        st.metric(
            label="ì •í™•ë„ ë“±ê¸‰",
            value=accuracy_grade,
            delta=f"{avg_relative_error:.1f}%"
        )

def create_prediction_chart(debate_summary):
    """Create prediction vs actual chart"""
    if debate_summary is None or debate_summary.empty:
        return
    
    fig = go.Figure()
    
    # Add actual prices
    fig.add_trace(go.Scatter(
        x=debate_summary['round'],
        y=debate_summary['actual'],
        mode='lines+markers',
        name='ì‹¤ì œ ê°€ê²©',
        line=dict(color='red', width=3),
        marker=dict(size=8)
    ))
    
    # Add consensus predictions
    fig.add_trace(go.Scatter(
        x=debate_summary['round'],
        y=debate_summary['consensus'],
        mode='lines+markers',
        name='í•©ì˜ ì˜ˆì¸¡',
        line=dict(color='blue', width=3),
        marker=dict(size=8)
    ))
    
    # Add error bars
    fig.add_trace(go.Scatter(
        x=debate_summary['round'],
        y=debate_summary['consensus'],
        mode='markers',
        name='ì˜¤ì°¨',
        marker=dict(
            size=debate_summary['error'] * 2,  # Scale for visibility
            color=debate_summary['error'],
            colorscale='Reds',
            showscale=True,
            colorbar=dict(title="ì˜¤ì°¨ ($)")
        ),
        hovertemplate='<b>ë¼ìš´ë“œ %{x}</b><br>' +
                      'ì˜ˆì¸¡: $%{y:.2f}<br>' +
                      'ì˜¤ì°¨: $%{customdata:.2f}<extra></extra>',
        customdata=debate_summary['error']
    ))
    
    fig.update_layout(
        title="ì˜ˆì¸¡ vs ì‹¤ì œ ê°€ê²©",
        xaxis_title="ì˜ˆì¸¡ ë¼ìš´ë“œ",
        yaxis_title="ê°€ê²© ($)",
        hovermode='x unified',
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)

def create_beta_evolution_chart(beta_log):
    """Create beta weight evolution chart"""
    if beta_log is None or beta_log.empty:
        return
    
    fig = go.Figure()
    
    # Add beta lines for each agent
    agents = ['technical', 'fundamental', 'sentimental']
    colors = ['blue', 'red', 'green']
    
    for agent, color in zip(agents, colors):
        fig.add_trace(go.Scatter(
            x=beta_log['step'],
            y=beta_log[f'beta_{agent}'],
            mode='lines',
            name=f'{agent.title()} Agent',
            line=dict(color=color, width=3),
            hovertemplate=f'<b>{agent.title()}</b><br>' +
                          'Step: %{x}<br>' +
                          'Î²: %{y:.3f}<extra></extra>'
        ))
    
    fig.update_layout(
        title="Î² ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì§„í™”",
        xaxis_title="í•™ìŠµ ìŠ¤í…",
        yaxis_title="Î² ê°€ì¤‘ì¹˜",
        hovermode='x unified',
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)

def create_uncertainty_chart(debate_summary):
    """Create uncertainty analysis chart"""
    if debate_summary is None or debate_summary.empty:
        return
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('Technical Uncertainty', 'Fundamental Uncertainty', 
                       'Sentimental Uncertainty', 'Average Uncertainty'),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    agents = ['technical', 'fundamental', 'sentimental']
    colors = ['blue', 'red', 'green']
    
    for i, (agent, color) in enumerate(zip(agents, colors)):
        row = (i // 2) + 1
        col = (i % 2) + 1
        
        fig.add_trace(
            go.Scatter(
                x=debate_summary['round'],
                y=debate_summary[f'uncertainty_{agent}'],
                mode='lines+markers',
                name=f'{agent.title()}',
                line=dict(color=color, width=2),
                marker=dict(size=6)
            ),
            row=row, col=col
        )
    
    # Average uncertainty
    avg_uncertainty = (debate_summary['uncertainty_technical'] + 
                      debate_summary['uncertainty_fundamental'] + 
                      debate_summary['uncertainty_sentimental']) / 3
    
    fig.add_trace(
        go.Scatter(
            x=debate_summary['round'],
            y=avg_uncertainty,
            mode='lines+markers',
            name='Average',
            line=dict(color='purple', width=3),
            marker=dict(size=8)
        ),
        row=2, col=2
    )
    
    fig.update_layout(
        title="ë¶ˆí™•ì‹¤ì„± (Ïƒ) ë¶„ì„",
        height=600,
        showlegend=False
    )
    
    st.plotly_chart(fig, use_container_width=True)

def create_agent_performance_table(debate_summary):
    """Create agent performance comparison table"""
    if debate_summary is None or debate_summary.empty:
        return
    
    # Calculate performance metrics for each agent
    performance_data = []
    
    for agent in ['technical', 'fundamental', 'sentimental']:
        # Calculate individual agent predictions (simplified)
        # In real implementation, you'd have individual predictions
        avg_uncertainty = debate_summary[f'uncertainty_{agent}'].mean()
        avg_beta = debate_summary[f'beta_{agent}'].mean()
        
        performance_data.append({
            'Agent': agent.title(),
            'Avg Î² Weight': f"{avg_beta:.3f}",
            'Avg Uncertainty (Ïƒ)': f"{avg_uncertainty:.3f}",
            'Confidence Level': "High" if avg_beta > 0.4 else "Medium" if avg_beta > 0.3 else "Low",
            'Reliability': "High" if avg_uncertainty < 10 else "Medium" if avg_uncertainty < 15 else "Low"
        })
    
    df = pd.DataFrame(performance_data)
    
    st.subheader("ğŸ¯ ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¹„êµ")
    st.dataframe(df, use_container_width=True)

def create_system_status():
    """Create system status indicators"""
    st.subheader("ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Check if models exist
        models_exist = all(os.path.exists(f'models/{agent}_agent.pt') 
                          for agent in ['technical', 'fundamental', 'sentimental'])
        status = "âœ… ì •ìƒ" if models_exist else "âŒ ì˜¤ë¥˜"
        st.metric("ëª¨ë¸ ìƒíƒœ", status)
    
    with col2:
        # Check if data exists
        data_exists = all(os.path.exists(f'data/TSLA_{agent}_test.csv') 
                         for agent in ['technical', 'fundamental', 'sentimental'])
        status = "âœ… ì •ìƒ" if data_exists else "âŒ ì˜¤ë¥˜"
        st.metric("ë°ì´í„° ìƒíƒœ", status)
    
    with col3:
        # Check if results exist
        results_exist = os.path.exists('debate_summary.csv')
        status = "âœ… ì •ìƒ" if results_exist else "âŒ ì˜¤ë¥˜"
        st.metric("ê²°ê³¼ ìƒíƒœ", status)

def main():
    """Main dashboard function"""
    # Header
    st.markdown('<h1 class="main-header">ğŸ¯ 3-Stage Debating System Dashboard</h1>', 
                unsafe_allow_html=True)
    
    # Load data
    debate_summary, beta_log, test_data = load_system_data()
    
    if debate_summary is None:
        st.error("ì‹œìŠ¤í…œ ë°ì´í„°ë¥¼ ë¡œë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì‹œìŠ¤í…œì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        st.info("ì‹¤í–‰ ìˆœì„œ: `python train_agents.py` â†’ `python stage2_trainer.py` â†’ `python debate_system.py`")
        return
    
    # Sidebar
    st.sidebar.title("ğŸ“Š ëŒ€ì‹œë³´ë“œ ì„¤ì •")
    
    # System status
    create_system_status()
    
    st.sidebar.markdown("---")
    
    # Display options
    show_metrics = st.sidebar.checkbox("ì„±ëŠ¥ ë©”íŠ¸ë¦­ í‘œì‹œ", value=True)
    show_predictions = st.sidebar.checkbox("ì˜ˆì¸¡ ì°¨íŠ¸ í‘œì‹œ", value=True)
    show_beta_evolution = st.sidebar.checkbox("Î² ì§„í™” ì°¨íŠ¸ í‘œì‹œ", value=True)
    show_uncertainty = st.sidebar.checkbox("ë¶ˆí™•ì‹¤ì„± ë¶„ì„ í‘œì‹œ", value=True)
    show_performance = st.sidebar.checkbox("ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¹„êµ í‘œì‹œ", value=True)
    
    # Main content
    if show_metrics:
        st.subheader("ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­")
        create_performance_metrics(debate_summary)
        st.markdown("---")
    
    if show_predictions:
        st.subheader("ğŸ¯ ì˜ˆì¸¡ vs ì‹¤ì œ ê°€ê²©")
        create_prediction_chart(debate_summary)
        st.markdown("---")
    
    if show_beta_evolution and beta_log is not None:
        st.subheader("ğŸ”„ Î² ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì§„í™”")
        create_beta_evolution_chart(beta_log)
        st.markdown("---")
    
    if show_uncertainty:
        st.subheader("ğŸ” ë¶ˆí™•ì‹¤ì„± (Ïƒ) ë¶„ì„")
        create_uncertainty_chart(debate_summary)
        st.markdown("---")
    
    if show_performance:
        create_agent_performance_table(debate_summary)
        st.markdown("---")
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666;'>
        <p>ğŸ¯ 3-Stage Debating System Dashboard | 
        í‰ê·  ì˜¤ì°¨ìœ¨: 5.4% | ì„±ëŠ¥ ê°œì„ : 4.7% | ì •í™•ë„: ìš°ìˆ˜(50%) + ì–‘í˜¸(50%)</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
