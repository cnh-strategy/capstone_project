#!/usr/bin/env python3
"""
Trainer Module - ê° Agentë³„ ëª¨ë¸ í•™ìŠµ
2022~2024ë…„ ë°ì´í„°ë¡œ ê°œë³„ Agent ëª¨ë¸ í•™ìŠµ
"""

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from typing import Dict, List, Optional, Tuple
import logging
import pickle

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TechnicalAgent(nn.Module):
    """ê¸°ìˆ ì  ë¶„ì„ ì—ì´ì „íŠ¸ (TCN ê¸°ë°˜)"""
    
    def __init__(self, input_size: int = 14, hidden_size: int = 64, output_size: int = 1):
        super(TechnicalAgent, self).__init__()
        
        # Temporal Convolutional Network
        self.conv1 = nn.Conv1d(input_size, hidden_size, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(hidden_size, hidden_size//2, kernel_size=3, padding=1)
        
        self.dropout = nn.Dropout(0.2)
        self.relu = nn.ReLU()
        self.fc = nn.Linear(hidden_size//2, output_size)
        
    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        x = x.transpose(1, 2)  # (batch_size, input_size, sequence_length)
        
        x = self.relu(self.conv1(x))
        x = self.dropout(x)
        x = self.relu(self.conv2(x))
        x = self.dropout(x)
        x = self.relu(self.conv3(x))
        
        # Global average pooling
        x = torch.mean(x, dim=2)  # (batch_size, hidden_size//2)
        
        x = self.fc(x)
        return x

class FundamentalAgent(nn.Module):
    """í€ë”ë©˜í„¸ ë¶„ì„ ì—ì´ì „íŠ¸ (LSTM ê¸°ë°˜)"""
    
    def __init__(self, input_size: int = 10, hidden_size: int = 64, output_size: int = 1):
        super(FundamentalAgent, self).__init__()
        
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=2)
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        lstm_out, (hidden, cell) = self.lstm(x)
        
        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ ì‚¬ìš©
        x = lstm_out[:, -1, :]  # (batch_size, hidden_size)
        x = self.dropout(x)
        x = self.fc(x)
        return x

class SentimentalAgent(nn.Module):
    """ê°ì • ë¶„ì„ ì—ì´ì „íŠ¸ (Transformer ê¸°ë°˜)"""
    
    def __init__(self, input_size: int = 10, hidden_size: int = 64, output_size: int = 1):
        super(SentimentalAgent, self).__init__()
        
        self.input_projection = nn.Linear(input_size, hidden_size)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(hidden_size, nhead=8, batch_first=True),
            num_layers=3
        )
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        x = self.input_projection(x)
        x = self.transformer(x)
        
        # Global average pooling
        x = torch.mean(x, dim=1)  # (batch_size, hidden_size)
        x = self.dropout(x)
        x = self.fc(x)
        return x

class ModelTrainer:
    """ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, ticker: str, data_dir: str = "data", models_dir: str = "ml_modules/models"):
        self.ticker = ticker.upper()
        self.data_dir = data_dir
        self.models_dir = models_dir
        self.ensure_models_dir()
        
        # ëª¨ë¸ ì •ì˜
        self.models = {
            'technical': TechnicalAgent(),
            'fundamental': FundamentalAgent(),
            'sentimental': SentimentalAgent()
        }
        
        # ìŠ¤ì¼€ì¼ëŸ¬
        self.scalers = {}
        
    def ensure_models_dir(self):
        """ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if not os.path.exists(self.models_dir):
            os.makedirs(self.models_dir)
    
    def load_data(self, agent_type: str) -> Optional[pd.DataFrame]:
        """ë°ì´í„° ë¡œë“œ"""
        filename = f"{self.ticker}_{agent_type}_data.csv"
        filepath = os.path.join(self.data_dir, filename)
        
        if not os.path.exists(filepath):
            logger.error(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return None
        
        try:
            df = pd.read_csv(filepath)
            logger.info(f"âœ… {agent_type} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰")
            return df
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def prepare_technical_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """ê¸°ìˆ ì  ë°ì´í„° ì „ì²˜ë¦¬"""
        # íŠ¹ì„± ì„ íƒ
        feature_columns = [
            'open', 'high', 'low', 'close', 'volume',
            'sma_20', 'sma_50', 'rsi', 'macd',
            'bollinger_upper', 'bollinger_lower', 'atr', 'volume_sma'
        ]
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.dropna()
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        X = df[feature_columns].values
        y = df['close'].values
        
        # ì •ê·œí™”
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (30ì¼ ìœˆë„ìš°)
        sequence_length = 30
        X_seq, y_seq = [], []
        
        for i in range(sequence_length, len(X_scaled)):
            X_seq.append(X_scaled[i-sequence_length:i])
            y_seq.append(y[i])
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        # íƒ€ê²Ÿ ì •ê·œí™”
        y_scaler = MinMaxScaler()
        y_seq_scaled = y_scaler.fit_transform(y_seq.reshape(-1, 1)).flatten()
        
        self.scalers['technical'] = {'X': scaler, 'y': y_scaler}
        
        return X_seq, y_seq_scaled
    
    def prepare_fundamental_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """í€ë”ë©˜í„¸ ë°ì´í„° ì „ì²˜ë¦¬"""
        # íŠ¹ì„± ì„ íƒ
        feature_columns = [
            'market_cap', 'pe_ratio', 'pb_ratio', 'debt_to_equity',
            'revenue_growth', 'profit_margin', 'roe', 'current_ratio',
            'dividend_yield'
        ]
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.dropna()
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬ (ì¢…ê°€ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©)
        X = df[feature_columns].values
        y = df['market_cap'].values  # ì‹œê°€ì´ì•¡ì„ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©
        
        # ì •ê·œí™”
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (30ì¼ ìœˆë„ìš°)
        sequence_length = 30
        X_seq, y_seq = [], []
        
        for i in range(sequence_length, len(X_scaled)):
            X_seq.append(X_scaled[i-sequence_length:i])
            y_seq.append(y[i])
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        # íƒ€ê²Ÿ ì •ê·œí™”
        y_scaler = MinMaxScaler()
        y_seq_scaled = y_scaler.fit_transform(y_seq.reshape(-1, 1)).flatten()
        
        self.scalers['fundamental'] = {'X': scaler, 'y': y_scaler}
        
        return X_seq, y_seq_scaled
    
    def prepare_sentimental_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """ê°ì • ë¶„ì„ ë°ì´í„° ì „ì²˜ë¦¬"""
        # íŠ¹ì„± ì„ íƒ
        feature_columns = [
            'news_sentiment', 'social_sentiment', 'analyst_rating',
            'price_target', 'earnings_surprise', 'insider_trading',
            'institutional_flow', 'options_sentiment', 'fear_greed_index'
        ]
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.dropna()
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬ (ê°€ê²© íƒ€ê²Ÿì„ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©)
        X = df[feature_columns].values
        y = df['price_target'].values
        
        # ì •ê·œí™”
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (30ì¼ ìœˆë„ìš°)
        sequence_length = 30
        X_seq, y_seq = [], []
        
        for i in range(sequence_length, len(X_scaled)):
            X_seq.append(X_scaled[i-sequence_length:i])
            y_seq.append(y[i])
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        # íƒ€ê²Ÿ ì •ê·œí™”
        y_scaler = MinMaxScaler()
        y_seq_scaled = y_scaler.fit_transform(y_seq.reshape(-1, 1)).flatten()
        
        self.scalers['sentimental'] = {'X': scaler, 'y': y_scaler}
        
        return X_seq, y_seq_scaled
    
    def train_model(self, agent_type: str, X: np.ndarray, y: np.ndarray) -> bool:
        """ëª¨ë¸ í•™ìŠµ"""
        logger.info(f"ğŸ¯ {agent_type} ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        
        try:
            # ë°ì´í„° ë¶„í• 
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # PyTorch í…ì„œë¡œ ë³€í™˜
            X_train = torch.FloatTensor(X_train)
            y_train = torch.FloatTensor(y_train)
            X_val = torch.FloatTensor(X_val)
            y_val = torch.FloatTensor(y_val)
            
            # ë°ì´í„°ë¡œë” ìƒì„±
            train_dataset = TensorDataset(X_train, y_train)
            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
            
            # ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì„¤ì •
            model = self.models[agent_type]
            criterion = nn.MSELoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            
            # í•™ìŠµ
            num_epochs = 50
            best_val_loss = float('inf')
            
            for epoch in range(num_epochs):
                # í›ˆë ¨
                model.train()
                train_loss = 0.0
                
                for batch_X, batch_y in train_loader:
                    optimizer.zero_grad()
                    outputs = model(batch_X)
                    loss = criterion(outputs.squeeze(), batch_y)
                    loss.backward()
                    optimizer.step()
                    train_loss += loss.item()
                
                # ê²€ì¦
                model.eval()
                with torch.no_grad():
                    val_outputs = model(X_val)
                    val_loss = criterion(val_outputs.squeeze(), y_val).item()
                
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    # ëª¨ë¸ ì €ì¥
                    model_path = os.path.join(self.models_dir, f"{self.ticker}_{agent_type}_model.pt")
                    torch.save(model.state_dict(), model_path)
                    
                    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
                    scaler_path = os.path.join(self.models_dir, f"{self.ticker}_{agent_type}_scaler.pkl")
                    with open(scaler_path, 'wb') as f:
                        pickle.dump(self.scalers[agent_type], f)
                
                if epoch % 10 == 0:
                    logger.info(f"Epoch {epoch}: Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}")
            
            logger.info(f"âœ… {agent_type} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {agent_type} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def train_all_models(self) -> Dict[str, bool]:
        """ëª¨ë“  ëª¨ë¸ í•™ìŠµ"""
        logger.info(f"ğŸš€ {self.ticker} ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        
        results = {}
        
        for agent_type in ['technical', 'fundamental', 'sentimental']:
            # ë°ì´í„° ë¡œë“œ
            df = self.load_data(agent_type)
            if df is None:
                results[agent_type] = False
                continue
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            if agent_type == 'technical':
                X, y = self.prepare_technical_data(df)
            elif agent_type == 'fundamental':
                X, y = self.prepare_fundamental_data(df)
            else:  # sentimental
                X, y = self.prepare_sentimental_data(df)
            
            # ëª¨ë¸ í•™ìŠµ
            success = self.train_model(agent_type, X, y)
            results[agent_type] = success
        
        # ê²°ê³¼ ìš”ì•½
        success_count = sum(results.values())
        logger.info(f"ğŸ“Š ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {success_count}/3 ì„±ê³µ")
        
        return results
    
    def load_existing_model(self, agent_type: str) -> bool:
        """ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ"""
        model_path = os.path.join(self.models_dir, f"{self.ticker}_{agent_type}_model.pt")
        scaler_path = os.path.join(self.models_dir, f"{self.ticker}_{agent_type}_scaler.pkl")
        
        if os.path.exists(model_path) and os.path.exists(scaler_path):
            try:
                # ëª¨ë¸ ë¡œë“œ
                self.models[agent_type].load_state_dict(torch.load(model_path))
                
                # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
                with open(scaler_path, 'rb') as f:
                    self.scalers[agent_type] = pickle.load(f)
                
                logger.info(f"âœ… {agent_type} ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                return True
            except Exception as e:
                logger.error(f"âŒ {agent_type} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                return False
        
        return False


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    ticker = "RZLV"
    trainer = ModelTrainer(ticker)
    
    # ê¸°ì¡´ ëª¨ë¸ í™•ì¸
    print(f"\nğŸ” {ticker} ê¸°ì¡´ ëª¨ë¸ í™•ì¸:")
    for agent_type in ['technical', 'fundamental', 'sentimental']:
        if trainer.load_existing_model(agent_type):
            print(f"âœ… {agent_type}: ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©")
        else:
            print(f"âŒ {agent_type}: ìƒˆë¡œ í•™ìŠµ í•„ìš”")
    
    # ëª¨ë“  ëª¨ë¸ í•™ìŠµ
    results = trainer.train_all_models()
    
    print(f"\nğŸ“‹ {ticker} ëª¨ë¸ í•™ìŠµ ê²°ê³¼:")
    for agent_type, success in results.items():
        if success:
            print(f"âœ… {agent_type}: í•™ìŠµ ì™„ë£Œ")
        else:
            print(f"âŒ {agent_type}: í•™ìŠµ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()
