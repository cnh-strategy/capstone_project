import json
import os
from collections import defaultdict
from typing import Optional, List, Dict

import requests
import torch
import yfinance as yf
from typing import Dict, List, Optional, Literal, Tuple, Any
import warnings
from dataclasses import dataclass, field
import joblib
import pandas as pd
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler, MinMaxScaler

from agents.dump import CAPSTONE_OPENAI_API
from agents.macro_shap_llm import LLMExplainer, AttributionAnalyzer
from agents.macro_sub import get_std_pred
from debate_ver4.prompts import REBUTTAL_PROMPTS
from debate_ver4.config.agents import dir_info, agents_info

warnings.filterwarnings("ignore", category=FutureWarning)

# ============================================================
# ê³µí†µ ì„¤ì •
# ============================================================
OUTPUT_DIR = "./data"
os.makedirs(OUTPUT_DIR, exist_ok=True)

START_DATE = "2020-01-01"
END_DATE = '2024-12-31'

save_dir=dir_info["data_dir"]
model_dir: str = dir_info["model_dir"]
data_dir: str = dir_info["data_dir"]



@dataclass
class Target:
    """ì˜ˆì¸¡ ëª©í‘œê°’ + ë¶ˆí™•ì‹¤ì„± ì •ë³´ í¬í•¨
    - next_close: ë‹¤ìŒ ê±°ë˜ì¼ ì¢…ê°€ ì˜ˆì¸¡ì¹˜
    - uncertainty: Monte Carlo Dropout ê¸°ë°˜ ì˜ˆì¸¡ í‘œì¤€í¸ì°¨(Ïƒ)
    - confidence: ëª¨ë¸ ì‹ ë¢°ë„ Î² (ì •ê·œí™”ëœ ì‹ ë¢°ë„; ì„ íƒì )
    """
    next_close: float
    uncertainty: Optional[float] = None
    confidence: Optional[float] = None

@dataclass
class Opinion:
    agent_id: str
    target: Target
    reason: str

@dataclass
class Rebuttal:
    from_agent_id: str
    to_agent_id: str
    stance: Literal["REBUT", "SUPPORT"]
    message: str

@dataclass
class RoundLog:
    round_no: int
    opinions: List[Opinion]
    rebuttals: List[Rebuttal]
    summary: Dict[str, Target]

@dataclass
class StockData:
    """ì—ì´ì „íŠ¸ ì…ë ¥ ì›ì²œ ë°ì´í„°(í•„ìš” ì‹œ ììœ  í™•ì¥)
    - sentimental: ì‹¬ë¦¬/ì»¤ë®¤ë‹ˆí‹°/ë‰´ìŠ¤ ìŠ¤ëƒ…ìƒ·
    - fundamental: ì¬ë¬´/ë°¸ë¥˜ì—ì´ì…˜ ìš”ì•½
    - technical  : ê°€ê²©/ì§€í‘œ ìŠ¤ëƒ…ìƒ·
    - last_price : ìµœì‹  ì¢…ê°€
    - currency   : í†µí™”ì½”ë“œ
    """
    SentimentalAgent: Optional[Dict[str, Any]] = field(default_factory=dict)
    FundamentalAgent: Optional[Dict[str, Any]] = field(default_factory=dict)
    TechnicalAgent: Optional[Dict[str, Any]] = field(default_factory=dict)
    last_price: Optional[float] = None
    currency: Optional[str] = None


# ============================================================
# MacroSentimentAgent â€” ì‹œì¥Â·ê±°ì‹œê²½ì œ ì‹œê³„ì—´ ê¸°ë°˜
# ============================================================
class MacroSentimentAgentDataset:
    OPENAI_URL = "https://api.openai.com/v1/responses"

    def __init__(self,
                 agent_id="MacroSentiAgent",
                 preferred_models: Optional[List[str]] = None,
                 option_model: Optional[str] = None,
                 verbose: bool = False,
                 temperature: float = 0.2,
                 ticker = None,
                 **kwargs
                 ):
        self.temperature = None
        self.target = None
        self.opinions = None
        self.macro_tickers = {
            "SPY": "SPY", "QQQ": "QQQ", "^GSPC": "^GSPC", "^DJI": "^DJI", "^IXIC": "^IXIC",
            "^TNX": "^TNX", "^IRX": "^IRX", "^FVX": "^FVX",
            "^VIX": "^VIX",
            "DX-Y.NYB": "DX-Y.NYB",
            "EURUSD=X": "EURUSD=X", "USDJPY=X": "USDJPY=X",
            "GC=F": "GC=F", "CL=F": "CL=F", "HG=F": "HG=F",
          #  "BTC-USD": "BTC-USD", "ETH-USD": "ETH-USD"
        }
        self.data = None
        self.agent_id = 'MacroSentiAgent'
        self.ticker_name = ticker


        self.tickers = [self.ticker_name] or ["AAPL", "MSFT", "NVDA"]
        # self.target_tickers = target_tickers or ["AAPL", "MSFT", "NVDA"]

        self.model = None
        self.scaler_X = None
        self.scaler_y = None
        self.macro_df = None
        self.pred_df = None
        self.X_scaled = None

        self.window_size = 40
        # ëª¨ë¸ í´ë°± ìš°ì„ ìˆœìœ„
        self.preferred_models = preferred_models or ["gpt-5-mini", "gpt-4.1-mini"]
        if option_model:
            self.preferred_models = [option_model] + [
                m for m in self.preferred_models if m != option_model
            ]

        # ê³µí†µ í—¤ë”
        self.api_key = CAPSTONE_OPENAI_API
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        self.temperature = temperature # Temperature ì„¤ì •
        self.verbose = verbose            # ë””ë²„ê¹… ëª¨ë“œ
        self.rebuttals: Dict[int, List[Rebuttal]] = defaultdict(list)

    def fetch_data(self):
        """ë‹¤ì¤‘ í‹°ì»¤ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        df = yf.download(
            tickers=list(self.macro_tickers.values()),
            start=START_DATE,
            end=END_DATE,
            interval="1d",
            group_by="ticker",
            auto_adjust=False
        )

        # âœ… pandas ë²„ì „/êµ¬ì¡° ê´€ê³„ì—†ì´ ì¼ê´€ëœ í¬ë§·ìœ¼ë¡œ ë³€í™˜
        # MultiIndex êµ¬ì¡°ì¼ ê²½ìš° (í‹°ì»¤ë³„ë¡œ OHLCV ì¡´ì¬)
        if isinstance(df.columns, pd.MultiIndex):
            # êµ¬ì¡°ë¥¼ (ë‚ ì§œ, í‹°ì»¤, ê°’) í˜•íƒœë¡œ ë³€í™˜
            df = df.stack(level=0)
            df.index.names = ["Date", "Ticker"]
            df.sort_index(inplace=True)

            # ì»¬ëŸ¼ ì´ë¦„ í‰íƒ„í™”
            df.columns = [col for col in df.columns]
            df = df.unstack(level="Ticker")
            df.columns = ["_".join(col).strip() for col in df.columns.values]
        else:
            # ë‹¨ì¼ ì¸ë±ìŠ¤ êµ¬ì¡°ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            df.index.name = "Date"

        self.data = df
        print(f"[MacroSentimentAgent] Data shape: {df.shape}, Columns: {len(df.columns)}")
        return df

    def add_features(self):
        """ìˆ˜ìµë¥ , ê¸ˆë¦¬ì°¨, ìœ„í—˜ì‹¬ë¦¬ ë“± ê³„ì‚°"""
        df = self.data.copy()

        # ê° ìì‚°ì˜ 1ì¼ ìˆ˜ìµë¥ 
        for ticker in self.macro_tickers.values():
            if (ticker, "Close") in df.columns:
                df[(ticker, "ret_1d")] = df[(ticker, "Close")].pct_change()

        # ê¸ˆë¦¬ ìŠ¤í”„ë ˆë“œ (10ë…„ - 3ê°œì›”)
        if ("^TNX", "Close") in df.columns and ("^IRX", "Close") in df.columns:
            df[("macro", "Yield_spread")] = df[("^TNX", "Close")] - df[("^IRX", "Close")]

        # ì‹œì¥ ìœ„í—˜ì‹¬ë¦¬ (SPY - DXY - VIX)
        if ("SPY", "ret_1d") in df.columns and ("DX-Y.NYB", "ret_1d") in df.columns and ("^VIX", "ret_1d") in df.columns:
            df[("macro", "Risk_Sentiment")] = (
                    df[("SPY", "ret_1d")] - df[("DX-Y.NYB", "ret_1d")] - df[("^VIX", "ret_1d")]
            )

        self.data = df
        return df

    def save_csv(self):
        path = os.path.join(OUTPUT_DIR, "macro_data/macro_sentiment.csv")
        self.data.to_csv(path, index=True)
        print(f"[MacroSentimentAgent] Saved {path}")


    def close_price_fetch(self, ticker_name):
        # ì—¬ëŸ¬ ì¢…ëª©ì˜ ì¼ë³„ ì¢…ê°€ ë¶ˆëŸ¬ì˜¤ê¸° (2020-01-01 ~ 2024-12-31)
        df_prices = yf.download(
            ticker_name,
            start="2020-01-01",
            end="2025-01-03"
        )["Close"]

        # CSV ì €ì¥
        df_prices.to_csv(f"data/macro_data/daily_closePrice_{ticker_name}.csv")

        print("ì €ì¥ ì™„ë£Œ:", df_prices.shape, "rows")

    # ì£¼ê°€ì™€ ë§¤í¬ë¡œ ë°ì´í„°ë¥¼ ë³‘í•© + ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥
    def make_dataset_seq(self, ticker_name):
        self.ticker_name = ticker_name
        # -------------------------------------------------------------
        # 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
        # -------------------------------------------------------------
        macro_df = pd.read_csv(f"data/macro_data/macro_sentiment.csv")
        price_df = pd.read_csv(f"data/macro_data/daily_closePrice_{ticker_name}.csv")

        macro_df['Date'] = pd.to_datetime(macro_df['Date'])
        price_df['Date'] = pd.to_datetime(price_df['Date'])

        # -------------------------------------------------------------
        # 2. ë§¤í¬ë¡œ í”¼ì²˜ í™•ì¥ (ì›ë³¸ + ë³€í™”ìœ¨)
        # -------------------------------------------------------------
        macro_features = [c for c in macro_df.columns if c != 'Date']
        macro_ret = macro_df[macro_features].pct_change()
        macro_ret.columns = [f"{c}_ret" for c in macro_ret.columns]
        macro_full = pd.concat([macro_df, macro_ret], axis=1)
        macro_full = macro_full.replace([np.inf, -np.inf], np.nan).dropna(subset=['Date']).fillna(0)

        # -------------------------------------------------------------
        # 3. ì£¼ê°€ ê¸°ë°˜ í”¼ì²˜ ìƒì„± (ê° ì¢…ëª©ë³„)
        # -------------------------------------------------------------
        target_ticker_list = ['AAPL', 'MSFT', 'NVDA']   # â† ì´ë¦„ì„ ë§ì¶¤

        if ticker_name in price_df.columns:
            price_df[f"{ticker_name}_ret1"] = price_df[ticker_name].pct_change()
            price_df[f"{ticker_name}_ma5"] = price_df[ticker_name].rolling(5).mean()
            price_df[f"{ticker_name}_ma10"] = price_df[ticker_name].rolling(10).mean()
        else:
            print(f"[WARN] '{ticker_name}' column not found in price_df.columns: {price_df.columns.tolist()}")

        price_df = price_df.fillna(method='bfill')

        # -------------------------------------------------------------
        # 4. ë‚ ì§œ ê¸°ì¤€ ë³‘í•©
        # -------------------------------------------------------------
        merged_df = pd.merge(price_df, macro_full, on='Date', how='inner').sort_values('Date').reset_index(drop=True)
        print(f"[INFO] ë³‘í•© í›„ ë°ì´í„° shape: {merged_df.shape}")

        # -------------------------------------------------------------
        # 5. Feature ì„ íƒ
        # -------------------------------------------------------------
        macro_cols = [c for c in macro_full.columns if c != 'Date']
        price_cols = [c for c in merged_df.columns if any(t in c for t in target_ticker_list) and ('_ret' in c or '_ma' in c)]
        feature_cols = macro_cols + price_cols

        X_all = merged_df[feature_cols]

        # -------------------------------------------------------------
        # 6. ì…ë ¥ ìŠ¤ì¼€ì¼ë§
        # -------------------------------------------------------------
        scaler_X = StandardScaler()
        X_scaled = scaler_X.fit_transform(X_all)
        X_scaled = pd.DataFrame(X_scaled, columns=feature_cols)

        # -------------------------------------------------------------
        # 7. íƒ€ê¹ƒ (í˜„ì¬ ticker_nameë§Œ ì˜ˆì¸¡)
        # -------------------------------------------------------------
        if ticker_name in merged_df.columns:
            merged_df[f"{ticker_name}_target"] = merged_df[ticker_name].pct_change().shift(-1)
            y_all = merged_df[[f"{ticker_name}_target"]].dropna().reset_index(drop=True)
        else:
            print(f"[WARN] '{ticker_name}' not found in merged_df.columns: {merged_df.columns.tolist()}")
            return  # í˜¹ì€ raise Exception("Ticker not found in merged_df")

        X_scaled = X_scaled.iloc[:len(y_all)]

        # -------------------------------------------------------------
        # 8. ì¶œë ¥ ìŠ¤ì¼€ì¼ë§
        # -------------------------------------------------------------
        scaler_y = MinMaxScaler(feature_range=(-1, 1))
        y_scaled = scaler_y.fit_transform(y_all)

        # -------------------------------------------------------------
        # 9. ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
        # -------------------------------------------------------------
        def create_sequences(X, y, window=40):
            Xs, ys = [], []
            for i in range(len(X) - window):
                Xs.append(X.iloc[i:(i + window)].values)
                ys.append(y[i + window])
            return np.array(Xs), np.array(ys)

        # -------------------------------------------------------------
        # 10. ì‹œí€€ìŠ¤ ë³€í™˜
        # -------------------------------------------------------------
        X_seq, y_seq = create_sequences(X_scaled, y_scaled, window=40)
        split_idx = int(len(X_seq) * 0.8)

        X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]
        y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]


        # -------------------------------------------------------------
        # 11. ìµœì¢… ë°ì´í„° ì…‹ ì €ì¥
        # -------------------------------------------------------------
        csv_path = os.path.join(save_dir, f"{ticker_name}_{self.agent_id}_dataset.csv")
        merged_df.to_csv(csv_path, index=False)

        return X_train, X_test, y_train, y_test, X_seq, y_seq


    #StockData ë° X_tensor ìƒì„± ê¸°ëŠ¥ (base_agentì˜ searcher ëŒ€ì‘)
    def macro_searcher_add_funs(self, X_seq, feature_cols, agent_id=None):
        """
        searcher()ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ì™€ ë™ì¼í•œ ê¸°ëŠ¥:
        - ìµœì‹  ìœˆë„ìš° ë°ì´í„°(X_tensor) ìƒì„±
        - feature_dict êµ¬ì„±
        - StockData ì´ˆê¸°í™” ë° ê°’ ì €ì¥
        """

        # ---------------------------------------------------------
        # ê¸°ë³¸ ì •ë³´ ì„¸íŒ…
        # ---------------------------------------------------------
        if agent_id is None:
            agent_id = self.agent_id

        ticker_name = self.ticker_name or "UNKNOWN"

        # StockData ê°ì²´ ìƒì„±
        self.stockdata = StockData()
        self.stockdata.ticker = ticker_name

        # ---------------------------------------------------------
        # ìµœì‹  ìœˆë„ìš°(X_latest â†’ X_tensor)
        # ---------------------------------------------------------
        X_latest = X_seq[-1:]              # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ (ì˜ˆì¸¡ìš©)
        X_tensor = torch.tensor(X_latest, dtype=torch.float32)

        # DataFrame ë³€í™˜
        df_latest = pd.DataFrame(X_latest[0], columns=feature_cols)

        # feature_dict êµ¬ì„±
        feature_dict = {col: df_latest[col].tolist() for col in df_latest.columns}

        # agent_id ì´ë¦„ìœ¼ë¡œ ì†ì„± ì¶”ê°€ (ì˜ˆ: self.stockdata.MacroSentiAgent)
        setattr(self.stockdata, agent_id, feature_dict)

        # ---------------------------------------------------------
        # ì¢…ê°€ ë° í†µí™” ì •ë³´ ìˆ˜ì§‘
        # ---------------------------------------------------------
        try:
            data = yf.download(ticker_name, period="1d", interval="1d")
            if not data.empty:
                self.stockdata.last_price = float(data["Close"].iloc[-1])
        except Exception as e:
            print(f"[WARN] yfinance ì˜¤ë¥˜ ë°œìƒ (ê°€ê²©): {e}")

        try:
            self.stockdata.currency = yf.Ticker(ticker_name).info.get("currency", "USD")
        except Exception as e:
            print(f"[WARN] yfinance ì˜¤ë¥˜ ë°œìƒ (í†µí™”): {e}")
            self.stockdata.currency = "USD"

        print(f"âœ… StockData ìƒì„± ì™„ë£Œ: {ticker_name} / {self.stockdata.currency}")

        return X_tensor, self.stockdata



    # ëª¨ë¸ ìƒì„±
    def make_lstm_macro_model(self, ticker_name, agent_id, X_train, y_train, scaler_X, scaler_y):
        # -------------------------------------------------------------
        # 11. ë‹¨ì¼ ì•„ì›ƒí’‹ LSTM ëª¨ë¸ ì •ì˜
        # -------------------------------------------------------------
        self.model = Sequential([
            LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
            Dropout(0.3),
            LSTM(64, return_sequences=True),
            Dropout(0.3),
            LSTM(32, return_sequences=False),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dense(1)  # ë‹¨ì¼ ì¢…ëª© ì˜ˆì¸¡
        ])

        optimizer = Adam(learning_rate=0.0005)
        self.model.compile(optimizer=optimizer, loss='mae')

        # -------------------------------------------------------------
        # 12. í•™ìŠµ
        # -------------------------------------------------------------
        history = self.model.fit(
            X_train, y_train,
            validation_split=0.1,
            epochs=60,
            batch_size=16,
            verbose=1
        )


        # ì „ì²´ ëª¨ë¸ ì €ì¥
        self.model.save(f"{model_dir}/{ticker_name}_{agent_id}.h5")
        joblib.dump(scaler_X, f"{model_dir}/scaler_X.pkl")
        joblib.dump(scaler_y, f"{model_dir}/scaler_y.pkl")
        print(f"âœ… {agent_id} model saved.\nâœ… pretraining finished.\n")


    #predict
    def macro_predictor(self, X_seq):
        print("[INFO] ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")

        # 1. ëª¨ë¸ ì˜ˆì¸¡
        pred_scaled = self.model.predict(X_seq)
        pred_inv = self.scaler_y.inverse_transform(pred_scaled)

        # 2. ì¢…ê°€ ì¶”ì¶œ
        last_prices = {}
        for t in self.tickers:
            close_candidates = [c for c in self.macro_df.columns
                                if c.startswith(t) and not c.endswith("_ma5") and "ret" not in c]
            if not close_candidates:
                raise ValueError(f"{t}ì˜ ì¢…ê°€ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            last_prices[t] = self.macro_df[close_candidates[0]].iloc[-1]

        # 3. ì˜ˆì¸¡ ì¢…ê°€ ë° ìˆ˜ìµë¥  ê³„ì‚°
        records = []
        pred_prices = {}
        for i, t in enumerate(self.tickers):
            pred_ret = float(pred_inv[0][i])
            last_price = float(last_prices[t])
            next_price = last_price * (1 + pred_ret)
            pred_prices[t] = next_price

            records.append({
                "Ticker": t,
                "Last_Close": last_price,
                "Predicted_Close": next_price,
                "Predicted_Return": pred_ret,
                "Predicted_%": pred_ret * 100
            })

            print(f"{t}: ë§ˆì§€ë§‰ ì¢…ê°€={last_price:.2f} â†’ ì˜ˆì¸¡ ì¢…ê°€={next_price:.2f} (ì˜ˆìƒ ìˆ˜ìµë¥  {pred_ret*100:.2f}%)")

        # 4. Monte Carlo Dropout ë¶ˆí™•ì‹¤ì„±
        mean_pred, std_pred, confidence, predicted_price = get_std_pred(
            self.model, X_seq, n_samples=30, scaler_y=self.scaler_y, stockdata=self.stockdata
        )

        # 5. ê²°ê³¼ ë³‘í•©
        for i, r in enumerate(records):
            r["uncertainty"] = float(std_pred[i]) if len(std_pred) > 1 else float(std_pred[-1])
            r["confidence"] = float(confidence[i]) if len(confidence) > 1 else float(confidence[-1])

        pred_df = pd.DataFrame(records).round(4)
        self.pred_df = pred_df
        self.pred_prices = pred_prices

        print("\n================= ì˜ˆì¸¡ ê²°ê³¼ (í‘œ) =================")
        print(pred_df)

        print("\n================= ì˜ˆì¸¡ ê²°ê³¼ (ê°’) =================")
        print(pred_prices)

        # ë‹¨ì¼ í‹°ì»¤ì¼ ê²½ìš° target ìš”ì•½ ì œê³µ
        self.target = Target(
            next_close=float(pred_df["Predicted_Close"].iloc[-1]),
            uncertainty=float(std_pred[-1]),
            confidence=float(pred_df["confidence"].iloc[-1])
        )


        return self.pred_prices, self.target



    def macro_reviewer_draft(self):
        temporal_summary, causal_summary, interaction_summary = self.make_macro_shap()
        # -------------------------------
        # 4ï¸âƒ£ llm ìƒì„±
        # -------------------------------
        print("\n4ï¸âƒ£ Generating explanation using LLM...")

        llm  = LLMExplainer()
        feature_summary = feature_df.tail(5).describe().round(3).to_dict()
        explanation = llm.generate_explanation(feature_summary, self.pred_prices,
                                               importance_dict,
                                               temporal_summary, causal_summary,
                                               interaction_summary)

        print(f"\n================= pred_prices:{self.pred_prices} =================")

        print("\n================= LLM Explanation =================")
        print(explanation)
        print("===================================================")

        total_json = {
            'agent_id' : self.agent_id,
            'target' : self.target,
            'reason' : explanation
        }

        stock_data = {
            'temporal_summary' : temporal_summary,
            'causal_summary' : causal_summary,
            'interaction_summary' : interaction_summary

        }

        context = json.dumps({
            "agent_id": self.agent_id,
            "predicted_next_close": round(self.target.next_close, 3),
            "uncertainty_sigma": round(self.target.uncertainty or 0.0, 4),
            "confidence_beta": round(self.target.confidence or 0.0, 4),
            "latest_data": str(stock_data)
        }, ensure_ascii=False, indent=2)

        reason = explanation

        # 4) Opinion ê¸°ë¡/ë°˜í™˜ (í•­ìƒ ìµœì‹  ê°’ append)
        self.opinions.append(Opinion(agent_id=self.agent_id, target=self.target, reason=reason))

        return total_json, self.opinions[-1]




    def make_macro_shap(self):
        # -------------------------------
        # 3ï¸âƒ£ SHAP ê³„ì‚°
        # -------------------------------
        # --- (run() ì•ˆì˜ ì•ˆì „ ì²˜ë¦¬) ---
        X_scaled = self.X_scaled.astype(np.float32)
        X_scaled = X_scaled[:, :, :300]
        feature_names = feature_names[:300]

        print("\n3ï¸âƒ£ Calculating feature importance...")
        analyzer = AttributionAnalyzer(self.model)
        importance_dict, temporal_df, causal_df, interaction_df = analyzer.run_all_shap(X_scaled, feature_names)

        temporal_summary = temporal_df.head().to_dict(orient="records") if temporal_df is not None else []
        causal_summary = causal_df.to_dict(orient="records") if causal_df is not None else []
        if isinstance(interaction_df, pd.DataFrame):
            interaction_summary = interaction_df.iloc[:5, :5].round(3).to_dict()
        else:
            interaction_summary = {}

        return temporal_summary, causal_summary, interaction_summary



    #[base_agent.py]
    def _msg(self, role: str, content: str) -> dict:
        """OpenAI ChatCompletionìš© ë©”ì‹œì§€ êµ¬ì¡° ìƒì„±"""
        if not isinstance(role, str) or not isinstance(content, str):
            raise ValueError(f"_msg() ì¸ì ì˜¤ë¥˜: role={role}, content={type(content)}")
        return {"role": role, "content": content}


    #[base_agent.py]
    def macro_reviewer_rebut(self, my_opinion: Opinion, other_opinion: Opinion, round: int) -> Rebuttal:
        """LLMì„ í†µí•´ ìƒëŒ€ ì˜ê²¬ì— ëŒ€í•œ ë°˜ë°•/ì§€ì§€ ìƒì„±"""

        # ë©”ì‹œì§€ ìƒì„± (context êµ¬ì„±ì€ ë³„ë„ í—¬í¼ì—ì„œ)
        sys_text, user_text = self._build_messages_rebuttal(
            my_opinion=my_opinion,
            target_opinion=other_opinion,
            stock_data=self.stockdata
        )

        # LLM í˜¸ì¶œ
        parsed = self._ask_with_fallback(
            self._msg("system", sys_text),
            self._msg("user", user_text),
            {
                "type": "object",
                "properties": {
                    "stance": {"type": "string", "enum": ["REBUT", "SUPPORT"]},
                    "message": {"type": "string"}
                },
                "required": ["stance", "message"],
                "additionalProperties": False
            }
        )

        # ê²°ê³¼ ì •ë¦¬ ë° ê¸°ë¡
        result = Rebuttal(
            from_agent_id=my_opinion.agent_id,
            to_agent_id=other_opinion.agent_id,
            stance=parsed.get("stance", "REBUT"),
            message=parsed.get("message", "(ë°˜ë°•/ì§€ì§€ ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨)")
        )

        # ì €ì¥
        self.rebuttals[round].append(result)

        # ë””ë²„ê¹… ë¡œê·¸
        if self.verbose:
            print(
                f"[{self.agent_id}] rebuttal ìƒì„± â†’ {result.stance} "
                f"({my_opinion.agent_id} â†’ {other_opinion.agent_id})"
            )

        return result



    #[m_agent.py]
    def _build_messages_rebuttal(self,
                                 my_opinion: Opinion,
                                 target_opinion: Opinion,
                                 stock_data: StockData) -> tuple[str, str]:

        t = stock_data.ticker or "UNKNOWN"
        ccy = (stock_data.currency or "USD").upper()
        agent_data = getattr(stock_data, self.agent_id, None)
        if not agent_data or not isinstance(agent_data, dict):
            raise ValueError(f"{self.agent_id} ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜: dictí˜• ì»¬ëŸ¼ ë°ì´í„°ê°€ í•„ìš”í•¨")

        ctx = {
            "ticker": t,
            "currency": ccy,
            "data_summary": getattr(stock_data, self.agent_id, {}).get("feature_cols", []),
            "me": {
                "agent_id": self.agent_id,
                "next_close": float(my_opinion.target.next_close),
                "reason": str(my_opinion.reason)[:2000],
                "uncertainty": float(my_opinion.target.uncertainty),
                "confidence": float(my_opinion.target.confidence),
            },
            "other": {
                "agent_id": target_opinion.agent_id,
                "next_close": float(target_opinion.target.next_close),
                "reason": str(target_opinion.reason)[:2000],
                "uncertainty": float(target_opinion.target.uncertainty),
                "confidence": float(target_opinion.target.confidence),
            }
        }
        # ê° ì»¬ëŸ¼ë³„ ìµœê·¼ ì‹œê³„ì—´ ê·¸ëŒ€ë¡œ í¬í•¨
        # (ìµœê·¼ 7~14ì¼ ì •ë„ë©´ LLMì´ ì´í•´ ê°€ëŠ¥í•œ ë²”ìœ„)
        for col, values in agent_data.items():
            if isinstance(values, (list, tuple)):
                ctx[col] = values[self.window_size:]  # ìµœê·¼ 14ì¼ì¹˜ ì „ì²´ ì‹œê³„ì—´
            else:
                ctx[col] = [values]

        system_text = REBUTTAL_PROMPTS[self.agent_id]["system"]
        user_text   = REBUTTAL_PROMPTS[self.agent_id]["user"].format(
            context=json.dumps(ctx, ensure_ascii=False)
        )
        return system_text, user_text



    #[base_agent.py] OpenAI API í˜¸ì¶œ
    def _ask_with_fallback(self, msg_sys: dict, msg_user: dict, schema_obj: dict) -> dict:
        """ëª¨ë¸ í´ë°± í¬í•¨ OpenAI Responses API í˜¸ì¶œ"""
        payload_base = {
            "input": [msg_sys, msg_user],
            "text": {
                "format": {
                    "type": "json_schema",
                    "name": "Response",
                    "strict": True,
                    "schema": schema_obj,
                }
            },
            "temperature": self.temperature,
        }
        last_err = None
        for model in self.preferred_models:
            payload = dict(payload_base, model=model)
            try:
                r = requests.post(self.OPENAI_URL, headers=self.headers, json=payload, timeout=120)
                if r.ok:
                    data = r.json()
                    # 1) output_text ìš°ì„  ì‚¬ìš©
                    if isinstance(data.get("output_text"), str) and data["output_text"].strip():
                        try:
                            return json.loads(data["output_text"])
                        except Exception:
                            return {"reason": data["output_text"]}  # JSON ì‹¤íŒ¨ ì‹œ ì›ë¬¸ í…ìŠ¤íŠ¸ ë³´ì¡´
                    # 2) output ë°°ì—´ì—ì„œ í…ìŠ¤íŠ¸ ëª¨ìœ¼ê¸°
                    out = data.get("output")
                    if isinstance(out, list) and out:
                        texts = []
                        for blk in out:
                            for c in blk.get("content", []):
                                if "text" in c:
                                    texts.append(c["text"])
                        joined = "\n".join(t for t in texts if t)
                        if joined.strip():
                            try:
                                return json.loads(joined)
                            except Exception:
                                return {"reason": joined}
                    # ë¹„ì •ìƒ ì‘ë‹µ
                    return {}
                # 400/404ëŠ” ë‹¤ìŒ ëª¨ë¸ë¡œ í´ë°±
                if r.status_code in (400, 404):
                    last_err = (r.status_code, r.text)
                    continue
                # ê¸°íƒ€ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì˜ˆì™¸
                r.raise_for_status()
            except Exception as e:
                self._p(f"â–  ëª¨ë¸ {model} ì‹¤íŒ¨: {e}")
                last_err = str(e)
                continue
        raise RuntimeError(f"ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_err}")













    def macro_reviewer_revise(
            self,
            my_opinion: Opinion,
            others: List[Opinion],
            rebuttals: List[Rebuttal],
            stock_data: StockData,
            fine_tune: bool = True,
            lr: float = 1e-4,
            epochs: int = 20,
    ):
        """
        Revision ë‹¨ê³„
        - Ïƒ ê¸°ë°˜ Î²-weighted ì‹ ë¢°ë„ ê³„ì‚°
        - Î³ ìˆ˜ë ´ìœ¨ë¡œ ì˜ˆì¸¡ê°’ ë³´ì •
        - fine-tuning (ìˆ˜ìµë¥  ë‹¨ìœ„)
        - reasoning ìƒì„±
        """
        gamma = getattr(self, "gamma", 0.3)               # ìˆ˜ë ´ìœ¨ (0~1)
        delta_limit = getattr(self, "delta_limit", 0.05)  # fine-tuning ë³´ì • í•œê³„

        try:
            # ===================================
            # â‘  Î² ê³„ì‚° (ë¶ˆí™•ì‹¤ì„± ì‘ì„ìˆ˜ë¡ ì‹ ë¢° ë†’ìŒ)
            # ===================================
            my_price = my_opinion.target.next_close
            my_sigma = abs(my_opinion.target.uncertainty or 1e-6)

            other_prices = np.array([o.target.next_close for o in others])
            other_sigmas = np.array([abs(o.target.uncertainty or 1e-6) for o in others])

            all_sigmas = np.concatenate([[my_sigma], other_sigmas])
            all_prices = np.concatenate([[my_price], other_prices])

            inv_sigmas = 1 / (all_sigmas + 1e-6)
            betas = inv_sigmas / inv_sigmas.sum()

            # ===================================
            # â‘¡ ë…¼ë¬¸ì‹ ìˆ˜ë ´ ì—…ë°ì´íŠ¸
            #     y_i_rev = y_i + Î³ Î£ Î²_j (y_j - y_i)
            # ===================================
            delta = np.sum(betas[1:] * (other_prices - my_price))
            revised_price = my_price + gamma * delta

        except Exception as e:
            print(f"[{self.agent_id}] revised_target ê³„ì‚° ì‹¤íŒ¨: {e}")
            revised_price = my_opinion.target.next_close
            current_price = getattr(self.stockdata, "last_price", 100.0)
            price_uplimit = current_price * (1 + delta_limit)
            price_downlimit = current_price * (1 - delta_limit)
            revised_price = min(max(revised_price, price_downlimit), price_uplimit)

        # ===================================
        # â‘¢ Fine-tuning (return ë‹¨ìœ„)
        # ===================================
        loss_value = None
        if fine_tune and hasattr(self, "model"):
            try:
                current_price = getattr(self.stockdata, "last_price", 100.0)
                revised_return = (revised_price / current_price) - 1  # ğŸ”¹ìˆ˜ìµë¥  ë³€í™˜

                X_input = self.searcher(self.ticker)
                device = next(self.model.parameters()).device
                X_tensor = torch.tensor(X_input, dtype=torch.float32).to(device)
                y_tensor = torch.tensor([[revised_return]], dtype=torch.float32).to(device)

                self.model.train()
                optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
                criterion = torch.nn.MSELoss()

                for _ in range(epochs):
                    optimizer.zero_grad()
                    pred = self.model(X_tensor)
                    delta_loss = pred - y_tensor
                    loss = criterion(pred - delta_loss, y_tensor)
                    loss.backward()
                    optimizer.step()

                loss_value = float(loss.item())
                print(f"[{self.agent_id}] fine-tuning ì™„ë£Œ: loss={loss_value:.6f}")

            except Exception as e:
                print(f"[{self.agent_id}] fine-tuning ì‹¤íŒ¨: {e}")

        # ===================================
        # â‘£ fine-tuning ì´í›„ ìƒˆ ì˜ˆì¸¡ ìƒì„±
        # ===================================
        try:
            X_latest = self.searcher(self.ticker)
            new_target = self.predict(X_latest)
        except Exception as e:
            print(f"[{self.agent_id}] predict ì‹¤íŒ¨: {e}")
            new_target = my_opinion.target

        # ===================================
        # â‘¤ reasoning ìƒì„±
        # ===================================
        try:
            sys_text, user_text = self._build_messages_revision(
                my_opinion=my_opinion,
                others=others,
                rebuttals=rebuttals,
                stock_data=stock_data,
            )
        except Exception as e:
            print(f"[{self.agent_id}] _build_messages_revision ì‹¤íŒ¨: {e}")
            sys_text, user_text = (
                "ë„ˆëŠ” ê¸ˆìœµ ë¶„ì„ê°€ë‹¤. ê°„ë‹¨íˆ reasonë§Œ ìƒì„±í•˜ë¼.",
                json.dumps({"reason": "ê¸°ë³¸ ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨"}),
            )

        parsed = self._ask_with_fallback(
            self._msg("system", sys_text),
            self._msg("user", user_text),
            {
                "type": "object",
                "properties": {"reason": {"type": "string"}},
                "required": ["reason"],
                "additionalProperties": False,
            },
        )

        revised_reason = parsed.get("reason", "(ìˆ˜ì • ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨)")
        revised_opinion = Opinion(
            agent_id=self.agent_id,
            target=new_target,
            reason=revised_reason,
        )

        self.opinions.append(revised_opinion)
        print(f"[{self.agent_id}] revise ì™„ë£Œ â†’ new_close={new_target.next_close:.2f}, loss={loss_value}")
        return self.opinions[-1]