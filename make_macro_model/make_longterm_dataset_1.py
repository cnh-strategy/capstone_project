import json
import os
from collections import defaultdict
from typing import Optional, List, Dict

import requests
import torch
import yfinance as yf
from typing import Dict, List, Optional, Literal, Tuple, Any
import warnings
from dataclasses import dataclass, field
import joblib
import pandas as pd
import numpy as np
from keras.src.saving import load_model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler, MinMaxScaler

from agents.dump import CAPSTONE_OPENAI_API
from agents.macro_shap_llm import LLMExplainer, AttributionAnalyzer
from agents.macro_sub import get_std_pred
from debate_ver4.prompts import REBUTTAL_PROMPTS

warnings.filterwarnings("ignore", category=FutureWarning)

# ============================================================
# ê³µí†µ ì„¤ì •
# ============================================================
PROJECT_ROOT = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..")
)
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "processed")
os.makedirs(OUTPUT_DIR, exist_ok=True)



dir_info = {
    "data_dir": os.path.join(PROJECT_ROOT, "data", "processed"),
    "model_dir": os.path.join(PROJECT_ROOT, "models"),
}
save_dir = dir_info["data_dir"]
model_dir: str = dir_info["model_dir"]
data_dir: str = dir_info["data_dir"]


@dataclass
class Target:
    """ì˜ˆì¸¡ ëª©í‘œê°’ + ë¶ˆí™•ì‹¤ì„± ì •ë³´ í¬í•¨
    - next_close: ë‹¤ìŒ ê±°ë˜ì¼ ì¢…ê°€ ì˜ˆì¸¡ì¹˜
    - uncertainty: Monte Carlo Dropout ê¸°ë°˜ ì˜ˆì¸¡ í‘œì¤€í¸ì°¨(Ïƒ)
    - confidence: ëª¨ë¸ ì‹ ë¢°ë„ Î² (ì •ê·œí™”ëœ ì‹ ë¢°ë„; ì„ íƒì )
    """
    next_close: float
    uncertainty: Optional[float] = None
    confidence: Optional[float] = None

@dataclass
class Opinion:
    agent_id: str
    target: Target
    reason: str

@dataclass
class Rebuttal:
    from_agent_id: str
    to_agent_id: str
    stance: Literal["REBUT", "SUPPORT"]
    message: str

@dataclass
class RoundLog:
    round_no: int
    opinions: List[Opinion]
    rebuttals: List[Rebuttal]
    summary: Dict[str, Target]

@dataclass
class StockData:
    """ì—ì´ì „íŠ¸ ì…ë ¥ ì›ì²œ ë°ì´í„°(í•„ìš” ì‹œ ììœ  í™•ì¥)
    - sentimental: ì‹¬ë¦¬/ì»¤ë®¤ë‹ˆí‹°/ë‰´ìŠ¤ ìŠ¤ëƒ…ìƒ·
    - fundamental: ì¬ë¬´/ë°¸ë¥˜ì—ì´ì…˜ ìš”ì•½
    - technical  : ê°€ê²©/ì§€í‘œ ìŠ¤ëƒ…ìƒ·
    - last_price : ìµœì‹  ì¢…ê°€
    - currency   : í†µí™”ì½”ë“œ
    """
    SentimentalAgent: Optional[Dict[str, Any]] = field(default_factory=dict)
    FundamentalAgent: Optional[Dict[str, Any]] = field(default_factory=dict)
    TechnicalAgent: Optional[Dict[str, Any]] = field(default_factory=dict)
    last_price: Optional[float] = None
    currency: Optional[str] = None


# ============================================================
# MacroSentimentAgent â€” ì‹œì¥Â·ê±°ì‹œê²½ì œ ì‹œê³„ì—´ ê¸°ë°˜
# ============================================================
class MacroSentimentAgentDataset:
    OPENAI_URL = "https://api.openai.com/v1/responses"

    def __init__(self,
                 agent_id="MacroSentiAgent",
                 preferred_models: Optional[List[str]] = None,
                 option_model: Optional[str] = None,
                 verbose: bool = False,
                 temperature: float = 0.2,
                 ticker = None,
                 **kwargs
                 ):
        self.merged_df = None
        self.price_df = None
        self.y_test = None
        self.X_test = None
        self.y_seq = None
        self.X_seq = None
        self.y_train = None
        self.X_train = None
        self.y_scaled = None
        self.y_all = None
        self.X_all = None
        self.feature_cols = None
        self.price_cols = None
        self.macro_cols = None
        self.macro_full = None
        self.temperature = None
        self.target = None
        self.opinions = None
        self.macro_tickers = {
            "SPY": "SPY", "QQQ": "QQQ", "^GSPC": "^GSPC", "^DJI": "^DJI", "^IXIC": "^IXIC",
            "^TNX": "^TNX", "^IRX": "^IRX", "^FVX": "^FVX",
            "^VIX": "^VIX",
            "DX-Y.NYB": "DX-Y.NYB",
            "EURUSD=X": "EURUSD=X", "USDJPY=X": "USDJPY=X",
            "GC=F": "GC=F", "CL=F": "CL=F", "HG=F": "HG=F",
          #  "BTC-USD": "BTC-USD", "ETH-USD": "ETH-USD"
        }
        self.data = None
        self.agent_id = 'MacroSentiAgent'
        self.ticker_name = ticker

        self.model_path = f"{model_dir}/{ticker}_{agent_id}.h5"
        self.scaler_X_path = f"{model_dir}/scaler_X.pkl"
        self.scaler_y_path = f"{model_dir}/scaler_y.pkl"

        self.tickers = [self.ticker_name] or ["AAPL", "MSFT", "NVDA"]
        # self.target_tickers = target_tickers or ["AAPL", "MSFT", "NVDA"]

        self.model = None
        self.scaler_X = None
        self.scaler_y = None
        self.macro_df = None
        self.pred_df = None
        self.X_scaled = None

        self.start_date = "2020-01-01"
        self.end_date = '2024-12-31'

        self.window_size = 40
        # ëª¨ë¸ í´ë°± ìš°ì„ ìˆœìœ„
        self.preferred_models = preferred_models or ["gpt-5-mini", "gpt-4.1-mini"]
        if option_model:
            self.preferred_models = [option_model] + [
                m for m in self.preferred_models if m != option_model
            ]

        # ê³µí†µ í—¤ë”
        self.api_key = CAPSTONE_OPENAI_API
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        self.temperature = temperature # Temperature ì„¤ì •
        self.verbose = verbose            # ë””ë²„ê¹… ëª¨ë“œ
        self.rebuttals: Dict[int, List[Rebuttal]] = defaultdict(list)

    def fetch_data(self):
        """ë§¤í¬ë¡œ ë° ê°œë³„ ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘"""
        print(f"[INFO] Collecting macro features ({len(self.macro_tickers)} tickers)...")

        # --------------------------------------------
        # â‘  ë§¤í¬ë¡œ ìì‚° ë°ì´í„° ìˆ˜ì§‘
        # --------------------------------------------
        df_macro = yf.download(
            tickers=list(self.macro_tickers.values()),
            start=self.start_date,
            end=self.end_date,
            interval="1d",
            group_by="ticker",
            auto_adjust=False
        )


        # MultiIndex â†’ ë‹¨ì¼ ì»¬ëŸ¼ëª… í†µì¼
        if isinstance(df_macro.columns, pd.MultiIndex):
            # yfinance ì»¬ëŸ¼ êµ¬ì¡°ê°€ (Ticker, Price) ë˜ëŠ” (Price, Ticker)ì¸ ê²½ìš° ìë™ ì²˜ë¦¬
            if df_macro.columns.names == ["Ticker", "Price"]:
                df_macro.columns = [f"{t}_{col}" for t, col in df_macro.columns]
            else:
                df_macro.columns = [f"{col}_{t}" for col, t in df_macro.columns]

            # âœ… flatten ì´í›„ ìˆœì„œ ë³´ì • (Adj Close_SPY â†’ SPY_Adj Close)
            df_macro.columns = [
                "_".join(reversed(c.split("_"))) if c.split("_")[0] in ["Adj", "Close", "Open", "High", "Low", "Volume"] else c
                for c in df_macro.columns
            ]
        else:
            df_macro.columns = [f"macro_{col}" for col in df_macro.columns]

        df_macro.reset_index(inplace=True)
        df_macro["Date"] = pd.to_datetime(df_macro["Date"])
        print(f"[INFO] Macro columns after flatten: {df_macro.columns[:10].tolist()}")
        print(f"[MacroSentimentAgent] Macro data loaded: {df_macro.shape}")

        # --------------------------------------------
        # â‘¡ ê°œë³„ ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘
        # --------------------------------------------
        stock_dfs = []
        tickers = [self.ticker_name] if isinstance(self.ticker_name, str) else (self.ticker_name or ["NVDA"])
        for t in tickers:
            print(f"[INFO] Downloading {t} ...")
            try:
                df_t = yf.download(t, start=self.start_date, end=self.end_date, interval="1d", group_by="ticker")

                # âœ… MultiIndex â†’ ë‹¨ì¼ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                if isinstance(df_t.columns, pd.MultiIndex):
                    df_t.columns = [f"{t}_{col}" for t, col in df_t.columns]
                    print(f"[INFO] {t} columns after flatten:", df_t.columns.tolist())

                df_t = df_t.rename(columns={
                    f"{t}_Open": f"{t}_Open",
                    f"{t}_High": f"{t}_High",
                    f"{t}_Low": f"{t}_Low",
                    f"{t}_Close": f"{t}_Close",
                    f"{t}_Volume": f"{t}_Volume"
                })

                df_t.reset_index(inplace=True)

                # âœ… íŒŒìƒ ì»¬ëŸ¼ ìƒì„±
                df_t[f"{t}_ret1"] = df_t[f"{t}_Close"].pct_change()
                df_t[f"{t}_ma5"] = df_t[f"{t}_Close"].rolling(5).mean()
                df_t[f"{t}_ma10"] = df_t[f"{t}_Close"].rolling(10).mean()

                stock_dfs.append(df_t)
            except Exception as e:
                print(f"[WARN] {t} download failed: {e}")

        # --------------------------------------------
        # â‘¢ ë³‘í•© (outer joinìœ¼ë¡œ ê³µë°±ì¼ í¬í•¨)
        # --------------------------------------------
        merged_df = df_macro
        for df_t in stock_dfs:
            # âœ… í˜¹ì‹œë¼ë„ MultiIndexê°€ ë‚¨ì•„ìˆìœ¼ë©´ í‰íƒ„í™”
            if isinstance(df_t.columns, pd.MultiIndex):
                df_t.columns = ["_".join([str(c) for c in col if c]) for col in df_t.columns]
            merged_df = pd.merge(merged_df, df_t, on="Date", how="outer")


        # --------------------------------------------
        # â‘£ ê²°ì¸¡ì¹˜ ë° ì •ë¦¬
        # --------------------------------------------
        merged_df = merged_df.sort_values("Date")
        merged_df = merged_df.fillna(method="ffill").fillna(method="bfill")
        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]  # ì¤‘ë³µ ì œê±°
        merged_df.reset_index(drop=True, inplace=True)

        self.data = merged_df
        print(f"[MacroSentimentAgent] Combined data shape: {merged_df.shape}")
        return merged_df


    def add_features(self):
        """ë§¤í¬ë¡œ ë°ì´í„° ê¸°ë°˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
        try:
            if self.data is None:
                raise ValueError("[ERROR] add_features() í˜¸ì¶œ ì „ fetch_data()ë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")

            df = self.data.copy()
            df.index.name = "Date"

            # âœ… ì´ë¯¸ Date ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ reset_index()ë¥¼ ìƒëµ
            if "Date" not in df.columns:
                df.reset_index(inplace=True)
            else:
                df = df.copy()  # ê·¸ëŒ€ë¡œ ìœ ì§€

            # --------------------------------------------
            # (1) ë§¤í¬ë¡œ í”¼ì²˜ ìƒì„±
            # --------------------------------------------
            for ticker in self.macro_tickers.values():
                col_name = f"{ticker}_Close"
                if col_name in df.columns:
                    df[f"{ticker}_ret_1d"] = df[col_name].pct_change()

            # ê¸ˆë¦¬ ìŠ¤í”„ë ˆë“œ
            if "^TNX_Close" in df.columns and "^IRX_Close" in df.columns:
                df["Yield_spread"] = df["^TNX_Close"] - df["^IRX_Close"]

            # ìœ„í—˜ì‹¬ë¦¬ ì§€í‘œ
            if (
                    "SPY_ret_1d" in df.columns
                    and "DX-Y.NYB_ret_1d" in df.columns
                    and "^VIX_ret_1d" in df.columns
            ):
                df["Risk_Sentiment"] = (
                        df["SPY_ret_1d"] - df["DX-Y.NYB_ret_1d"] - df["^VIX_ret_1d"]
                )

            # --------------------------------------------
            # (2) ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            # --------------------------------------------
            df = df.fillna(method="ffill").fillna(method="bfill")

            # --------------------------------------------
            # (3) í”¼ì²˜ ìˆœì„œ ì •ë ¬ (ìŠ¤ì¼€ì¼ëŸ¬ ê¸°ì¤€ ì •ë ¬ ì œê±°)
            # --------------------------------------------
            print("[INFO] feature order sync skipped (manual override)")
            df = df.reindex(sorted(df.columns), axis=1)

            # --------------------------------------------
            # (4) reset_index() ì¤‘ë³µ ì œê±° â€” ê¸°ì¡´ ì½”ë“œ ìˆ˜ì •
            # --------------------------------------------
            # ê¸°ì¡´ ì½”ë“œì—ì„œëŠ” ì—¬ê¸°ì„œ ë˜ reset_index()ë¥¼ ì‹¤í–‰í–ˆì§€ë§Œ,
            # ì´ë¯¸ Date ì»¬ëŸ¼ì´ ìˆì„ ê²½ìš° ì¤‘ë³µ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë¯€ë¡œ ì¡°ê±´ë¶€ë¡œ ìˆ˜í–‰
            if "Date" not in df.columns:
                df.reset_index(inplace=True)

            # --------------------------------------------
            # (5) ë§ˆë¬´ë¦¬
            # --------------------------------------------
            self.data = df
            self.macro_df = df.copy()   # âœ… macro_predictorì—ì„œë„ ë™ì¼ ë°ì´í„° ì°¸ì¡°
            print(f"[MacroSentimentAgent] Feature engineering complete. Final shape: {df.shape}")
            return df

        except Exception as e:
            print(f"[add_features]Error: {e}")


    def save_csv(self):
        path = os.path.join(OUTPUT_DIR, "macro_sentiment.csv")
        os.makedirs(os.path.dirname(path), exist_ok=True)
        self.data.to_csv(path, index=True)
        print(f"[MacroSentimentAgent] Saved {path}")


    # -------------------------------------------------------------
    # 1. í‹°ì»¤ë³„ ì¢…ê°€ ì €ì¥
    # -------------------------------------------------------------
    def close_price_fetch(self, ticker_name):
        """í‹°ì»¤ë³„ ì¼ë³„ ì¢…ê°€ ì €ì¥ (NVDA_Close ì¤‘ë³µ, MultiIndex ë¬¸ì œ ì™„ë²½ ë°©ì§€)"""
        print(f"[INFO] Fetching close prices for {ticker_name} ...")

        df_prices = yf.download(
            tickers=ticker_name,
            start="2020-01-01",
            end="2025-01-03",
            group_by="ticker",
            auto_adjust=False
        )

        # (1) MultiIndex êµ¬ì¡° ì²˜ë¦¬
        if isinstance(df_prices.columns, pd.MultiIndex):
            df_prices.columns = ["_".join(col).strip() for col in df_prices.columns.values]
            close_cols = [c for c in df_prices.columns if "Close" in c]
            if close_cols:
                df_prices = df_prices[close_cols]
                df_prices.columns = [f"{ticker_name}_Close" for _ in close_cols]
            else:
                raise KeyError(f"[ERROR] MultiIndex êµ¬ì¡°ì—ì„œ Close ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {df_prices.columns.tolist()}")

        # (2) ë‹¨ì¼ ì»¬ëŸ¼ êµ¬ì¡°
        elif isinstance(df_prices, pd.DataFrame) and "Close" in df_prices.columns:
            df_prices = df_prices[["Close"]].rename(columns={"Close": f"{ticker_name}_Close"})

        # (3) Series ë°˜í™˜
        elif isinstance(df_prices, pd.Series):
            df_prices = df_prices.to_frame(name=f"{ticker_name}_Close")

        else:
            raise KeyError(f"[ERROR] Close ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {df_prices.columns.tolist()}")

        # ë‚ ì§œ ì»¬ëŸ¼ í™•ë³´
        if "Date" not in df_prices.columns:
            df_prices.reset_index(inplace=True)
            if "index" in df_prices.columns:
                df_prices.rename(columns={"index": "Date"}, inplace=True)

        # ì¤‘ë³µ ì»¬ëŸ¼ ì •ë¦¬
        df_prices = df_prices.loc[:, ~df_prices.columns.duplicated()]
        if "Date.1" in df_prices.columns:
            df_prices = df_prices.drop(columns=["Date.1"])

        # ì €ì¥
        path = os.path.join(OUTPUT_DIR, f"daily_closePrice_{ticker_name}.csv")
        if os.path.exists(path):
            os.remove(path)

        os.makedirs(os.path.dirname(path), exist_ok=True)
        df_prices.to_csv(path, index=False)

        print(f"âœ… ì €ì¥ ì™„ë£Œ: {df_prices.shape} rows >> {path}")
        return df_prices

    # -------------------------------------------------------------
    # 2. ë§¤í¬ë¡œ + ì£¼ê°€ ë³‘í•© ë°ì´í„°ì…‹ ìƒì„±
    # -------------------------------------------------------------
    def make_dataset_seq(self, ticker_name):
        self.ticker_name = ticker_name
        print(f"[INFO] make_dataset_seq() ì‹œì‘ â€” {ticker_name}")

        macro_path = os.path.join(OUTPUT_DIR, "macro_sentiment.csv")
        price_path = os.path.join(OUTPUT_DIR, f"daily_closePrice_{ticker_name}.csv")

        # íŒŒì¼ ì¡´ì¬ ê²€ì¦
        if not os.path.exists(macro_path):
            raise FileNotFoundError(f"[ERROR] macro_sentiment.csvê°€ ì—†ìŠµë‹ˆë‹¤ â†’ {macro_path}")
        if not os.path.exists(price_path):
            raise FileNotFoundError(f"[ERROR] {price_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. close_price_fetch('{ticker_name}') ì‹¤í–‰ í•„ìš”.")

        # íŒŒì¼ ë¡œë“œ
        self.macro_df = pd.read_csv(macro_path)
        self.price_df = pd.read_csv(price_path)

        # ì¤‘ë³µ ë° ê²°ì¸¡ ì²˜ë¦¬
        self.price_df = self.price_df.loc[:, ~self.price_df.columns.duplicated()]
        for col in self.price_df.columns:
            if col.endswith(".1"):
                base_col = col.split(".")[0]
                if base_col not in self.price_df.columns:
                    self.price_df.rename(columns={col: base_col}, inplace=True)

        # ë‚ ì§œ ì •ë¦¬
        self.macro_df["Date"] = pd.to_datetime(self.macro_df["Date"])
        self.price_df["Date"] = pd.to_datetime(self.price_df["Date"])

        # -------------------------------------------------------------
        # ë§¤í¬ë¡œ í”¼ì²˜ í™•ì¥
        # -------------------------------------------------------------
        numeric_df = self.macro_df.select_dtypes(include=[np.number])
        macro_features = [c for c in numeric_df.columns if c != "Date"]
        macro_ret = numeric_df[macro_features].pct_change()
        macro_ret.columns = [f"{c}_ret" for c in macro_ret.columns]

        self.macro_full = pd.concat([self.macro_df, macro_ret], axis=1)
        self.macro_full = self.macro_full.replace([np.inf, -np.inf], np.nan).dropna(subset=["Date"]).fillna(0)

        # -------------------------------------------------------------
        # ì£¼ê°€ ê¸°ë°˜ í”¼ì²˜ ìƒì„±
        # -------------------------------------------------------------
        close_col = [c for c in self.price_df.columns if "Close" in c and ticker_name in c]
        if not close_col:
            close_col = [c for c in self.price_df.columns if "Close" in c]
        if not close_col:
            raise KeyError(f"[ERROR] {ticker_name} ì¢…ê°€ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.price_df.columns.tolist()}")
        close_col = close_col[0]

        self.price_df[f"{ticker_name}_ret1"] = self.price_df[close_col].pct_change()
        self.price_df[f"{ticker_name}_ma5"] = self.price_df[close_col].rolling(5).mean()
        self.price_df[f"{ticker_name}_ma10"] = self.price_df[close_col].rolling(10).mean()
        self.price_df = self.price_df.fillna(method="bfill")

        # -------------------------------------------------------------
        # ë³‘í•© ìˆ˜í–‰
        # -------------------------------------------------------------
        print("[INFO] ë³‘í•© ì „ Date ì¼ì¹˜í™” ì¤‘...")
        self.macro_df["Date"] = pd.to_datetime(self.macro_df["Date"])
        self.price_df["Date"] = pd.to_datetime(self.price_df["Date"])

        merged_df = pd.merge(self.macro_df, self.price_df, on="Date", how="inner")
        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]  # ì¤‘ë³µ ì œê±°
        if "Date.1" in merged_df.columns:
            merged_df.drop(columns=["Date.1"], inplace=True)
        if merged_df.empty:
            raise ValueError("[ERROR] macro_dfì™€ price_df ê°„ ê³µí†µ Dateê°€ ì—†ìŠµë‹ˆë‹¤.")

        # âœ… NVDA_Close_x/y ë¬¸ì œ í•´ê²°
        rename_map = {}
        for c in merged_df.columns:
            if c.endswith("_Close_x") or c.endswith("_Close_y"):
                rename_map[c] = c.replace("_Close_x", "_Close").replace("_Close_y", "_Close")
        merged_df.rename(columns=rename_map, inplace=True)

        self.merged_df = merged_df

        print(f"[INFO] ë³‘í•© í›„ ë°ì´í„° shape: {merged_df.shape}")
        print(f"[DEBUG] NVDA ê´€ë ¨ ì»¬ëŸ¼: {[c for c in merged_df.columns if 'NVDA' in c]}")

        # -------------------------------------------------------------
        # Feature êµ¬ì„±
        # -------------------------------------------------------------
        target_ticker_list = ["AAPL", "MSFT", "NVDA"]
        self.macro_cols = [c for c in self.macro_full.columns if c != "Date"]
        self.price_cols = [c for c in merged_df.columns if any(t in c for t in target_ticker_list) and ("_ret" in c or "_ma" in c)]
        self.feature_cols = self.macro_cols + self.price_cols

        self.X_all = merged_df[self.feature_cols]

        # -------------------------------------------------------------
        # ì…ë ¥ ìŠ¤ì¼€ì¼ë§
        # -------------------------------------------------------------
        self.scaler_X = StandardScaler()
        self.X_scaled = pd.DataFrame(self.scaler_X.fit_transform(self.X_all), columns=self.feature_cols)

        # -------------------------------------------------------------
        # íƒ€ê¹ƒ
        # -------------------------------------------------------------
        close_target = [c for c in merged_df.columns if f"{ticker_name}_Close" in c]
        if not close_target:
            raise KeyError(f"[ERROR] íƒ€ê¹ƒ ì¢…ê°€ ì»¬ëŸ¼({ticker_name}_Close)ì´ ì—†ìŠµë‹ˆë‹¤: {merged_df.columns.tolist()}")
        close_target = close_target[0]

        merged_df[f"{ticker_name}_target"] = merged_df[close_target].pct_change().shift(-1)
        self.y_all = merged_df[[f"{ticker_name}_target"]].dropna().reset_index(drop=True)
        self.X_scaled = self.X_scaled.iloc[: len(self.y_all)]

        # -------------------------------------------------------------
        # ì¶œë ¥ ìŠ¤ì¼€ì¼ë§
        # -------------------------------------------------------------
        self.scaler_y = MinMaxScaler(feature_range=(-1, 1))
        self.y_scaled = self.scaler_y.fit_transform(self.y_all)

        # -------------------------------------------------------------
        # ì‹œí€€ìŠ¤ ìƒì„±
        # -------------------------------------------------------------
        def create_sequences(X, y, window=40):
            Xs, ys = [], []
            for i in range(len(X) - window):
                Xs.append(X.iloc[i:(i + window)].values)
                ys.append(y[i + window])
            return np.array(Xs), np.array(ys)

        self.X_seq, self.y_seq = create_sequences(self.X_scaled, self.y_scaled, window=40)
        split_idx = int(len(self.X_seq) * 0.8)
        self.X_train, self.X_test = self.X_seq[:split_idx], self.X_seq[split_idx:]
        self.y_train, self.y_test = self.y_seq[:split_idx], self.y_seq[split_idx:]

        # -------------------------------------------------------------
        # ë°ì´í„°ì…‹ ì €ì¥
        # -------------------------------------------------------------
        csv_path = os.path.join(OUTPUT_DIR, f"{ticker_name}_{self.__class__.__name__}_dataset.csv")
        os.makedirs(os.path.dirname(csv_path), exist_ok=True)
        merged_df.to_csv(csv_path, index=False)
        print(f"[OK] ë³‘í•© ì™„ë£Œ ë° ì €ì¥ â†’ {csv_path}")

        return self.X_train, self.X_test, self.y_train, self.y_test, self.X_seq, self.y_seq, self.feature_cols



    def macro_searcher_add_funs(self, X_seq, feature_cols, agent_id=None):
        """
        searcher()ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ì™€ ë™ì¼í•œ ê¸°ëŠ¥:
        - ìµœì‹  ìœˆë„ìš° ë°ì´í„°(X_tensor) ìƒì„±
        - feature_dict êµ¬ì„±
        - StockData ì´ˆê¸°í™” ë° ê°’ ì €ì¥
        """

        # ---------------------------------------------------------
        # ê¸°ë³¸ ì •ë³´ ì„¸íŒ…
        # ---------------------------------------------------------
        if agent_id is None:
            agent_id = self.agent_id

        ticker_name = self.ticker_name or "UNKNOWN"

        # StockData ê°ì²´ ìƒì„±
        self.stockdata = StockData()
        self.stockdata.ticker = ticker_name

        # ---------------------------------------------------------
        # ìµœì‹  ìœˆë„ìš°(X_latest â†’ X_tensor)
        # ---------------------------------------------------------
        X_latest = X_seq[-1:]              # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ (ì˜ˆì¸¡ìš©)
        X_tensor = torch.tensor(X_latest, dtype=torch.float32)

        # DataFrame ë³€í™˜
        df_latest = pd.DataFrame(X_latest[0], columns=feature_cols)

        # feature_dict êµ¬ì„±
        feature_dict = {col: df_latest[col].tolist() for col in df_latest.columns}

        # agent_id ì´ë¦„ìœ¼ë¡œ ì†ì„± ì¶”ê°€ (ì˜ˆ: self.stockdata.MacroSentiAgent)
        setattr(self.stockdata, agent_id, feature_dict)

        # ---------------------------------------------------------
        # ì¢…ê°€ ë° í†µí™” ì •ë³´ ìˆ˜ì§‘
        # ---------------------------------------------------------
        try:
            data = yf.download(ticker_name, period="1d", interval="1d")
            if not data.empty:
                self.stockdata.last_price = float(data["Close"].iloc[-1])
        except Exception as e:
            print(f"[WARN] yfinance ì˜¤ë¥˜ ë°œìƒ (ê°€ê²©): {e}")

        try:
            self.stockdata.currency = yf.Ticker(ticker_name).info.get("currency", "USD")
        except Exception as e:
            print(f"[WARN] yfinance ì˜¤ë¥˜ ë°œìƒ (í†µí™”): {e}")
            self.stockdata.currency = "USD"

        print(f"âœ… StockData ìƒì„± ì™„ë£Œ: {ticker_name} / {self.stockdata.currency}")

        return X_tensor, self.stockdata



    # ëª¨ë¸ ìƒì„±
    # -------------------------------------------------------------
    # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    # -------------------------------------------------------------
    def make_lstm_macro_model(self, ticker_name, X_train, y_train):
        print(f'[make_lstm_macro_model] ì‹œì‘: {ticker_name}')

        # -------------------------------------------------------------
        # 1. ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„± ë° ìŠ¤ì¼€ì¼ë§
        # -------------------------------------------------------------
        print("[INFO] ìŠ¤ì¼€ì¼ë§ ì‹œì‘")
        self.scaler_X = StandardScaler().fit(X_train.reshape(-1, X_train.shape[-1]))
        self.scaler_y = StandardScaler().fit(y_train.reshape(-1, 1))

        X_scaled = self.scaler_X.transform(X_train.reshape(-1, X_train.shape[-1]))
        X_scaled = X_scaled.reshape(X_train.shape)
        y_scaled = self.scaler_y.transform(y_train.reshape(-1, 1))

        print(f"[INFO] X_scaled shape: {X_scaled.shape}, y_scaled shape: {y_scaled.shape}")

        # -------------------------------------------------------------
        # 2. LSTM ëª¨ë¸ ì •ì˜
        # -------------------------------------------------------------
        self.model = Sequential([
            LSTM(128, return_sequences=True, input_shape=(X_scaled.shape[1], X_scaled.shape[2])),
            Dropout(0.3),
            LSTM(64, return_sequences=True),
            Dropout(0.3),
            LSTM(32, return_sequences=False),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dense(1)
        ])

        optimizer = Adam(learning_rate=0.0005)
        self.model.compile(optimizer=optimizer, loss='mae')

        # -------------------------------------------------------------
        # 3. í•™ìŠµ
        # -------------------------------------------------------------
        print("[INFO] ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        history = self.model.fit(
            X_scaled, y_scaled,
            validation_split=0.1,
            epochs=60,
            batch_size=16,
            verbose=1
        )

        # -------------------------------------------------------------
        # 4. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        # -------------------------------------------------------------
        model_path = os.path.join(model_dir, f"{ticker_name}_{self.agent_id}.h5")
        scaler_X_path = os.path.join(model_dir, "scaler_X.pkl")
        scaler_y_path = os.path.join(model_dir, "scaler_y.pkl")

        os.makedirs(model_dir, exist_ok=True)
        self.model.save(model_path)
        joblib.dump(self.scaler_X, scaler_X_path)
        joblib.dump(self.scaler_y, scaler_y_path)

        print(f"âœ… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ:")
        print(f"   - Model: {model_path}")
        print(f"   - Scaler X: {scaler_X_path}")
        print(f"   - Scaler y: {scaler_y_path}")
        print(f"âœ… pretraining finished.\n")

        return self.model




    # -------------------------------------------------------------
    # 1. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    # -------------------------------------------------------------
    def load_assets(self):
        print("[INFO] ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì¤‘...")
        self.model = load_model(self.model_path, compile=False)
        self.scaler_X = joblib.load(self.scaler_X_path)
        self.scaler_y = joblib.load(self.scaler_y_path)
        print("[OK] ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")


    #predict
    def macro_predictor(self, X_seq):
        print("[INFO] ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")

        # 1. ëª¨ë¸ ì˜ˆì¸¡
        self.load_assets()
        pred_scaled = self.model.predict(X_seq)
        pred_inv = self.scaler_y.inverse_transform(pred_scaled)

        # 2. ì¢…ê°€ ì¶”ì¶œ
        last_prices = {}
        for t in self.tickers:
            close_candidates = [
                c for c in self.macro_df.columns
                if "Close" in c and (c.startswith(t) or c.endswith(t))
            ]
            if not close_candidates:
                raise ValueError(f"[ERROR] {t}ì˜ ì¢…ê°€ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                                 f"í˜„ì¬ ì»¬ëŸ¼ë“¤: {self.macro_df.columns.tolist()}"
                                 )
            last_prices[t] = self.macro_df[close_candidates[0]].iloc[-1]

        # 3. ì˜ˆì¸¡ ì¢…ê°€ ë° ìˆ˜ìµë¥  ê³„ì‚°
        records = []
        pred_prices = {}
        for i, t in enumerate(self.tickers):
            pred_ret = float(pred_inv[0][i])
            last_price = float(last_prices[t])
            next_price = last_price * (1 + pred_ret)
            pred_prices[t] = next_price

            records.append({
                "Ticker": t,
                "Last_Close": last_price,
                "Predicted_Close": next_price,
                "Predicted_Return": pred_ret,
                "Predicted_%": pred_ret * 100
            })

            print(f"{t}: ë§ˆì§€ë§‰ ì¢…ê°€={last_price:.2f} â†’ ì˜ˆì¸¡ ì¢…ê°€={next_price:.2f} (ì˜ˆìƒ ìˆ˜ìµë¥  {pred_ret*100:.2f}%)")

        # 4. Monte Carlo Dropout ë¶ˆí™•ì‹¤ì„±
        mean_pred, std_pred, confidence, predicted_price = get_std_pred(
            self.model, X_seq, n_samples=30, scaler_y=self.scaler_y, stockdata=self.stockdata
        )

        # 5. ê²°ê³¼ ë³‘í•©
        for i, r in enumerate(records):
            r["uncertainty"] = float(std_pred[i]) if len(std_pred) > 1 else float(std_pred[-1])
            r["confidence"] = float(confidence[i]) if len(confidence) > 1 else float(confidence[-1])

        pred_df = pd.DataFrame(records).round(4)
        self.pred_df = pred_df
        self.pred_prices = pred_prices

        print("\n================= ì˜ˆì¸¡ ê²°ê³¼ (í‘œ) =================")
        print(pred_df)

        print("\n================= ì˜ˆì¸¡ ê²°ê³¼ (ê°’) =================")
        print(pred_prices)

        # ë‹¨ì¼ í‹°ì»¤ì¼ ê²½ìš° target ìš”ì•½ ì œê³µ
        self.target = Target(
            next_close=float(pred_df["Predicted_Close"].iloc[-1]),
            uncertainty=float(std_pred[-1]),
            confidence=float(pred_df["confidence"].iloc[-1])
        )


        return self.pred_prices, self.target



    def macro_reviewer_draft(self):
        temporal_summary, causal_summary, interaction_summary = self.make_macro_shap()
        # -------------------------------
        # 4ï¸âƒ£ llm ìƒì„±
        # -------------------------------
        print("\n4ï¸âƒ£ Generating explanation using LLM...")

        llm  = LLMExplainer()
        feature_summary = feature_df.tail(5).describe().round(3).to_dict()
        explanation = llm.generate_explanation(feature_summary, self.pred_prices,
                                               importance_dict,
                                               temporal_summary, causal_summary,
                                               interaction_summary)

        print(f"\n================= pred_prices:{self.pred_prices} =================")

        print("\n================= LLM Explanation =================")
        print(explanation)
        print("===================================================")

        total_json = {
            'agent_id' : self.agent_id,
            'target' : self.target,
            'reason' : explanation
        }

        stock_data = {
            'temporal_summary' : temporal_summary,
            'causal_summary' : causal_summary,
            'interaction_summary' : interaction_summary

        }

        context = json.dumps({
            "agent_id": self.agent_id,
            "predicted_next_close": round(self.target.next_close, 3),
            "uncertainty_sigma": round(self.target.uncertainty or 0.0, 4),
            "confidence_beta": round(self.target.confidence or 0.0, 4),
            "latest_data": str(stock_data)
        }, ensure_ascii=False, indent=2)

        reason = explanation

        # 4) Opinion ê¸°ë¡/ë°˜í™˜ (í•­ìƒ ìµœì‹  ê°’ append)
        self.opinions.append(Opinion(agent_id=self.agent_id, target=self.target, reason=reason))

        return total_json, self.opinions[-1]




    def make_macro_shap(self):
        # -------------------------------
        # 3ï¸âƒ£ SHAP ê³„ì‚°
        # -------------------------------
        # --- (run() ì•ˆì˜ ì•ˆì „ ì²˜ë¦¬) ---
        X_scaled = self.X_scaled.astype(np.float32)
        X_scaled = X_scaled[:, :, :300]
        feature_names = feature_names[:300]

        print("\n3ï¸âƒ£ Calculating feature importance...")
        analyzer = AttributionAnalyzer(self.model)
        importance_dict, temporal_df, causal_df, interaction_df = analyzer.run_all_shap(X_scaled, feature_names)

        temporal_summary = temporal_df.head().to_dict(orient="records") if temporal_df is not None else []
        causal_summary = causal_df.to_dict(orient="records") if causal_df is not None else []
        if isinstance(interaction_df, pd.DataFrame):
            interaction_summary = interaction_df.iloc[:5, :5].round(3).to_dict()
        else:
            interaction_summary = {}

        return temporal_summary, causal_summary, interaction_summary



    #[base_agent.py]
    def _msg(self, role: str, content: str) -> dict:
        """OpenAI ChatCompletionìš© ë©”ì‹œì§€ êµ¬ì¡° ìƒì„±"""
        if not isinstance(role, str) or not isinstance(content, str):
            raise ValueError(f"_msg() ì¸ì ì˜¤ë¥˜: role={role}, content={type(content)}")
        return {"role": role, "content": content}


    #[base_agent.py]
    def macro_reviewer_rebut(self, my_opinion: Opinion, other_opinion: Opinion, round: int) -> Rebuttal:
        """LLMì„ í†µí•´ ìƒëŒ€ ì˜ê²¬ì— ëŒ€í•œ ë°˜ë°•/ì§€ì§€ ìƒì„±"""

        # ë©”ì‹œì§€ ìƒì„± (context êµ¬ì„±ì€ ë³„ë„ í—¬í¼ì—ì„œ)
        sys_text, user_text = self._build_messages_rebuttal(
            my_opinion=my_opinion,
            target_opinion=other_opinion,
            stock_data=self.stockdata
        )

        # LLM í˜¸ì¶œ
        parsed = self._ask_with_fallback(
            self._msg("system", sys_text),
            self._msg("user", user_text),
            {
                "type": "object",
                "properties": {
                    "stance": {"type": "string", "enum": ["REBUT", "SUPPORT"]},
                    "message": {"type": "string"}
                },
                "required": ["stance", "message"],
                "additionalProperties": False
            }
        )

        # ê²°ê³¼ ì •ë¦¬ ë° ê¸°ë¡
        result = Rebuttal(
            from_agent_id=my_opinion.agent_id,
            to_agent_id=other_opinion.agent_id,
            stance=parsed.get("stance", "REBUT"),
            message=parsed.get("message", "(ë°˜ë°•/ì§€ì§€ ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨)")
        )

        # ì €ì¥
        self.rebuttals[round].append(result)

        # ë””ë²„ê¹… ë¡œê·¸
        if self.verbose:
            print(
                f"[{self.agent_id}] rebuttal ìƒì„± â†’ {result.stance} "
                f"({my_opinion.agent_id} â†’ {other_opinion.agent_id})"
            )

        return result



    #[m_agent.py]
    def _build_messages_rebuttal(self,
                                 my_opinion: Opinion,
                                 target_opinion: Opinion,
                                 stock_data: StockData) -> tuple[str, str]:

        t = stock_data.ticker or "UNKNOWN"
        ccy = (stock_data.currency or "USD").upper()
        agent_data = getattr(stock_data, self.agent_id, None)
        if not agent_data or not isinstance(agent_data, dict):
            raise ValueError(f"{self.agent_id} ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜: dictí˜• ì»¬ëŸ¼ ë°ì´í„°ê°€ í•„ìš”í•¨")

        ctx = {
            "ticker": t,
            "currency": ccy,
            "data_summary": getattr(stock_data, self.agent_id, {}).get("feature_cols", []),
            "me": {
                "agent_id": self.agent_id,
                "next_close": float(my_opinion.target.next_close),
                "reason": str(my_opinion.reason)[:2000],
                "uncertainty": float(my_opinion.target.uncertainty),
                "confidence": float(my_opinion.target.confidence),
            },
            "other": {
                "agent_id": target_opinion.agent_id,
                "next_close": float(target_opinion.target.next_close),
                "reason": str(target_opinion.reason)[:2000],
                "uncertainty": float(target_opinion.target.uncertainty),
                "confidence": float(target_opinion.target.confidence),
            }
        }
        # ê° ì»¬ëŸ¼ë³„ ìµœê·¼ ì‹œê³„ì—´ ê·¸ëŒ€ë¡œ í¬í•¨
        # (ìµœê·¼ 7~14ì¼ ì •ë„ë©´ LLMì´ ì´í•´ ê°€ëŠ¥í•œ ë²”ìœ„)
        for col, values in agent_data.items():
            if isinstance(values, (list, tuple)):
                ctx[col] = values[self.window_size:]  # ìµœê·¼ 14ì¼ì¹˜ ì „ì²´ ì‹œê³„ì—´
            else:
                ctx[col] = [values]

        system_text = REBUTTAL_PROMPTS[self.agent_id]["system"]
        user_text   = REBUTTAL_PROMPTS[self.agent_id]["user"].format(
            context=json.dumps(ctx, ensure_ascii=False)
        )
        return system_text, user_text



    #[base_agent.py] OpenAI API í˜¸ì¶œ
    def _ask_with_fallback(self, msg_sys: dict, msg_user: dict, schema_obj: dict) -> dict:
        """ëª¨ë¸ í´ë°± í¬í•¨ OpenAI Responses API í˜¸ì¶œ"""
        payload_base = {
            "input": [msg_sys, msg_user],
            "text": {
                "format": {
                    "type": "json_schema",
                    "name": "Response",
                    "strict": True,
                    "schema": schema_obj,
                }
            },
            "temperature": self.temperature,
        }
        last_err = None
        for model in self.preferred_models:
            payload = dict(payload_base, model=model)
            try:
                r = requests.post(self.OPENAI_URL, headers=self.headers, json=payload, timeout=120)
                if r.ok:
                    data = r.json()
                    # 1) output_text ìš°ì„  ì‚¬ìš©
                    if isinstance(data.get("output_text"), str) and data["output_text"].strip():
                        try:
                            return json.loads(data["output_text"])
                        except Exception:
                            return {"reason": data["output_text"]}  # JSON ì‹¤íŒ¨ ì‹œ ì›ë¬¸ í…ìŠ¤íŠ¸ ë³´ì¡´
                    # 2) output ë°°ì—´ì—ì„œ í…ìŠ¤íŠ¸ ëª¨ìœ¼ê¸°
                    out = data.get("output")
                    if isinstance(out, list) and out:
                        texts = []
                        for blk in out:
                            for c in blk.get("content", []):
                                if "text" in c:
                                    texts.append(c["text"])
                        joined = "\n".join(t for t in texts if t)
                        if joined.strip():
                            try:
                                return json.loads(joined)
                            except Exception:
                                return {"reason": joined}
                    # ë¹„ì •ìƒ ì‘ë‹µ
                    return {}
                # 400/404ëŠ” ë‹¤ìŒ ëª¨ë¸ë¡œ í´ë°±
                if r.status_code in (400, 404):
                    last_err = (r.status_code, r.text)
                    continue
                # ê¸°íƒ€ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì˜ˆì™¸
                r.raise_for_status()
            except Exception as e:
                self._p(f"â–  ëª¨ë¸ {model} ì‹¤íŒ¨: {e}")
                last_err = str(e)
                continue
        raise RuntimeError(f"ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_err}")













    def macro_reviewer_revise(
            self,
            my_opinion: Opinion,
            others: List[Opinion],
            rebuttals: List[Rebuttal],
            stock_data: StockData,
            fine_tune: bool = True,
            lr: float = 1e-4,
            epochs: int = 20,
    ):
        """
        Revision ë‹¨ê³„
        - Ïƒ ê¸°ë°˜ Î²-weighted ì‹ ë¢°ë„ ê³„ì‚°
        - Î³ ìˆ˜ë ´ìœ¨ë¡œ ì˜ˆì¸¡ê°’ ë³´ì •
        - fine-tuning (ìˆ˜ìµë¥  ë‹¨ìœ„)
        - reasoning ìƒì„±
        """
        gamma = getattr(self, "gamma", 0.3)               # ìˆ˜ë ´ìœ¨ (0~1)
        delta_limit = getattr(self, "delta_limit", 0.05)  # fine-tuning ë³´ì • í•œê³„

        try:
            # ===================================
            # â‘  Î² ê³„ì‚° (ë¶ˆí™•ì‹¤ì„± ì‘ì„ìˆ˜ë¡ ì‹ ë¢° ë†’ìŒ)
            # ===================================
            my_price = my_opinion.target.next_close
            my_sigma = abs(my_opinion.target.uncertainty or 1e-6)

            other_prices = np.array([o.target.next_close for o in others])
            other_sigmas = np.array([abs(o.target.uncertainty or 1e-6) for o in others])

            all_sigmas = np.concatenate([[my_sigma], other_sigmas])
            all_prices = np.concatenate([[my_price], other_prices])

            inv_sigmas = 1 / (all_sigmas + 1e-6)
            betas = inv_sigmas / inv_sigmas.sum()

            # ===================================
            # â‘¡ ë…¼ë¬¸ì‹ ìˆ˜ë ´ ì—…ë°ì´íŠ¸
            #     y_i_rev = y_i + Î³ Î£ Î²_j (y_j - y_i)
            # ===================================
            delta = np.sum(betas[1:] * (other_prices - my_price))
            revised_price = my_price + gamma * delta

        except Exception as e:
            print(f"[{self.agent_id}] revised_target ê³„ì‚° ì‹¤íŒ¨: {e}")
            revised_price = my_opinion.target.next_close
            current_price = getattr(self.stockdata, "last_price", 100.0)
            price_uplimit = current_price * (1 + delta_limit)
            price_downlimit = current_price * (1 - delta_limit)
            revised_price = min(max(revised_price, price_downlimit), price_uplimit)

        # ===================================
        # â‘¢ Fine-tuning (return ë‹¨ìœ„)
        # ===================================
        loss_value = None
        if fine_tune and hasattr(self, "model"):
            try:
                current_price = getattr(self.stockdata, "last_price", 100.0)
                revised_return = (revised_price / current_price) - 1  # ğŸ”¹ìˆ˜ìµë¥  ë³€í™˜

                X_input = self.searcher(self.ticker)
                device = next(self.model.parameters()).device
                X_tensor = torch.tensor(X_input, dtype=torch.float32).to(device)
                y_tensor = torch.tensor([[revised_return]], dtype=torch.float32).to(device)

                self.model.train()
                optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
                criterion = torch.nn.MSELoss()

                for _ in range(epochs):
                    optimizer.zero_grad()
                    pred = self.model(X_tensor)
                    delta_loss = pred - y_tensor
                    loss = criterion(pred - delta_loss, y_tensor)
                    loss.backward()
                    optimizer.step()

                loss_value = float(loss.item())
                print(f"[{self.agent_id}] fine-tuning ì™„ë£Œ: loss={loss_value:.6f}")

            except Exception as e:
                print(f"[{self.agent_id}] fine-tuning ì‹¤íŒ¨: {e}")

        # ===================================
        # â‘£ fine-tuning ì´í›„ ìƒˆ ì˜ˆì¸¡ ìƒì„±
        # ===================================
        try:
            X_latest = self.searcher(self.ticker)
            new_target = self.predict(X_latest)
        except Exception as e:
            print(f"[{self.agent_id}] predict ì‹¤íŒ¨: {e}")
            new_target = my_opinion.target

        # ===================================
        # â‘¤ reasoning ìƒì„±
        # ===================================
        try:
            sys_text, user_text = self._build_messages_revision(
                my_opinion=my_opinion,
                others=others,
                rebuttals=rebuttals,
                stock_data=stock_data,
            )
        except Exception as e:
            print(f"[{self.agent_id}] _build_messages_revision ì‹¤íŒ¨: {e}")
            sys_text, user_text = (
                "ë„ˆëŠ” ê¸ˆìœµ ë¶„ì„ê°€ë‹¤. ê°„ë‹¨íˆ reasonë§Œ ìƒì„±í•˜ë¼.",
                json.dumps({"reason": "ê¸°ë³¸ ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨"}),
            )

        parsed = self._ask_with_fallback(
            self._msg("system", sys_text),
            self._msg("user", user_text),
            {
                "type": "object",
                "properties": {"reason": {"type": "string"}},
                "required": ["reason"],
                "additionalProperties": False,
            },
        )

        revised_reason = parsed.get("reason", "(ìˆ˜ì • ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨)")
        revised_opinion = Opinion(
            agent_id=self.agent_id,
            target=new_target,
            reason=revised_reason,
        )

        self.opinions.append(revised_opinion)
        print(f"[{self.agent_id}] revise ì™„ë£Œ â†’ new_close={new_target.next_close:.2f}, loss={loss_value}")
        return self.opinions[-1]