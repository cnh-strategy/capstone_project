{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "build_dataset 해당",
   "id": "4d4e0cb5279bad2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T00:52:52.179094Z",
     "start_time": "2025-11-06T00:52:02.852150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.macro_classes.macro_class_dataset import MacroAData\n",
    "\n",
    "macro_agent = MacroAData()\n",
    "macro_agent.fetch_data()\n",
    "macro_agent.add_features()\n",
    "macro_agent.save_csv()\n",
    "macro_agent.make_close_price()"
   ],
   "id": "7c9656cd98e36922",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  15 of 15 completed\n",
      "C:\\Users\\SPOTV\\IdeaProjects\\capstone_project\\core\\macro_classes\\macro_class_dataset.py:85: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  df = df.stack(level=0)\n",
      "C:\\Users\\SPOTV\\IdeaProjects\\capstone_project\\core\\macro_classes\\macro_class_dataset.py:133: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df_prices = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MacroSentimentAgent] Data shape: (1304, 90), Columns: 90\n",
      "[TRACE A] add_features() for self.data:            Open_CL=F  Open_DX-Y.NYB  Open_EURUSD=X    Open_GC=F  Open_HG=F  \\\n",
      "Date                                                                          \n",
      "2020-01-01        NaN            NaN       1.122083          NaN        NaN   \n",
      "2020-01-02  61.599998      96.480003       1.121894  1518.099976     2.8165   \n",
      "2020-01-03  61.180000      96.790001       1.117081  1530.099976     2.7935   \n",
      "2020-01-06  63.709999      96.900002       1.116246  1580.000000     2.7780   \n",
      "2020-01-07  62.910000      96.650002       1.119583  1558.300049     2.8010   \n",
      "...               ...            ...            ...          ...        ...   \n",
      "2024-12-24  69.559998     108.160004       1.040583  2613.000000     4.0525   \n",
      "2024-12-25        NaN            NaN       1.040258          NaN        NaN   \n",
      "2024-12-26  70.199997     108.169998       1.039955  2628.500000     4.0730   \n",
      "2024-12-27  69.680000     108.080002       1.042318  2617.699951     4.0615   \n",
      "2024-12-30  70.419998     108.010002       1.042938  2620.699951     4.0900   \n",
      "\n",
      "              Open_QQQ    Open_SPY  Open_USDJPY=X     Open_^DJI  Open_^FVX  \\\n",
      "Date                                                                         \n",
      "2020-01-01         NaN         NaN     108.680000           NaN        NaN   \n",
      "2020-01-02  214.399994  323.540009     108.713997  28638.970703      1.683   \n",
      "2020-01-03  213.300003  321.160004     108.540001  28553.330078      1.622   \n",
      "2020-01-06  212.500000  320.489990     107.999001  28465.500000      1.591   \n",
      "2020-01-07  215.639999  323.019989     108.411003  28639.179688      1.596   \n",
      "...                ...         ...            ...           ...        ...   \n",
      "2024-12-24  524.830017  596.059998     157.164993  42916.480469      4.452   \n",
      "2024-12-25         NaN         NaN     157.106995           NaN        NaN   \n",
      "2024-12-26  528.320007  599.500000     157.132996  43201.851562      4.484   \n",
      "2024-12-27  526.010010  597.539978     157.748001  43142.371094      4.447   \n",
      "2024-12-30  515.510010  587.890015     157.873001  42863.859375      4.408   \n",
      "\n",
      "            ...  Volume_QQQ  Volume_SPY  Volume_USDJPY=X  Volume_^DJI  \\\n",
      "Date        ...                                                         \n",
      "2020-01-01  ...         NaN         NaN              0.0          NaN   \n",
      "2020-01-02  ...  30969400.0  59151200.0              0.0  251820000.0   \n",
      "2020-01-03  ...  27518900.0  77709700.0              0.0  239590000.0   \n",
      "2020-01-06  ...  21655300.0  55653900.0              0.0  252760000.0   \n",
      "2020-01-07  ...  22139300.0  40496400.0              0.0  258900000.0   \n",
      "...         ...         ...         ...              ...          ...   \n",
      "2024-12-24  ...  17558200.0  33160100.0              0.0  230410000.0   \n",
      "2024-12-25  ...         NaN         NaN              0.0          NaN   \n",
      "2024-12-26  ...  19090500.0  41219100.0              0.0  270350000.0   \n",
      "2024-12-27  ...  33839600.0  64969300.0              0.0  376960000.0   \n",
      "2024-12-30  ...  34584000.0  56578800.0              0.0  383300000.0   \n",
      "\n",
      "            Volume_^FVX  Volume_^GSPC  Volume_^IRX  Volume_^IXIC  Volume_^TNX  \\\n",
      "Date                                                                            \n",
      "2020-01-01          NaN           NaN          NaN           NaN          NaN   \n",
      "2020-01-02          0.0  3.459930e+09          0.0  2.862700e+09          0.0   \n",
      "2020-01-03          0.0  3.484700e+09          0.0  2.586520e+09          0.0   \n",
      "2020-01-06          0.0  3.702460e+09          0.0  2.810450e+09          0.0   \n",
      "2020-01-07          0.0  3.435910e+09          0.0  2.381740e+09          0.0   \n",
      "...                 ...           ...          ...           ...          ...   \n",
      "2024-12-24          0.0  1.757720e+09          0.0  4.739190e+09          0.0   \n",
      "2024-12-25          NaN           NaN          NaN           NaN          NaN   \n",
      "2024-12-26          0.0  2.904530e+09          0.0  6.467910e+09          0.0   \n",
      "2024-12-27          0.0  3.159610e+09          0.0  7.765120e+09          0.0   \n",
      "2024-12-30          0.0  3.433250e+09          0.0  8.384090e+09          0.0   \n",
      "\n",
      "            Volume_^VIX  \n",
      "Date                     \n",
      "2020-01-01          NaN  \n",
      "2020-01-02          0.0  \n",
      "2020-01-03          0.0  \n",
      "2020-01-06          0.0  \n",
      "2020-01-07          0.0  \n",
      "...                 ...  \n",
      "2024-12-24          0.0  \n",
      "2024-12-25          NaN  \n",
      "2024-12-26          0.0  \n",
      "2024-12-27          0.0  \n",
      "2024-12-30          0.0  \n",
      "\n",
      "[1304 rows x 90 columns]\n",
      "[TRACE A] add_features() for NVDA columns: ['Open_CL=F', 'Open_DX-Y.NYB', 'Open_EURUSD=X', 'Open_GC=F', 'Open_HG=F', 'Open_QQQ', 'Open_SPY', 'Open_USDJPY=X', 'Open_^DJI', 'Open_^FVX', 'Open_^GSPC', 'Open_^IRX', 'Open_^IXIC', 'Open_^TNX', 'Open_^VIX']\n",
      "[MacroSentimentAgent] Saved data/processed\\NVDA_MacroSentiAgent.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  101 of 101 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: (1259, 101) rows\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T01:01:14.617002Z",
     "start_time": "2025-11-06T00:53:00.176502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.macro_classes.macro_class_dataset import MacroAData\n",
    "macro_agent = MacroAData()\n",
    "macro_agent.model_maker()"
   ],
   "id": "910104a88ee5e69b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SPOTV\\IdeaProjects\\capstone_project\\core\\macro_classes\\macro_class_dataset.py:164: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  macro_ret = macro_df[macro_features].pct_change()\n",
      "C:\\Users\\SPOTV\\IdeaProjects\\capstone_project\\core\\macro_classes\\macro_class_dataset.py:177: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  price_df = price_df.fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 병합 후 데이터 shape: (1257, 285)\n",
      "Epoch 1/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 72ms/step - loss: 0.1484 - val_loss: 0.1307\n",
      "Epoch 2/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 66ms/step - loss: 0.1347 - val_loss: 0.0847\n",
      "Epoch 3/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 45ms/step - loss: 0.1293 - val_loss: 0.0952\n",
      "Epoch 4/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 48ms/step - loss: 0.1274 - val_loss: 0.1022\n",
      "Epoch 5/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 33ms/step - loss: 0.1259 - val_loss: 0.0944\n",
      "Epoch 6/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 44ms/step - loss: 0.1241 - val_loss: 0.1121\n",
      "Epoch 7/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 56ms/step - loss: 0.1242 - val_loss: 0.0818\n",
      "Epoch 8/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 82ms/step - loss: 0.1234 - val_loss: 0.0876\n",
      "Epoch 9/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 79ms/step - loss: 0.1215 - val_loss: 0.0845\n",
      "Epoch 10/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 75ms/step - loss: 0.1233 - val_loss: 0.0933\n",
      "Epoch 11/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 70ms/step - loss: 0.1213 - val_loss: 0.0812\n",
      "Epoch 12/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 123ms/step - loss: 0.1225 - val_loss: 0.0855\n",
      "Epoch 13/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 147ms/step - loss: 0.1225 - val_loss: 0.0813\n",
      "Epoch 14/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 87ms/step - loss: 0.1205 - val_loss: 0.0821\n",
      "Epoch 15/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 128ms/step - loss: 0.1181 - val_loss: 0.0811\n",
      "Epoch 16/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 202ms/step - loss: 0.1195 - val_loss: 0.0886\n",
      "Epoch 17/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 172ms/step - loss: 0.1200 - val_loss: 0.0833\n",
      "Epoch 18/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 169ms/step - loss: 0.1198 - val_loss: 0.0829\n",
      "Epoch 19/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 139ms/step - loss: 0.1185 - val_loss: 0.0824\n",
      "Epoch 20/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 143ms/step - loss: 0.1193 - val_loss: 0.0845\n",
      "Epoch 21/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 138ms/step - loss: 0.1189 - val_loss: 0.0824\n",
      "Epoch 22/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 131ms/step - loss: 0.1183 - val_loss: 0.0838\n",
      "Epoch 23/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 164ms/step - loss: 0.1175 - val_loss: 0.0831\n",
      "Epoch 24/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 186ms/step - loss: 0.1168 - val_loss: 0.0844\n",
      "Epoch 25/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 174ms/step - loss: 0.1151 - val_loss: 0.0830\n",
      "Epoch 26/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 182ms/step - loss: 0.1166 - val_loss: 0.0874\n",
      "Epoch 27/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 158ms/step - loss: 0.1151 - val_loss: 0.0921\n",
      "Epoch 28/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 208ms/step - loss: 0.1170 - val_loss: 0.0875\n",
      "Epoch 29/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 184ms/step - loss: 0.1164 - val_loss: 0.0927\n",
      "Epoch 30/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 180ms/step - loss: 0.1145 - val_loss: 0.0873\n",
      "Epoch 31/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 147ms/step - loss: 0.1149 - val_loss: 0.0855\n",
      "Epoch 32/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 138ms/step - loss: 0.1148 - val_loss: 0.0921\n",
      "Epoch 33/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 118ms/step - loss: 0.1149 - val_loss: 0.0900\n",
      "Epoch 34/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 152ms/step - loss: 0.1135 - val_loss: 0.0967\n",
      "Epoch 35/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 140ms/step - loss: 0.1131 - val_loss: 0.0945\n",
      "Epoch 36/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 185ms/step - loss: 0.1125 - val_loss: 0.0999\n",
      "Epoch 37/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 193ms/step - loss: 0.1127 - val_loss: 0.0960\n",
      "Epoch 38/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 208ms/step - loss: 0.1127 - val_loss: 0.0939\n",
      "Epoch 39/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 146ms/step - loss: 0.1115 - val_loss: 0.0929\n",
      "Epoch 40/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 141ms/step - loss: 0.1117 - val_loss: 0.0957\n",
      "Epoch 41/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 143ms/step - loss: 0.1115 - val_loss: 0.0945\n",
      "Epoch 42/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 178ms/step - loss: 0.1117 - val_loss: 0.1002\n",
      "Epoch 43/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 137ms/step - loss: 0.1101 - val_loss: 0.0968\n",
      "Epoch 44/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 135ms/step - loss: 0.1109 - val_loss: 0.0980\n",
      "Epoch 45/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 136ms/step - loss: 0.1092 - val_loss: 0.1027\n",
      "Epoch 46/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 131ms/step - loss: 0.1105 - val_loss: 0.0986\n",
      "Epoch 47/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 130ms/step - loss: 0.1090 - val_loss: 0.1010\n",
      "Epoch 48/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 132ms/step - loss: 0.1082 - val_loss: 0.1004\n",
      "Epoch 49/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 121ms/step - loss: 0.1083 - val_loss: 0.1028\n",
      "Epoch 50/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 132ms/step - loss: 0.1079 - val_loss: 0.1010\n",
      "Epoch 51/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 129ms/step - loss: 0.1057 - val_loss: 0.1018\n",
      "Epoch 52/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 128ms/step - loss: 0.1058 - val_loss: 0.1052\n",
      "Epoch 53/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 144ms/step - loss: 0.1068 - val_loss: 0.1056\n",
      "Epoch 54/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 139ms/step - loss: 0.1062 - val_loss: 0.1014\n",
      "Epoch 55/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 144ms/step - loss: 0.1047 - val_loss: 0.0985\n",
      "Epoch 56/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 135ms/step - loss: 0.1052 - val_loss: 0.0959\n",
      "Epoch 57/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 155ms/step - loss: 0.1045 - val_loss: 0.1031\n",
      "Epoch 58/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 143ms/step - loss: 0.1039 - val_loss: 0.0992\n",
      "Epoch 59/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 137ms/step - loss: 0.1032 - val_loss: 0.1024\n",
      "Epoch 60/60\n",
      "\u001B[1m55/55\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 129ms/step - loss: 0.1026 - val_loss: 0.1042\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 340ms/step\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$$$   X = macro_sercher(agent, ticker) 역할",
   "id": "ea224a544273913e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T01:19:55.788630Z",
     "start_time": "2025-11-06T01:19:37.459791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from agents.macro_agent import MacroPredictor\n",
    "from datetime import datetime\n",
    "\n",
    "macro_sub = MacroPredictor(\n",
    "    agent_id='MacroSentiAgent',\n",
    "    base_date=datetime.today(),\n",
    "    window=40,\n",
    "    ticker=\"NVDA\"\n",
    ")\n",
    "macro_sub.load_assets()               # 모델, 스케일러 등 불러오기\n"
   ],
   "id": "20f2916c5b0a7ce1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 모델 및 스케일러 로드 중...\n",
      "model_path: models/NVDA_MacroSentiAgent.keras\n",
      "[OK] 모델 및 스케일러 로드 완료\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T01:27:03.786164Z",
     "start_time": "2025-11-06T01:27:02.575017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "macro_sub.fetch_macro_data()          # macro_df 불러오기\n",
    "X_seq, X_scaled = macro_sub.prepare_features()  # 입력 시퀀스 준비"
   ],
   "id": "1f6df604ed6e7c6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] MacroSentimentAgent 데이터 수집 중...\n",
      "1️⃣ Collecting macro features (15 tickers)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  15 of 15 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MacroSentimentAgent] Macro data: (44, 91)\n",
      "   ↳ Downloading NVDA ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MacroSentimentAgent] Stock data: (43, 9)\n",
      "[MacroSentimentAgent] Data shape: (43, 99)\n",
      "[MacroSentimentAgent] Feature engineering complete. Final shape: (43, 184)\n",
      "[OK] 매크로 데이터 수집 완료: (43, 185)\n",
      "[INFO] 피처 정리 및 스케일링 중...\n",
      "[Check] 입력 피처 수: 183 / 스케일러 기준 피처 수: 183\n",
      "[OK] 스케일링 및 시퀀스 변환 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T01:27:06.081461Z",
     "start_time": "2025-11-06T01:27:06.064695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"X_seq: {X_seq}\")\n",
    "print(f\"X_scaled: {X_scaled}\")"
   ],
   "id": "ce331ea370f7d962",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_seq: [[[-3.58716003e-01 -3.78179663e-01  1.05884921e+00 ... -1.15013917e-01\n",
      "    3.59026218e+00  3.63362588e+00]\n",
      "  [-4.34728495e-01 -4.25056313e-01  1.11257430e+00 ...  1.80760152e-02\n",
      "    3.59026218e+00  3.63362588e+00]\n",
      "  [-3.99951473e-01 -4.05958615e-01  1.09840824e+00 ... -1.01681348e-01\n",
      "    3.64010922e+00  3.63362588e+00]\n",
      "  ...\n",
      "  [-4.77951267e-01 -3.78954535e-02  7.57917528e-01 ...  5.49018948e-01\n",
      "    4.37608945e+00  4.15011879e+00]\n",
      "  [-4.96333524e-01  2.03645824e-03  7.42651070e-01 ... -1.25707310e+00\n",
      "    4.36374573e+00  4.19667214e+00]\n",
      "  [-5.63403347e-01  2.42588248e-02  6.84992780e-01 ... -6.06381612e-01\n",
      "    4.30134135e+00  4.23632086e+00]]]\n",
      "X_scaled:     Open_CL=F  Open_DX-Y.NYB  Open_EURUSD=X  Open_GC=F  Open_HG=F  Open_QQQ  \\\n",
      "0   -0.448142      -0.378180       1.075549   6.229004   1.045188  3.003366   \n",
      "1   -0.426779      -0.449362       1.171576   6.427028   1.090371  3.020078   \n",
      "2   -0.411378      -0.371235       1.057673   6.343827   1.077688  3.072398   \n",
      "3   -0.358716      -0.378180       1.058849   6.373568   1.151408  3.065970   \n",
      "4   -0.434728      -0.425056       1.112574   6.458651   1.213238  3.089109   \n",
      "5   -0.399951      -0.405959       1.098408   6.400298   1.230676  3.132817   \n",
      "6   -0.383060      -0.456307       1.162313   6.556156   1.286164  3.186422   \n",
      "7   -0.319468      -0.581308       1.347051   6.509474   1.183115  3.167011   \n",
      "8   -0.349276      -0.527488       1.269726   6.455263   1.166469  3.215989   \n",
      "9   -0.369149      -0.456307       1.209196   6.471827   1.141896  3.247098   \n",
      "10  -0.411378      -0.402486       1.116354   6.581757   1.187079  3.252369   \n",
      "11  -0.431748      -0.459779       1.226386   6.803122   1.183115  3.311888   \n",
      "12  -0.366665      -0.473668       1.249351   6.888957   1.202139  3.276022   \n",
      "13  -0.309034      -0.372971       1.129600   6.787310   1.457384  3.181152   \n",
      "14  -0.289162      -0.254914       0.989060   6.909663   1.352750  3.208790   \n",
      "15  -0.295621      -0.312207       1.067078   6.832486   1.402688  3.269980   \n",
      "16  -0.391506      -0.357345       1.106905   7.106180   1.540615  3.261239   \n",
      "17  -0.425289      -0.372971       1.114464   7.241710   1.565981  3.245041   \n",
      "18  -0.459072      -0.393806       1.111392   7.214227   1.615128  3.372178   \n",
      "19  -0.512728      -0.367763       1.096050   7.210463   1.793481  3.365108   \n",
      "20  -0.490869      -0.348666       1.074371   7.496957   1.868787  3.390046   \n",
      "21  -0.461557      -0.326096       1.070606   7.602745   1.971836  3.397374   \n",
      "22  -0.445658      -0.235816       0.975066   7.707404   1.935372  3.350967   \n",
      "23  -0.432741      -0.195885       0.936223   7.797757   1.954396  3.428997   \n",
      "24  -0.473480      -0.100397       0.823804   7.593710   2.048726  3.427969   \n",
      "25  -0.597187      -0.178523       0.885512   7.815828   1.972628  3.277307   \n",
      "26  -0.568371      -0.116021       0.824494   8.251405   1.726103  3.221131   \n",
      "27  -0.617059      -0.159425       0.885281   8.301475   1.849762  3.332970   \n",
      "28  -0.608613      -0.225399       0.960861   8.504015   1.753055  3.347110   \n",
      "29  -0.671709      -0.296581       1.052271   9.090935   1.793481  3.255068   \n",
      "30  -0.660282      -0.225399       0.979962   8.674558   1.839458  3.373206   \n",
      "31  -0.678167      -0.235816       0.957603   9.008111   1.769701  3.431054   \n",
      "32  -0.667237      -0.173315       0.882048   8.168581   1.714214  3.420513   \n",
      "33  -0.550486      -0.181996       0.895911   8.050745   1.810921  3.344539   \n",
      "34  -0.458576      -0.178523       0.912341   8.197945   2.013847  3.486973   \n",
      "35  -0.457085      -0.195885       0.935758   7.981475   2.162079  3.596627   \n",
      "36  -0.472983      -0.211510       0.970173   7.490933   1.991652  3.671700   \n",
      "37  -0.538563      -0.213246       0.980662   7.694227   2.181104  3.738933   \n",
      "38  -0.528130      -0.143800       0.887130   7.605004   2.141470  3.694968   \n",
      "39  -0.532601      -0.083036       0.834139   7.885475   1.989274  3.720678   \n",
      "40  -0.477951      -0.037895       0.757918   7.665992   2.004335  3.732762   \n",
      "41  -0.496334       0.002036       0.742651   7.733757   1.737201  3.580687   \n",
      "42  -0.563403       0.024259       0.684993   7.726980   1.853726  3.519111   \n",
      "\n",
      "    Open_SPY  Open_USDJPY=X  Open_^DJI  Open_^FVX  ...  Volume_^DJI_ret  \\\n",
      "0   2.930416       1.107791   2.529134   0.656034  ...        -0.106965   \n",
      "1   2.934997       1.054548   2.554235   0.663442  ...        -0.106965   \n",
      "2   2.995868       1.056069   2.593679   0.682580  ...        -0.106965   \n",
      "3   3.003199       1.052810   2.560556   0.675172  ...        -0.106965   \n",
      "4   3.047969       1.046942   2.667824   0.688137  ...        -0.106965   \n",
      "5   3.074674       1.070956   2.618754   0.682580  ...        -0.106965   \n",
      "6   3.098629       1.056776   2.634016   0.675172  ...        -0.106965   \n",
      "7   3.079517       1.002880   2.603740   0.667764  ...        -0.106965   \n",
      "8   3.104128       1.028850   2.663407   0.688137  ...        -0.106965   \n",
      "9   3.109887       1.086223   2.696573   0.726414  ...        -0.106965   \n",
      "10  3.108186       1.094263   2.695615   0.721475  ...        -0.106965   \n",
      "11  3.167354       1.074162   2.729383   0.726414  ...        -0.106965   \n",
      "12  3.138425       1.068077   2.730419   0.725796  ...        -0.106965   \n",
      "13  3.052420       1.129904   2.672176   0.752960  ...        -0.106965   \n",
      "14  3.072972       1.190210   2.673039   0.767160  ...        -0.106965   \n",
      "15  3.136461       1.165436   2.716991   0.761604  ...        -0.106965   \n",
      "16  3.117741       1.121428   2.711904   0.748021  ...        -0.106965   \n",
      "17  3.120883       1.088939   2.729956   0.726414  ...        -0.106965   \n",
      "18  3.216183       1.039281   2.750191   0.723327  ...        -0.106965   \n",
      "19  3.210161       1.042921   2.776542   0.717771  ...        -0.106965   \n",
      "20  3.231499       1.176356   2.817748   0.762221  ...        -0.106965   \n",
      "21  3.243542       1.215745   2.801629   0.762838  ...        -0.106965   \n",
      "22  3.213565       1.309192   2.790565   0.730735  ...        -0.106965   \n",
      "23  3.256502       1.343366   2.784770   0.752343  ...        -0.106965   \n",
      "24  3.238175       1.358850   2.735983   0.731970  ...        -0.106965   \n",
      "25  3.087895       1.309464   2.586592   0.701102  ...        -0.106965   \n",
      "26  3.042340       1.330001   2.623795   0.673937  ...        -0.106965   \n",
      "27  3.168664       1.291807   2.731756   0.679494  ...        -0.106965   \n",
      "28  3.168664       1.250679   2.713290   0.678259  ...        -0.106965   \n",
      "29  3.072841       1.204281   2.621759   0.656034  ...        -0.106965   \n",
      "30  3.175209       1.246659   2.718393   0.667764  ...        -0.106965   \n",
      "31  3.229142       1.239813   2.802955   0.653564  ...        -0.106965   \n",
      "32  3.236473       1.300609   2.853254   0.643686  ...        -0.106965   \n",
      "33  3.185682       1.303922   2.762637   0.664059  ...        -0.106965   \n",
      "34  3.294857       1.339129   2.825357   0.683815  ...        -0.106965   \n",
      "35  3.376934       1.364066   2.954342   0.686902  ...        -0.106965   \n",
      "36  3.433486       1.344018   3.027181   0.679494  ...        -0.106965   \n",
      "37  3.455347       1.291807   3.025988   0.688137  ...        -0.106965   \n",
      "38  3.392251       1.345648   2.961652   0.751108  ...        -0.106965   \n",
      "39  3.407174       1.409648   3.007362   0.748021  ...        -0.106965   \n",
      "40  3.415421       1.425676   3.015377   0.743700  ...        -0.106965   \n",
      "41  3.290275       1.426165   2.897547   0.736292  ...        -0.106965   \n",
      "42  3.275483       1.391014   2.886665   0.738144  ...        -0.106965   \n",
      "\n",
      "    Volume_^FVX_ret  Volume_^GSPC_ret  Volume_^IRX_ret  Volume_^IXIC_ret  \\\n",
      "0               0.0         -0.078646              0.0         -0.078528   \n",
      "1               0.0         -0.078646              0.0         -0.078528   \n",
      "2               0.0         -0.078646              0.0         -0.078528   \n",
      "3               0.0         -0.078646              0.0         -0.078528   \n",
      "4               0.0         -0.078646              0.0         -0.078528   \n",
      "5               0.0         -0.078646              0.0         -0.078528   \n",
      "6               0.0         -0.078646              0.0         -0.078528   \n",
      "7               0.0         -0.078646              0.0         -0.078528   \n",
      "8               0.0         -0.078646              0.0         -0.078528   \n",
      "9               0.0         -0.078646              0.0         -0.078528   \n",
      "10              0.0         -0.078646              0.0         -0.078528   \n",
      "11              0.0         -0.078646              0.0         -0.078528   \n",
      "12              0.0         -0.078646              0.0         -0.078528   \n",
      "13              0.0         -0.078646              0.0         -0.078528   \n",
      "14              0.0         -0.078646              0.0         -0.078528   \n",
      "15              0.0         -0.078646              0.0         -0.078528   \n",
      "16              0.0         -0.078646              0.0         -0.078528   \n",
      "17              0.0         -0.078646              0.0         -0.078528   \n",
      "18              0.0         -0.078646              0.0         -0.078528   \n",
      "19              0.0         -0.078646              0.0         -0.078528   \n",
      "20              0.0         -0.078646              0.0         -0.078528   \n",
      "21              0.0         -0.078646              0.0         -0.078528   \n",
      "22              0.0         -0.078646              0.0         -0.078528   \n",
      "23              0.0         -0.078646              0.0         -0.078528   \n",
      "24              0.0         -0.078646              0.0         -0.078528   \n",
      "25              0.0         -0.078646              0.0         -0.078528   \n",
      "26              0.0         -0.078646              0.0         -0.078528   \n",
      "27              0.0         -0.078646              0.0         -0.078528   \n",
      "28              0.0         -0.078646              0.0         -0.078528   \n",
      "29              0.0         -0.078646              0.0         -0.078528   \n",
      "30              0.0         -0.078646              0.0         -0.078528   \n",
      "31              0.0         -0.078646              0.0         -0.078528   \n",
      "32              0.0         -0.078646              0.0         -0.078528   \n",
      "33              0.0         -0.078646              0.0         -0.078528   \n",
      "34              0.0         -0.078646              0.0         -0.078528   \n",
      "35              0.0         -0.078646              0.0         -0.078528   \n",
      "36              0.0         -0.078646              0.0         -0.078528   \n",
      "37              0.0         -0.078646              0.0         -0.078528   \n",
      "38              0.0         -0.078646              0.0         -0.078528   \n",
      "39              0.0         -0.078646              0.0         -0.078528   \n",
      "40              0.0         -0.078646              0.0         -0.078528   \n",
      "41              0.0         -0.078646              0.0         -0.078528   \n",
      "42              0.0         -0.078646              0.0         -0.078528   \n",
      "\n",
      "    Volume_^TNX_ret  Volume_^VIX_ret  NVDA_ret1  NVDA_ma5  NVDA_ma10  \n",
      "0               0.0              0.0   0.339026  3.590262   3.633626  \n",
      "1               0.0              0.0   0.339026  3.590262   3.633626  \n",
      "2               0.0              0.0   1.044110  3.590262   3.633626  \n",
      "3               0.0              0.0  -0.115014  3.590262   3.633626  \n",
      "4               0.0              0.0   0.018076  3.590262   3.633626  \n",
      "5               0.0              0.0  -0.101681  3.640109   3.633626  \n",
      "6               0.0              0.0  -0.566042  3.661893   3.633626  \n",
      "7               0.0              0.0  -0.863785  3.624809   3.633626  \n",
      "8               0.0              0.0   0.939914  3.619904   3.633626  \n",
      "9               0.0              0.0  -0.018154  3.613837   3.633626  \n",
      "10              0.0              0.0   1.067905  3.644749   3.674282  \n",
      "11              0.0              0.0  -0.921722  3.663476   3.694677  \n",
      "12              0.0              0.0  -0.331281  3.698714   3.693747  \n",
      "13              0.0              0.0   0.029857  3.706362   3.695128  \n",
      "14              0.0              0.0  -0.007127  3.714381   3.696111  \n",
      "15              0.0              0.0   0.515408  3.705097   3.706999  \n",
      "16              0.0              0.0   0.676671  3.748089   3.738070  \n",
      "17              0.0              0.0   0.014201  3.802264   3.783083  \n",
      "18              0.0              0.0   0.169694  3.861345   3.816677  \n",
      "19              0.0              0.0  -0.288274  3.911089   3.845756  \n",
      "20              0.0              0.0  -0.416881  3.930554   3.850881  \n",
      "21              0.0              0.0  -0.169515  3.922431   3.868435  \n",
      "22              0.0              0.0   0.558311  3.932295   3.900675  \n",
      "23              0.0              0.0   0.449269  3.951707   3.940191  \n",
      "24              0.0              0.0  -1.530550  3.928181   3.953389  \n",
      "25              0.0              0.0   0.740394  3.942845   3.970571  \n",
      "26              0.0              0.0  -1.387742  3.916417   3.953177  \n",
      "27              0.0              0.0  -0.122824  3.867464   3.933498  \n",
      "28              0.0              0.0   0.234493  3.810704   3.914696  \n",
      "29              0.0              0.0   0.138540  3.811021   3.903012  \n",
      "30              0.0              0.0  -0.183393  3.781058   3.895310  \n",
      "31              0.0              0.0  -0.328950  3.787019   3.885006  \n",
      "32              0.0              0.0  -0.233271  3.789393   3.861557  \n",
      "33              0.0              0.0   0.217333  3.791239   3.833912  \n",
      "34              0.0              0.0   0.573414  3.807275   3.842144  \n",
      "35              0.0              0.0   0.737652  3.853960   3.850563  \n",
      "36              0.0              0.0   1.378534  3.958776   3.906331  \n",
      "37              0.0              0.0   0.791212  4.099938   3.978591  \n",
      "38              0.0              0.0  -0.680953  4.209291   4.034572  \n",
      "39              0.0              0.0  -0.148192  4.294906   4.085746  \n",
      "40              0.0              0.0   0.549019  4.376089   4.150119  \n",
      "41              0.0              0.0  -1.257073  4.363746   4.196672  \n",
      "42              0.0              0.0  -0.606382  4.301341   4.236321  \n",
      "\n",
      "[43 rows x 183 columns]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$$$   macro_predictor(X) 역할",
   "id": "a0ec178490422fd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T01:27:16.172435Z",
     "start_time": "2025-11-06T01:27:10.453211Z"
    }
   },
   "cell_type": "code",
   "source": "pred_prices, target = macro_sub.m_predictor(X_seq)",
   "id": "3f208c72ef015a74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 예측 수행 중...\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 249ms/step\n",
      "NVDA: 마지막 종가=197.41 → 예측 종가=193.62 (예상 수익률 -1.92%)\n",
      "\n",
      "================= 예측 결과 (표) =================\n",
      "  Ticker  Last_Close  Predicted_Close  Predicted_Return  Predicted_%  \\\n",
      "0   NVDA     197.414         193.6174           -0.0192      -1.9232   \n",
      "\n",
      "   uncertainty  confidence  \n",
      "0       0.0469      21.325  \n",
      "\n",
      "================= 예측 결과 (값) =================\n",
      "{'NVDA': 193.61740833527153}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T01:28:23.442448Z",
     "start_time": "2025-11-06T01:28:23.437908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"pred_prices: {pred_prices}\")\n",
    "print(f\"target: {target}\")"
   ],
   "id": "3962e7a2abc09557",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_prices: {'NVDA': 193.61740833527153}\n",
      "target: Target(next_close=193.6174, uncertainty=0.046893410384655, confidence=21.325)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "llm 생성 용.. macro_reviewer_draft",
   "id": "c4c1ab7b4259bf8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " total_json, opinion = macro_sub.macro_reviewer_draft(X_scaled, pred_prices, target)\n",
    " "
   ],
   "id": "c0580e3afb75317b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"total_json: {total_json}\")\n",
    "print(f\"opinion: {opinion}\")"
   ],
   "id": "e465cbfdec3fa3a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "reviewer_rebut",
   "id": "38958bae53f76919"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# reviewer_rebut()",
   "id": "8861ba8ea57e6dd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[토론 테스트]",
   "id": "9b3beaffa3bfa8ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from agents.debate_agent import DebateAgent\n",
    "\n",
    "debate_agent = DebateAgent(1, \"NVDA\")"
   ],
   "id": "f98afe82a1fb3f38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "debate_agent.run_dataset()",
   "id": "36da10f24a119bdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for agent_id, agent in debate_agent.agents.items():\n",
    "    scaler = getattr(agent, \"scaler\", None)\n",
    "    y_scaler = getattr(scaler, \"y_scaler\", None)\n",
    "    print(f\"{agent_id}:\", type(y_scaler), y_scaler)\n",
    "    print(agent.ticker)\n",
    "    # if agent_id =='MacroSentiAgent':\n",
    "    #     print(agent.macro_df.columns[:20])\n"
   ],
   "id": "66b249254f17dec8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "opinions = debate_agent.get_opinion(0, \"NVDA\")",
   "id": "f589370acbb473ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"opinions:{opinions}\")",
   "id": "fc427bc9ea25ee20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "리뷰탈 해보기..",
   "id": "f49eed29113fe250"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "debate_agent.get_rebuttal(1)",
   "id": "d004d0920d0e7a30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "get_revise 해보기",
   "id": "f2046191560121fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "debate_agent.get_revise(1)\n",
    "\n",
    "'''\n",
    "에러 나오는 중..\n",
    "[TechnicalAgent] fine-tuning 실패: 'NoneType' object has no attribute 'parameters'\n",
    "[SentimentalAgent] fine-tuning 실패: 'NoneType' object has no attribute 'parameters'\n",
    "'''"
   ],
   "id": "5864e38c717b2091",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "결과 도출해보기",
   "id": "3dd6e0132cd336ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "debate_agent.get_ensemble()",
   "id": "965fafa4037a8779",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "통합해서 한번에 실행하기: 기본 라운드는 총 3회. 클래스 정의떄 수정 가능",
   "id": "d95433c6fdbf4269"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#debate_agent.run()",
   "id": "73e302062a6607e2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
