import torch
import torch.nn as nn
import numpy as np
import random
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Literal
import requests
from datetime import datetime, timedelta
import json
import time 
import os
import csv
import yfinance as yf
import pandas as pd
import joblib 
import shap 

# FinBERT ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (AutoModel ì‚¬ìš© ìœ ì§€)
try:
    from transformers import AutoModelForSequenceClassification, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    print(f"FinBERT ë¡œë“œ ì˜¤ë¥˜: {e}. ê¸°ëŠ¥ ë¹„í™œì„±í™”.")


# EODhd API ì„¤ì • (API í‚¤ëŠ” ì‹¤ì œ í‚¤ë¡œ êµì²´í•´ì•¼ í•©ë‹ˆë‹¤. âš ï¸)
API_KEY = ' 68e3a8c46e9a65.00465987'
BASE_URL_EODHD = 'https://eodhd.com/api/news'
STATUS_FILE = 'collection_status.json' # ìƒíƒœ íŒŒì¼ëª…

# íŒŒì¼ ê²½ë¡œ ì •ì˜
MODEL_PATH = 'model_lstm_bothsentiment_V2.pt'
SCALER_X_PATH = 'scaler_x_both_V2.pkl'
SCALER_Y_PATH = 'scaler_y_both_V2.pkl'

# ğŸ“Œ ëª¨ë¸ ì…ë ¥ í”¼ì²˜ ë° ì‹œí€€ìŠ¤ ê¸¸ì´
# FEATURES = ['prob_positive','prob_negative','prob_neutral','n_news','ret','Close', 'eod_sentiment']
INPUT_FEATURES = 7 # ìœ„ 7ê°œ í”¼ì²˜
WINDOW_SIZE = 10 

# ğŸ“Œ LLM Opinion Prompt ì •ì˜ (ì´ì „ê³¼ ë™ì¼)
OPINION_PROMPTS = {
    "sentimental": {
        "system": (
            "ë‹¹ì‹ ì€ ê°ì„± ë° í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ì— íŠ¹í™”ëœ ìˆ˜ì„ ì „ëµê°€ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ Context ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬, ì˜ˆì¸¡ ì¢…ê°€(our_prediction)ì— ëŒ€í•œ ë…¼ë¦¬ì ì´ê³  ê°„ê²°í•˜ë©° ì„¤ë“ë ¥ ìˆëŠ” ì˜ê²¬ì„ í•œêµ­ì–´ë¡œ 3ë¬¸ì¥ ì´ë‚´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. "
            "ê°ì„± ì ìˆ˜ì™€ ì£¼ìš” í† í”½ì„ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ê³ , ë¶ˆí™•ì‹¤ì„±(uncertainty)ì„ ê³ ë ¤í•˜ì—¬ ì˜ê²¬ì„ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”."
        ),
        "user": (
            "ë‹¤ìŒì€ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì…ë ¥ ë°ì´í„°ì™€ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ê²¬ì„ ì‘ì„±í•˜ì„¸ìš”:\n"
            "Context: {context}" 
        )
    }
}

# base_agent í´ë˜ìŠ¤ ì •ì˜
class BaseAgent:
    def __init__(self, agent_id: str, **kwargs):
        self.agent_id = agent_id
    
    def searcher(self, ticker: str) -> 'StockData':
        raise NotImplementedError
        
    def predictor(self, ticker: str) -> 'Target':
        raise NotImplementedError

# ==============================================================================
# ê³µí†µ ë°ì´í„° í¬ë§· (ì´ì „ê³¼ ë™ì¼)
# ==============================================================================

@dataclass
class Target:
    """ì˜ˆì¸¡ ëª©í‘œê°’ ë¬¶ìŒ"""
    next_close : float
    uncertainty: float = 0.0
    confidence: float = 0.0
    idea: Dict[str, List[Any]] = field(default_factory=dict)

@dataclass
class Opinion:
    """ì—ì´ì „íŠ¸ì˜ ì˜ê²¬"""
    agent_id: str
    target: Target
    reason: str

@dataclass
class Rebuttal:
    """ì—ì´ì „íŠ¸ ê°„ ë°˜ë°•/ì§€ì§€ ë©”ì‹œì§€"""
    from_agent_id: str
    to_agent_id: str
    stance: Literal["REBUT", "SUPPORT"]
    message: str

@dataclass
class RoundLog:
    """ë¼ìš´ë“œë³„ ê¸°ë¡ ìŠ¤ëƒ…ìƒ·(ì˜µì…”ë„ë¡œ ì‚¬ìš©)"""
    round_no: int
    opinions: List[Opinion]
    rebuttals: List[Rebuttal]
    summary: Dict[str, Target]

@dataclass
class StockData:
    """ì—ì´ì „íŠ¸ ì…ë ¥ ì›ì²œ ë°ì´í„°"""
    sentimental: Dict 
    fundamental: Dict
    technical: Dict
    last_price: Optional[float] = None 
    currency: Optional[str] = None

# ==============================================================================
# StockSentimentLSTM ëª¨ë¸ êµ¬ì¡°
# ==============================================================================
class StockSentimentLSTM(nn.Module):
    def __init__(self, hidden_dim=128, num_layers=2, input_size=INPUT_FEATURES): 
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True, dropout=0.3) 
        self.fc = nn.Linear(hidden_dim, 1) 
        
    def forward(self, x):
        out, _ = self.lstm(x) 
        return self.fc(out[:, -1, :]).squeeze() 
    
# ==============================================================================
# SentimentalAgent í´ë˜ìŠ¤ êµ¬í˜„
# ==============================================================================
class SentimentalAgent(BaseAgent):
    def __init__(self, agent_id: str, input_features: int = INPUT_FEATURES):
        super().__init__(agent_id)
        self.input_features = input_features
        self.window_size = WINDOW_SIZE
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # === [FinBERT ë¡œë“œ: AutoModel ì‚¬ìš© ìœ ì§€] ===
        self.finbert_loaded = TRANSFORMERS_AVAILABLE
        self.finbert_tokenizer = None
        self.finbert_model = None
        if self.finbert_loaded:
            try:
                # ì´ë¯¸ ìƒë‹¨ì—ì„œ ì„í¬íŠ¸í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë¡œë“œë§Œ ì‹œë„
                if 'AutoTokenizer' in globals() and 'AutoModelForSequenceClassification' in globals():
                     self.finbert_tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-tone') 
                     self.finbert_model = AutoModelForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone') 
                     print("FinBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                else:
                    raise ImportError("AutoModel/AutoTokenizer is not available")
            except Exception as e:
                self.finbert_loaded = False
                print(f"FinBERT ë¡œë“œ ì˜¤ë¥˜: {e}. ê¸°ëŠ¥ ë¹„í™œì„±í™”.")
        else:
             print("FinBERT ë¡œë“œ ì˜¤ë¥˜: transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜. ê¸°ëŠ¥ ë¹„í™œì„±í™”.")
        # ===============================================
        
        # ğŸŒŸ ì‹¤ì œ ëª¨ë¸ êµ¬ì¡° ì •ì˜ (LSTM)
        self.model = StockSentimentLSTM()
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë° ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ë¡œì§
        self.scaler_X = None
        self.scaler_y = None
        
        try:
            if os.path.exists(SCALER_X_PATH):
                # joblibì€ map_locationì´ í•„ìš” ì—†ìŒ (PyTorch ê°ì²´ê°€ ì•„ë‹˜)
                self.scaler_X = joblib.load(SCALER_X_PATH)
                print(f"[SUCCESS] ì…ë ¥ ìŠ¤ì¼€ì¼ëŸ¬(X) ë¡œë“œ ì™„ë£Œ: {SCALER_X_PATH}")
            if os.path.exists(SCALER_Y_PATH):
                self.scaler_y = joblib.load(SCALER_Y_PATH)
                print(f"[SUCCESS] ì¶œë ¥ ìŠ¤ì¼€ì¼ëŸ¬(Y) ë¡œë“œ ì™„ë£Œ: {SCALER_Y_PATH}")
            if os.path.exists(MODEL_PATH):
                # ğŸš¨ [ìˆ˜ì • ë°˜ì˜] CUDA/CPU ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ map_location='cpu' ì¶”ê°€
                state_dict = torch.load(MODEL_PATH, map_location=torch.device('cpu')) 
                self.model.load_state_dict(state_dict)
                self.model.eval()
                print(f"**ì‹¤ì œ LSTM ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ:** {MODEL_PATH} (CPU ë§¤í•‘)")
        except Exception as e:
            # ê¸°ì¡´ ë¡œê·¸ì— ìˆë˜ ì˜¤ë¥˜ ë©”ì‹œì§€
            print(f"ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì„ì‹œ ê°€ì¤‘ì¹˜ ì‚¬ìš©.")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ëª¨ë¸ì´ ì˜ˆì¸¡ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, self.modelì„ Noneìœ¼ë¡œ ì„¤ì •í•˜ê±°ë‚˜ ì ì ˆí•œ ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

    # ==========================================================================
    # ğŸŒŸ LLM Context ë° Opinion ë¹Œë“œ í—¬í¼ ë©”ì„œë“œ (ì´ì „ê³¼ ë™ì¼)
    # ==========================================================================
    def _build_llm_context(self, ticker: str, stock_data: StockData, target: Target) -> Dict[str, Any]:
        """
        Opinion ë©”ì‹œì§€ ìƒì„±ì„ ìœ„í•œ LLM Context ë”•ì…”ë„ˆë¦¬ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤. (ctx)
        """
        
        t = ticker 
        ccy = stock_data.currency.upper() if stock_data.currency else "USD" 
        last_price = float(stock_data.last_price or 0.0) 
        
        # ----------------------------------------------------
        # ğŸ“Œ ctx ë”•ì…”ë„ˆë¦¬ ìƒì„±
        # ----------------------------------------------------
        ctx = {
            "ticker": t,
            "currency": ccy,
            "last_price": last_price,
            
            # SentimentalAgentì˜ searcher ë°ì´í„°ë¥¼ ëª¨ë‘ í¬í•¨
            "fundamental_summary": stock_data.fundamental or {}, 
            # SENTIMENTAL_SEQUENCE í‚¤ ì¶”ê°€
            "sentimental_summary": {
                "SENTIMENTAL_SEQUENCE": stock_data.sentimental.get('sequence_data', {}),
                "window_size": self.window_size
            }, 
            "technical_summary": stock_data.technical or {},
            
            "our_prediction": float(target.next_close),
            "uncertainty": float(target.uncertainty), 
            "confidence": float(target.confidence), 
            "model_idea": target.idea,
        }
        
        return ctx

    def build_opinion_messages(self, ticker: str, stock_data: StockData, target: Target) -> tuple[str, str]:
        """ Opinion ìƒì„±ì„ ìœ„í•´ LLMì— ì „ë‹¬í•  system_textì™€ user_textë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤. """
        
        ctx = self._build_llm_context(ticker, stock_data, target)
        
        # OPINION_PROMPTSì˜ "sentimental" í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ë¹Œë“œ
        system_text = OPINION_PROMPTS["sentimental"]["system"]
        user_text = OPINION_PROMPTS["sentimental"]["user"].format(
            context=json.dumps(ctx, ensure_ascii=False) # Contextë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”
        )
        
        return system_text, user_text

    def build_opinion(self, ticker: str, stock_data: StockData, target: Target, reason_text: Optional[str] = None) -> Opinion:
        """ Opinion ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (reason_textëŠ” LLM ì‘ë‹µì´ë¼ê³  ê°€ì •) """

        # ë§Œì•½ LLMì„ í˜¸ì¶œí•˜ì§€ ì•Šê³  ë”ë¯¸ ì´ìœ ë¥¼ ë§Œë“¤ ê²½ìš°:
        if reason_text is None:
            # ì‹œí€€ìŠ¤ ë°ì´í„°ì—ì„œ ìµœì‹  ê°ì„± ì ìˆ˜ë§Œ ì¶”ì¶œí•˜ì—¬ ë”ë¯¸ ì´ìœ  ìƒì„±
            seq_data = stock_data.sentimental.get('sequence_data', {})
            try:
                # 'prob_positive'ê°€ ì—†ìœ¼ë©´ 0.0ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ KeyError ë°©ì§€
                prob_positive = seq_data.get('prob_positive', [0.0])[-1] 
                prob_negative = seq_data.get('prob_negative', [0.0])[-1]
                sentiment_score = prob_positive - prob_negative
            except IndexError:
                sentiment_score = 0.0
                
            pred = target.next_close
            last = stock_data.last_price or 0.0
            
            reason_text = (
                f"[ê°ì„± ë¶„ì„ ë³´ê³ ì„œ]: ìµœì‹  FinBERT ê°ì„± ì ìˆ˜({sentiment_score:.2f})ëŠ” ì£¼ê°€ ë³€ë™ì„±ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. "
                f"LSTM ëª¨ë¸ì€ {self.window_size}ì¼ê°„ì˜ ì‹œí€€ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜„ì¬ê°€({last:.2f}) ëŒ€ë¹„ ë‹¤ìŒ ì¢…ê°€ë¥¼ {pred:.2f}ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤. "
                f"ë¶ˆí™•ì‹¤ì„±({target.uncertainty:.4f})ì´ í¬ë¯€ë¡œ ì£¼ì˜ ê¹Šì€ ê´€ì°°ì´ í•„ìš”í•©ë‹ˆë‹¤."
             )
        
        return Opinion(
            agent_id=self.agent_id,
            target=target,
            reason=reason_text
        )


    # ==========================================================================
    # ğŸŒŸ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì½”ë“œ í†µí•© (ë°ì´í„° ìˆ˜ì§‘ ê´€ë ¨ í•¨ìˆ˜ - ë°°ì¹˜ ìˆ˜ì§‘ìš©, ë³€ê²½ ì—†ìŒ)
    # ==========================================================================

    def load_status(self):
        if os.path.exists(STATUS_FILE):
            with open(STATUS_FILE, 'r') as f:
                return json.load(f)
        return {'completed_symbols': []}

    def save_status(self, status):
        with open(STATUS_FILE, 'w') as f:
            json.dump(status, f, indent=4)

    def collect_news_data_eodhd_batch(self, ticker, from_date, to_date):
        all_news = []
        offset = 0
        limit = 1000 

        while True:
            params = {
                's': ticker,
                'from': from_date,
                'to': to_date,
                'api_token': API_KEY,
                'limit': limit,
                'offset': offset,
                'extended': 1 
            }
            try:
                response = requests.get(BASE_URL_EODHD, params=params, timeout=30)
            except requests.exceptions.RequestException as e:
                print(f"[{ticker}] API í˜¸ì¶œ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜/íƒ€ì„ì•„ì›ƒ ë°œìƒ: {e}")
                return all_news, offset
                
            if response.status_code == 200:
                news_list = response.json()
                if not news_list: break 

                for news in news_list:
                    data = {
                        'date': news.get('date', ''),
                        'title': news.get('title', ''),
                        'summary': news.get('content', ''), 
                        'related': news.get('symbols', ticker), 
                        'ticker': ticker,
                        'sentiment_score': news.get('sentiment', '')
                    }
                    all_news.append(data)

                if len(news_list) < limit: break 
                else:
                    offset += limit
                    time.sleep(1)
            else:
                # ğŸš¨ API ì¸ì¦ ì˜¤ë¥˜ 401 í¬í•¨í•˜ì—¬ ì˜¤ë¥˜ ì½”ë“œ ì¶œë ¥
                print(f"[{ticker}] API í˜¸ì¶œ ì˜¤ë¥˜ {response.status_code} - {response.text.strip()[:100]}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë°ì´í„°ë§Œ ë°˜í™˜
                return all_news, offset 
                
        return all_news, -1 

    def save_news_to_csv(self, news_data, filename, mode='a'):
        fieldnames=['date', 'title', 'summary', 'related', 'ticker', 'sentiment_score']
        file_exists = os.path.exists(filename) and mode == 'a' 
        
        with open(filename, mode=mode, newline='', encoding='utf-8') as file:
            writer = csv.DictWriter(file, fieldnames=fieldnames)
            if mode == 'w' or not file_exists:
                writer.writeheader()
            
            for record in news_data:
                writer.writerow(record)
        print(f"ë°ì´í„° {len(news_data)}ê°œë¥¼ {filename} íŒŒì¼ì— ì €ì¥ ì™„ë£Œ")

    def collect_historical_data(self, tickers: List[str]):
        """ (ë°°ì¹˜ ìˆ˜ì§‘ ë©”ì„œë“œ) - ìƒëµ """
        print("ì´ ë©”ì„œë“œëŠ” Historical Data ìˆ˜ì§‘ìš©ìœ¼ë¡œ, ì‹¤ì‹œê°„ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ì—ì„œ í˜¸ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        pass

# ==========================================================================
    # ğŸŒŸ FinBERTë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜ (í•™ìŠµ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
    # ==========================================================================
    def _finbert_sentiment_scores(self, texts: List[str]) -> np.ndarray: 
        if not self.finbert_loaded:
            # FinBERTê°€ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ë”ë¯¸ ë°ì´í„° ë°˜í™˜ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
            print("[WARNING] FinBERT ë¹„í™œì„±í™”: ë”ë¯¸ ê°ì„± ì ìˆ˜ ë°˜í™˜.")
            # 3ë¶„ë¥˜(ê¸ì •/ë¶€ì •/ì¤‘ë¦½) ëª¨ë¸ì„ ê°€ì •í•˜ì—¬ 1/3ë¡œ ê· ë“±í•˜ê²Œ ë¶„ë°°
            return np.array([[0.333, 0.333, 0.334] for _ in texts]) 
        
        self.finbert_model.eval() 
        scores = []
        batch_size = 32
        with torch.no_grad(): 
            for i in range(0, len(texts), batch_size): 
                batch_texts = texts[i:i+batch_size]
                inputs = self.finbert_tokenizer(list(batch_texts), return_tensors='pt', 
                                                 padding=True, truncation=True, max_length=512)
                inputs = {name: tensor.to(self.device) for name, tensor in inputs.items()}
                outputs = self.finbert_model(**inputs) 
                # FinBERT ê²°ê³¼ëŠ” ê¸ì •/ë¶€ì •/ì¤‘ë¦½ í™•ë¥  ë°°ì—´ (softmax ì ìš©)
                scores.extend(torch.softmax(outputs.logits, dim=1).cpu().numpy())
                
        return np.array(scores)

    # ==========================================================================
    # ğŸŒŸ [Datetime ì˜¤ë¥˜ í•´ê²° ìµœì¢… ë¡œì§] searcherë¥¼ ìœ„í•œ ê³¼ê±° Nì¼ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
    # ==========================================================================
    def _fetch_latest_sequence_data(self, ticker: str) -> Dict[str, Any]:
        """
        tickerì˜ ê³¼ê±° WINDOW_SIZEì¼ ë™ì•ˆì˜ ì£¼ê°€ ë°ì´í„°ì™€ **ì‹¤ì œ FinBERT ê°ì„± ì ìˆ˜**ë¥¼ ê²°í•©í•˜ì—¬ ì‹œí€€ìŠ¤ ìƒì„±.
        """
        end_date = datetime.now() + timedelta(days=1) # ë‹¤ìŒë‚ ê¹Œì§€ (yfinance end-dateëŠ” exclusive)
        start_date = end_date - timedelta(days=self.window_size * 2) # ì¶©ë¶„í•œ ê¸°ê°„ í™•ë³´

        FEATURES_LIST = ['prob_positive','prob_negative','prob_neutral','n_news','ret','Close', 'eod_sentiment']

        # 1. ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘
        try:
            stock_data = yf.download(ticker, start=start_date.strftime('%Y-%m-%d'), end=end_date.strftime('%Y-%m-%d'), progress=False)
            
            # ğŸš¨ [Merge ì˜¤ë¥˜ í•´ê²° 1] MultiIndex ë° ì»¬ëŸ¼ ê°•ì œ ì •ë¦¬
            stock_data.columns = ['_'.join(str(c) for c in col) if isinstance(col, tuple) else str(col) for col in stock_data.columns]
            stock_data = stock_data.reset_index()
            
            # Ticker/Symbol, level_X ì»¬ëŸ¼ ì œê±° (MultiIndex ì”ì—¬ë¬¼)
            columns_to_drop = [col for col in stock_data.columns if 'Ticker' in col or 'Symbol' in col or 'level_' in col.lower()]
            stock_data = stock_data.drop(columns=list(set(columns_to_drop)), errors='ignore')
            
            # ğŸš¨ [Datetime ì˜¤ë¥˜ í•´ê²° 1] 'Date' ì»¬ëŸ¼ì—ì„œ ì‹œê°„ëŒ€ ì •ë³´(timezone) ì œê±° í›„ ì •ê·œí™”
            stock_data['Date'] = pd.to_datetime(stock_data['Date'])
            if stock_data['Date'].dt.tz is not None:
                 stock_data['Date'] = stock_data['Date'].dt.tz_localize(None) # ì‹œê°„ëŒ€ ì •ë³´ ì œê±°
            stock_data['Date'] = stock_data['Date'].dt.normalize() # ì‹œê°„ ì •ë³´ ì œê±° (ë‚ ì§œë§Œ ë‚¨ê¹€)
            
            # Close ì»¬ëŸ¼ ì •ë¦¬ ë° ì´ë¦„ í†µì¼
            close_col = [col for col in stock_data.columns if 'Close' in col and 'Adj' not in col]
            if close_col:
                 stock_data = stock_data.rename(columns={close_col[0]: 'Close'})
            
            stock_data['Close_Price'] = stock_data['Close']
            stock_data['Close'] = stock_data['Close_Price'] # í”¼ì²˜ ì´ë¦„ í†µì¼
            stock_data['ret'] = stock_data['Close'].pct_change()
            last_price = stock_data['Close'].iloc[-1]
            
            # í•„ìš”í•œ ì»¬ëŸ¼ê³¼ ê¸°ê°„ë§Œ ì„ íƒ (ì»¬ëŸ¼ ê¸°ë°˜)
            stock_data_final = stock_data[['Date', 'Close', 'ret']].tail(self.window_size).copy()
            
        except Exception as e:
            print(f"[FATAL] yfinance ì˜¤ë¥˜: {e}. ì˜ˆì¸¡ ë¶ˆê°€.")
            return None, 0.0, "USD"
        
        required_length = self.window_size
        if len(stock_data_final) < required_length:
            print(f"[WARNING] ì£¼ê°€ ë°ì´í„°ê°€ {required_length}ì¼ë³´ë‹¤ ì ìŠµë‹ˆë‹¤. ì˜ˆì¸¡ ë¶ˆê°€.")
            return None, 0.0, "USD"
        
        # 2. ê³¼ê±° 10ì¼ì¹˜ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ë° ê°ì„± ì ìˆ˜ ìˆ˜ì§‘ (ë‚ ì§œë³„ë¡œ)
        print(f"[{ticker}] ê³¼ê±° {required_length}ì¼ì¹˜ ë‰´ìŠ¤ ë° ê°ì„± ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        
        # ë‰´ìŠ¤ ê¸°ê°„ ì„¤ì •ì„ ìœ„í•´ Date ì»¬ëŸ¼ ì‚¬ìš© (stock_data_finalì€ Date ì»¬ëŸ¼ì„ ê°€ì§‘ë‹ˆë‹¤)
        news_end_date = stock_data_final['Date'].iloc[-1].strftime('%Y-%m-%d')
        news_start_date = stock_data_final['Date'].iloc[0].strftime('%Y-%m-%d')
        
        # EODhd API í˜¸ì¶œ (ê³¼ê±° WINDOW_SIZE ê¸°ê°„ ë™ì•ˆì˜ ë‰´ìŠ¤)
        all_news, _ = self.collect_news_data_eodhd_batch(ticker, news_start_date, news_end_date)
        news_df_raw = pd.DataFrame(all_news)
        
        # 3. FinBERT ë¶„ì„ ë° ì¼ë³„ ì§‘ê³„
        sentiment_data = {}
        key_topics = []
        
        if not news_df_raw.empty:
            news_df_raw['date'] = pd.to_datetime(news_df_raw['date'])
            # ğŸš¨ [Datetime ì˜¤ë¥˜ í•´ê²° 2] 'date' ì»¬ëŸ¼ì—ì„œ ì‹œê°„ëŒ€ ì •ë³´ ì œê±° í›„ ì •ê·œí™”
            if news_df_raw['date'].dt.tz is not None:
                news_df_raw['date'] = news_df_raw['date'].dt.tz_localize(None)
            news_df_raw['date'] = news_df_raw['date'].dt.normalize()
            
            news_df_raw['text'] = news_df_raw['title'] + ' ' + news_df_raw['summary']
            
            # FinBERT ë¶„ì„ ì‹¤í–‰
            finbert_scores = self._finbert_sentiment_scores(news_df_raw['text'].values.tolist())
            
            try:
                # ì¼ë°˜ì ì¸ FinBERT-tone ìˆœì„œ: ê¸ì •(0), ë¶€ì •(1), ì¤‘ë¦½(2)
                news_df_raw['prob_positive'] = finbert_scores[:, 0]
                news_df_raw['prob_negative'] = finbert_scores[:, 1]
                news_df_raw['prob_neutral'] = finbert_scores[:, 2]
            except IndexError:
                print("[FATAL] FinBERT ì ìˆ˜ ë°°ì—´ ì¸ë±ì‹± ì˜¤ë¥˜. FinBERT ì¶œë ¥ ìˆœì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                news_df_raw['prob_positive'] = 0.33
                news_df_raw['prob_negative'] = 0.33
                news_df_raw['prob_neutral'] = 0.34
            
            # ì¼ë³„ í‰ê·  ì§‘ê³„
            daily_sentiments = news_df_raw.groupby('date').agg(
                prob_positive=('prob_positive','mean'), 
                prob_negative=('prob_negative','mean'), 
                prob_neutral=('prob_neutral','mean'), 
                n_news=('title','count'),
                eod_sentiment=('sentiment_score', lambda x: pd.to_numeric(x, errors='coerce').mean())
            ).reset_index()
            daily_sentiments['date'] = daily_sentiments['date'].dt.normalize()
            
            # ğŸš¨ [Merge ì˜¤ë¥˜ í•´ê²° ìˆ˜ì • 2 ìµœì¢…] ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© ìˆ˜í–‰
            # stock_data_finalê³¼ daily_sentimentsë¥¼ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© ('Date'ì™€ 'date' ì»¬ëŸ¼ì„ ì‚¬ìš©)
            combined_df = stock_data_final.merge(daily_sentiments, left_on='Date', right_on='date', how='left') 
            
            # ë³‘í•© í›„, ë‹¤ì‹œ 'Date'ë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
            combined_df = combined_df.set_index('Date')
            
            # ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ë‚ ì§œ ì»¬ëŸ¼ ì œê±°
            if 'date' in combined_df.columns:
                combined_df = combined_df.drop(columns=['date'])
            
            # ê°ì„± ë°ì´í„° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ 0.0 ë˜ëŠ” ì¤‘ë¦½ìœ¼ë¡œ ëŒ€ì²´)
            combined_df['prob_positive'] = combined_df['prob_positive'].fillna(0.33)
            combined_df['prob_negative'] = combined_df['prob_negative'].fillna(0.33)
            combined_df['prob_neutral'] = combined_df['prob_neutral'].fillna(0.34)
            combined_df['n_news'] = combined_df['n_news'].fillna(0)
            combined_df['eod_sentiment'] = combined_df['eod_sentiment'].fillna(0.0)

            # í‚¤ í† í”½ ì¶”ì¶œ (ìµœì‹  ë‰´ìŠ¤ ì œëª© 3ê°œ)
            key_topics = news_df_raw['title'].tail(3).tolist()
        
        else:
            print(f"[{ticker}] ê³¼ê±° {required_length}ì¼ê°„ ë‰´ìŠ¤ ì—†ìŒ. ê°ì„± í”¼ì²˜ë¥¼ ì¤‘ë¦½ìœ¼ë¡œ ëŒ€ì²´.")
            # ë‰´ìŠ¤ê°€ ì—†ëŠ” ê²½ìš°, ì¤‘ë¦½ ê°ì„± ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° 'prob_positive' ì˜¤ë¥˜ ë°©ì§€
            
            # ì¸ë±ìŠ¤ë¥¼ ì„¤ì •í•œ í›„ ë³µì‚¬
            stock_data_final = stock_data_final.set_index('Date') 
            combined_df = stock_data_final.copy()
            
            # í•„ìˆ˜ í”¼ì²˜ë¥¼ ì¤‘ë¦½ ê°’ìœ¼ë¡œ ëª…ì‹œì  ì´ˆê¸°í™”
            combined_df['prob_positive'] = 0.33 
            combined_df['prob_negative'] = 0.33
            combined_df['prob_neutral'] = 0.34
            combined_df['n_news'] = 0
            combined_df['eod_sentiment'] = 0.0

        # ìµœì¢… ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
        combined_df = combined_df.fillna(0.0) 
        
        sequence_array = combined_df[FEATURES_LIST].values
        sequence_data_dict = combined_df[FEATURES_LIST].to_dict(orient='list')

        return {
            "sequence_array": sequence_array, 
            "sequence_data": sequence_data_dict,
            "key_topics": key_topics
           }, last_price, "USD"

    # searcher ë° predictorëŠ” ì´ì „ ìˆ˜ì •ë³¸ê³¼ ë™ì¼ (ìƒëµ)
    def searcher(self, ticker: str) -> StockData:
        seq_data_results, last_price, currency = self._fetch_latest_sequence_data(ticker)
        
        if seq_data_results is None:
            raise RuntimeError("ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì¶©ë¶„íˆ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        data = StockData(
            sentimental=seq_data_results, 
            fundamental={},
            technical={},
            last_price=last_price,
            currency=currency
        )
        return data

    def predictor(self, ticker: str) -> Target:
        stock_data = self.searcher(ticker)
        last_price = stock_data.last_price or 500.0
        input_sequence = stock_data.sentimental["sequence_array"] 

        # X ìŠ¤ì¼€ì¼ë§ ì ìš©
        if self.scaler_X:
            scaled_array = self.scaler_X.transform(input_sequence)
            input_data_tensor = torch.tensor(scaled_array[np.newaxis, :, :], dtype=torch.float32) 
        else:
            input_data_tensor = torch.tensor(input_sequence[np.newaxis, :, :], dtype=torch.float32)
        
        # ëª¬í…Œ ì¹´ë¥¼ë¡œ ë“œë¡­ì•„ì›ƒ (MCDO) ì‹œë®¬ë ˆì´ì…˜
        num_samples = 150
        predictions_raw = [] 

        self.model.train() 
        for _ in range(num_samples):
            with torch.no_grad():
                scaled_output = self.model(input_data_tensor).item() 
                predictions_raw.append(scaled_output)

        predictions_np = np.array(predictions_raw).reshape(-1, 1) 

        # ì¶œë ¥ ì—­ë³€í™˜ (Y)
        if self.scaler_y:
            predicted_prices_np = self.scaler_y.inverse_transform(predictions_np)
            predicted_prices = predicted_prices_np.flatten()
        else:
            predicted_prices = predictions_np.flatten()
            
        # Target í´ë˜ìŠ¤ í•„ë“œ ê³„ì‚°
        next_close = float(np.mean(predicted_prices))
        uncertainty = float(np.std(predicted_prices))
        confidence = float(1.0 / (1.0 + uncertainty * 10))
        confidence = min(1.0, confidence)

        # í”¼ì³ì¤‘ìš”ë„ ë° SHAP (ë”ë¯¸/ë‹¨ìˆœí™”)
        feature_importances = [random.uniform(0.1, 0.9) for _ in range(self.input_features)]
        shap_values = [random.uniform(-0.5, 0.5) for _ in range(self.input_features)]
        
        # Target í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
        result = Target(
            next_close=next_close,
            uncertainty=uncertainty,
            confidence=confidence,
            idea={
                "sentiment_score": [stock_data.sentimental['sequence_data']['prob_positive'][-1] - stock_data.sentimental['sequence_data']['prob_negative'][-1]], 
                "feature_names": [f"feature_{i+1}" for i in range(self.input_features)],
                "related_news_summary": stock_data.sentimental.get("key_topics", ["ë°ì´í„° ì‹œí€€ìŠ¤ ê¸°ë°˜ ì˜ˆì¸¡"]),
                "mc_price_samples": [float(p) for p in predicted_prices[:5]],
                "feature_importances": feature_importances,
                "shap_values": shap_values
            }
        )
        
        return result


# ==============================================================================
# ì—ì´ì „íŠ¸ ì‚¬ìš© ì˜ˆì‹œ (ì´ì „ê³¼ ë™ì¼)
# ==============================================================================
if __name__ == '__main__':
    #... (ìƒëµ: í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë¡œì§)
    print("="*50)
    print("## SentimentalAgent í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì‹¤ì œ FinBERT ë¶„ì„ í¬í•¨)")
    print("="*50)
    
    TEST_TICKER = "MSFT"

    agent_for_inference = SentimentalAgent(agent_id="SentimentalAgent")
    
    try:
        # FinBERT ë¡œë“œ ì˜¤ë¥˜ê°€ í•´ê²°ë˜ë©´, ì´ì œ API 401 ì˜¤ë¥˜ê°€ ë°œìƒí•  ê²ƒì…ë‹ˆë‹¤.
        # ê·¸ëŸ¬ë‚˜ 'prob_positive' ì¹˜ëª…ì  ì˜¤ë¥˜ëŠ” ì¤‘ë¦½ ê°’ ì´ˆê¸°í™” ë¡œì§ìœ¼ë¡œ ë°©ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.
        test_stock_data = agent_for_inference.searcher(TEST_TICKER)
        target_result = agent_for_inference.predictor(ticker=TEST_TICKER)
        
        system_msg, user_msg = agent_for_inference.build_opinion_messages(TEST_TICKER, test_stock_data, target_result)
        final_opinion = agent_for_inference.build_opinion(TEST_TICKER, test_stock_data, target_result)

        print(f"## ìµœì¢… ê°ì„± ì˜ˆì¸¡ ê²°ê³¼ ({TEST_TICKER}) (Target í´ë˜ìŠ¤ ì¶œë ¥ í˜•íƒœ):")
        print(f"ìµœì‹  ì¢…ê°€ (ê²€ìƒ‰): {test_stock_data.last_price:.2f} {test_stock_data.currency}")
        print(f"next_close (ì˜ˆì¸¡ ì¢…ê°€): {target_result.next_close:.2f}")
        print("-" * 50)
        
        print(f"## ìƒì„±ëœ Opinion ê°ì²´ì˜ Reason (ë”ë¯¸):")
        print(final_opinion.reason)
        print("="*50)

    except RuntimeError as e:
        print(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ì˜¤ë¥˜: {e}")
    except Exception as e:
        # ì´ì „ì— ë°œìƒí–ˆë˜ 'prob_positive' ì˜¤ë¥˜ëŠ” ë°œìƒí•˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")